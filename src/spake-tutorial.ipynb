{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b501bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a4f0d",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "Part 1: SparkSession 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf7835f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/19 05:35:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession 생성 완료!\n",
      "  버전: 3.5.8\n",
      "  Master: spark://spark-master:7077\n",
      "  총 코어: 2\n"
     ]
    }
   ],
   "source": [
    "# SparkSession 생성\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Day11-SparkBasics\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.executor.memory\", \"512m\")\n",
    "    .config(\"spark.executor.cores\", \"1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"SparkSession 생성 완료!\")\n",
    "print(f\"  버전: {spark.version}\")\n",
    "print(f\"  Master: {spark.sparkContext.master}\")\n",
    "print(f\"  총 코어: {spark.sparkContext.defaultParallelism}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e43402",
   "metadata": {},
   "source": [
    "Part 2: 테스트 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9a6c340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/api_events_100k.csv 이미 존재\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 생성\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def generate_api_events(num_records):\n",
    "    \"\"\"API 이벤트 데이터 생성\"\"\"\n",
    "    endpoints = [\"/api/products\", \"/api/users\", \"/api/orders\", \"/api/payments\", \"/api/search\"]\n",
    "    methods = [\"GET\", \"POST\", \"PUT\", \"DELETE\"]\n",
    "    status_codes = [200, 200, 200, 200, 201, 400, 404, 500]\n",
    "    base_time = datetime(2024, 1, 1, 0, 0, 0)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"request_id\": [f\"REQ_{i:08d}\" for i in range(num_records)],\n",
    "        \"user_id\": [f\"U{np.random.randint(1, 1001):04d}\" for _ in range(num_records)],\n",
    "        \"endpoint\": np.random.choice(endpoints, num_records),\n",
    "        \"method\": np.random.choice(methods, num_records),\n",
    "        \"status_code\": np.random.choice(status_codes, num_records),\n",
    "        \"response_time_ms\": np.random.randint(10, 500, num_records),\n",
    "        \"timestamp\": [base_time + timedelta(seconds=i * 0.1) for i in range(num_records)],\n",
    "    })\n",
    "\n",
    "\n",
    "# 10만 건 데이터 생성\n",
    "filepath = \"/data/api_events_100k.csv\"\n",
    "if not os.path.exists(filepath):\n",
    "    df = generate_api_events(100_000)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"{filepath} 생성 완료 (100,000건)\")\n",
    "else:\n",
    "    print(f\"{filepath} 이미 존재\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f27de1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 05:35:23 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터: 100,000건\n",
      "\n",
      "스키마:\n",
      "root\n",
      " |-- request_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- status_code: integer (nullable = true)\n",
      " |-- response_time_ms: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "df = spark.read.csv(\"/data/api_events_100k.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(f\"데이터: {df.count():,}건\")\n",
    "print(\"\\n스키마:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d9c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+-------------+------+-----------+----------------+--------------------+\n",
      "|  request_id|user_id|     endpoint|method|status_code|response_time_ms|           timestamp|\n",
      "+------------+-------+-------------+------+-----------+----------------+--------------------+\n",
      "|REQ_00000000|  U0103|   /api/users|  POST|        200|             289| 2024-01-01 00:00:00|\n",
      "|REQ_00000001|  U0436|/api/payments|   PUT|        400|             337|2024-01-01 00:00:...|\n",
      "|REQ_00000002|  U0861|/api/products|  POST|        201|             427|2024-01-01 00:00:...|\n",
      "|REQ_00000003|  U0271|  /api/orders|DELETE|        400|             409|2024-01-01 00:00:...|\n",
      "|REQ_00000004|  U0107|/api/products|DELETE|        404|             392|2024-01-01 00:00:...|\n",
      "+------------+-------+-------------+------+-----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 데이터 미리보기\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ff4bc0",
   "metadata": {},
   "source": [
    "1. 컬럼 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a531336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----------------+\n",
      "|     endpoint|status_code|response_time_ms|\n",
      "+-------------+-----------+----------------+\n",
      "|   /api/users|        200|             289|\n",
      "|/api/payments|        400|             337|\n",
      "|/api/products|        201|             427|\n",
      "|  /api/orders|        400|             409|\n",
      "|/api/products|        404|             392|\n",
      "+-------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas: df[[\"endpoint\", \"status_code\", \"response_time_ms\"]]\n",
    "# Spark:\n",
    "selected = df.select(\"endpoint\", \"status_code\", \"response_time_ms\")\n",
    "selected.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13834ea9",
   "metadata": {},
   "source": [
    "2. 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033aaeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에러 건수: 37,665건\n",
      "+------------+-------+-------------+------+-----------+----------------+--------------------+\n",
      "|  request_id|user_id|     endpoint|method|status_code|response_time_ms|           timestamp|\n",
      "+------------+-------+-------------+------+-----------+----------------+--------------------+\n",
      "|REQ_00000001|  U0436|/api/payments|   PUT|        400|             337|2024-01-01 00:00:...|\n",
      "|REQ_00000003|  U0271|  /api/orders|DELETE|        400|             409|2024-01-01 00:00:...|\n",
      "|REQ_00000004|  U0107|/api/products|DELETE|        404|             392|2024-01-01 00:00:...|\n",
      "|REQ_00000009|  U0122|  /api/search|   GET|        400|             249|2024-01-01 00:00:...|\n",
      "|REQ_00000012|  U0331|/api/payments|   GET|        400|             300|2024-01-01 00:00:...|\n",
      "+------------+-------+-------------+------+-----------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Pandas: df[df[\"status_code\"] >= 400]\n",
    "# Spark:\n",
    "errors = df.filter(col(\"status_code\") >= 400)\n",
    "print(f\"에러 건수: {errors.count():,}건\")\n",
    "errors.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52dd3b",
   "metadata": {},
   "source": [
    "3. 새 컬럼 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f2ec681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------+\n",
      "|  request_id|status_code|is_error|\n",
      "+------------+-----------+--------+\n",
      "|REQ_00000000|        200|   false|\n",
      "|REQ_00000001|        400|    true|\n",
      "|REQ_00000002|        201|   false|\n",
      "|REQ_00000003|        400|    true|\n",
      "|REQ_00000004|        404|    true|\n",
      "+------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Pandas: df[\"is_error\"] = df[\"status_code\"] >= 400\n",
    "# Spark:\n",
    "df_with_flag = df.withColumn(\n",
    "    \"is_error\",\n",
    "    when(col(\"status_code\") >= 400, True).otherwise(False)\n",
    ")\n",
    "df_with_flag.select(\"request_id\", \"status_code\", \"is_error\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ba93d",
   "metadata": {},
   "source": [
    "4. 그룹별 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3eae942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------------------+------------+\n",
      "|     endpoint|요청수|      평균응답시간|최대응답시간|\n",
      "+-------------+------+------------------+------------+\n",
      "|/api/products| 19982|254.71939745771195|         499|\n",
      "|  /api/search| 20137|253.70283557630233|         499|\n",
      "|  /api/orders| 19809|  254.466202231309|         499|\n",
      "|   /api/users| 20119|254.30170485610617|         499|\n",
      "|/api/payments| 19953|255.65869794015939|         499|\n",
      "+-------------+------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, avg, max as spark_max\n",
    "\n",
    "# Pandas: df.groupby(\"endpoint\").agg({\"request_id\": \"count\", \"response_time_ms\": \"mean\"})\n",
    "# Spark:\n",
    "endpoint_stats = df.groupBy(\"endpoint\").agg(\n",
    "    count(\"request_id\").alias(\"요청수\"),\n",
    "    avg(\"response_time_ms\").alias(\"평균응답시간\"),\n",
    "    spark_max(\"response_time_ms\").alias(\"최대응답시간\"),\n",
    ")\n",
    "endpoint_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852dcaea",
   "metadata": {},
   "source": [
    "5. 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f04817e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+----------------+\n",
      "|  request_id|     endpoint|response_time_ms|\n",
      "+------------+-------------+----------------+\n",
      "|REQ_00084401|   /api/users|             499|\n",
      "|REQ_00001991|  /api/search|             499|\n",
      "|REQ_00082961|/api/products|             499|\n",
      "|REQ_00003287|/api/payments|             499|\n",
      "|REQ_00082048|   /api/users|             499|\n",
      "+------------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pandas: df.sort_values(\"response_time_ms\", ascending=False)\n",
    "# Spark:\n",
    "sorted_df = df.orderBy(col(\"response_time_ms\").desc())\n",
    "sorted_df.select(\"request_id\", \"endpoint\", \"response_time_ms\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3cdfc5",
   "metadata": {},
   "source": [
    "SQL 쿼리 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdb75c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+------------+------+\n",
      "|     endpoint|요청수|평균응답시간|에러수|\n",
      "+-------------+------+------------+------+\n",
      "|  /api/search| 20137|       253.7|  7591|\n",
      "|   /api/users| 20119|       254.3|  7567|\n",
      "|/api/products| 19982|      254.72|  7539|\n",
      "|/api/payments| 19953|      255.66|  7452|\n",
      "|  /api/orders| 19809|      254.47|  7516|\n",
      "+-------------+------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Spark의 강점: SQL로 데이터 처리 가능\n",
    "df.createOrReplaceTempView(\"api_events\")\n",
    "\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        endpoint,\n",
    "        COUNT(*) as `요청수`,\n",
    "        ROUND(AVG(response_time_ms), 2) as `평균응답시간`,\n",
    "        SUM(CASE WHEN status_code >= 400 THEN 1 ELSE 0 END) as `에러수`\n",
    "    FROM api_events\n",
    "    GROUP BY endpoint\n",
    "    ORDER BY `요청수` DESC\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c8d74e",
   "metadata": {},
   "source": [
    "Part 4: Lazy Evaluation 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0c87b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation 정의 중... (아직 실행 안 됨)\n",
      "여기까지 아무것도 실행되지 않았음!\n",
      "Action을 호출해야 실행됨\n"
     ]
    }
   ],
   "source": [
    "# Transformation은 실행되지 않음 (계획만 세움)\n",
    "print(\"Transformation 정의 중... (아직 실행 안 됨)\")\n",
    "\n",
    "step1 = df.filter(col(\"status_code\") >= 400)\n",
    "step2 = step1.groupBy(\"endpoint\").count()\n",
    "step3 = step2.orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"여기까지 아무것도 실행되지 않았음!\")\n",
    "print(\"Action을 호출해야 실행됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d52cb364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Action 호출 (collect):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(endpoint='/api/search', count=7591),\n",
       " Row(endpoint='/api/users', count=7567),\n",
       " Row(endpoint='/api/products', count=7539),\n",
       " Row(endpoint='/api/orders', count=7516),\n",
       " Row(endpoint='/api/payments', count=7452)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action 호출 → 모든 Transformation이 한번에 실행\n",
    "print(\"\\nAction 호출 (collect):\")\n",
    "result = step3.collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f6ac2",
   "metadata": {},
   "source": [
    "실행 계획 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e56496a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 실행 계획 ===\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=true\n",
      "+- == Final Plan ==\n",
      "   *(3) Sort [count#335L DESC NULLS LAST], true, 0\n",
      "   +- AQEShuffleRead coalesced\n",
      "      +- ShuffleQueryStage 1\n",
      "         +- Exchange rangepartitioning(count#335L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=343]\n",
      "            +- *(2) HashAggregate(keys=[endpoint#19], functions=[count(1)])\n",
      "               +- AQEShuffleRead coalesced\n",
      "                  +- ShuffleQueryStage 0\n",
      "                     +- Exchange hashpartitioning(endpoint#19, 200), ENSURE_REQUIREMENTS, [plan_id=312]\n",
      "                        +- *(1) HashAggregate(keys=[endpoint#19], functions=[partial_count(1)])\n",
      "                           +- *(1) Project [endpoint#19]\n",
      "                              +- *(1) Filter (isnotnull(status_code#21) AND (status_code#21 >= 400))\n",
      "                                 +- FileScan csv [endpoint#19,status_code#21] Batched: false, DataFilters: [isnotnull(status_code#21), (status_code#21 >= 400)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/api_events_100k.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status_code), GreaterThanOrEqual(status_code,400)], ReadSchema: struct<endpoint:string,status_code:int>\n",
      "+- == Initial Plan ==\n",
      "   Sort [count#335L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#335L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=292]\n",
      "      +- HashAggregate(keys=[endpoint#19], functions=[count(1)])\n",
      "         +- Exchange hashpartitioning(endpoint#19, 200), ENSURE_REQUIREMENTS, [plan_id=289]\n",
      "            +- HashAggregate(keys=[endpoint#19], functions=[partial_count(1)])\n",
      "               +- Project [endpoint#19]\n",
      "                  +- Filter (isnotnull(status_code#21) AND (status_code#21 >= 400))\n",
      "                     +- FileScan csv [endpoint#19,status_code#21] Batched: false, DataFilters: [isnotnull(status_code#21), (status_code#21 >= 400)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/data/api_events_100k.csv], PartitionFilters: [], PushedFilters: [IsNotNull(status_code), GreaterThanOrEqual(status_code,400)], ReadSchema: struct<endpoint:string,status_code:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 실행 계획 보기\n",
    "print(\"=== 실행 계획 ===\")\n",
    "step3.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81740475",
   "metadata": {},
   "source": [
    "Part 5: 복합 분석 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e209d092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "엔드포인트별 에러율 분석:\n",
      "+-------------+------+------+------------------+------------------+\n",
      "|     endpoint|총요청|에러수|      평균응답시간|            에러율|\n",
      "+-------------+------+------+------------------+------------------+\n",
      "|  /api/orders| 19809|  7516|  254.466202231309| 37.94234943712454|\n",
      "|/api/products| 19982|  7539|254.71939745771195| 37.72895606045441|\n",
      "|  /api/search| 20137|  7591|253.70283557630233|37.696777077022396|\n",
      "|   /api/users| 20119|  7567|254.30170485610617| 37.61121328097818|\n",
      "|/api/payments| 19953|  7452|255.65869794015939| 37.34776725304466|\n",
      "+-------------+------+------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# 엔드포인트별 에러율 분석\n",
    "error_analysis = (\n",
    "    #엔드포인트별로 요청이 몇 개야?\n",
    "    df.groupBy(\"endpoint\")\n",
    "    #agg()는 “여러 개 집계함수를 한 번에” 넣는 상자\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"총요청\"),\n",
    "        #그중에 에러(400 이상)는 몇 개야?\n",
    "        spark_sum(when(col(\"status_code\") >= 400, 1).otherwise(0)).alias(\"에러수\"),\n",
    "       #평균 응답 시간은?\n",
    "        avg(\"response_time_ms\").alias(\"평균응답시간\"),\n",
    "    )\n",
    "    #새 컬럼 만들기\n",
    "    .withColumn(\"에러율\", col(\"에러수\") / col(\"총요청\") * 100)\n",
    "    .orderBy(col(\"에러율\").desc())\n",
    ")\n",
    "\n",
    "print(\"엔드포인트별 에러율 분석:\")\n",
    "error_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09a87ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답시간 구간별 분포:\n",
      "+----------------+-----+\n",
      "|    응답시간구간|count|\n",
      "+----------------+-----+\n",
      "|   빠름 (<100ms)|18287|\n",
      "|   느림 (>300ms)|40726|\n",
      "|보통 (100-300ms)|40987|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 응답시간 구간별 분포\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "response_dist = (\n",
    "    df.withColumn(\n",
    "        \"응답시간구간\",\n",
    "        when(col(\"response_time_ms\") < 100, \"빠름 (<100ms)\")\n",
    "        .when(col(\"response_time_ms\") < 300, \"보통 (100-300ms)\")\n",
    "        .otherwise(\"느림 (>300ms)\")\n",
    "    )\n",
    "    .groupBy(\"응답시간구간\")\n",
    "    .count()\n",
    "    .orderBy(\"count\")\n",
    ")\n",
    "\n",
    "print(\"응답시간 구간별 분포:\")\n",
    "response_dist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae7529a",
   "metadata": {},
   "source": [
    "SparkSession 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1babc05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/19 07:10:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession 생성 완료!\n",
      "  버전: 3.5.8\n",
      "  앱 이름: PySpark-API-Basics\n",
      "  마스터: local[*]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# SparkSession 생성: 기본 형태\n",
    "# -----------------------------------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "# SparkSession.builder: SparkSession을 만들기 위한 빌더 객체 반환\n",
    "# .appName(\"이름\"): Spark UI에 표시될 애플리케이션 이름 설정\n",
    "# .getOrCreate(): 새 세션 생성, 이미 있으면 기존 세션 반환\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-API-Basics\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 생성된 세션 정보 확인\n",
    "print(\"SparkSession 생성 완료!\")\n",
    "print(f\"  버전: {spark.version}\")                          # Spark 버전\n",
    "print(f\"  앱 이름: {spark.sparkContext.appName}\")          # 앱 이름\n",
    "print(f\"  마스터: {spark.sparkContext.master}\")            # 실행 모드 (local/cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f1cf9",
   "metadata": {},
   "source": [
    "Builder 패턴으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8937ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "설정된 SparkSession 생성 완료!\n",
      "  셔플 파티션: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 07:10:17 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 07:10:18 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# SparkSession 생성: 설정 포함 (실무 패턴)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 기존 세션 종료 (설정 변경을 위해)\n",
    "# spark.stop()\n",
    "\n",
    "# 새 세션 생성 (설정 포함)\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    # 애플리케이션 이름: Spark UI에서 식별용\n",
    "    .appName(\"PySpark-API-Basics-Configured\")\n",
    "\n",
    "    # 실행 환경 설정\n",
    "    # - \"local[*]\": 로컬 모드, 모든 코어 사용\n",
    "    # - \"spark://master:7077\": 클러스터 모드\n",
    "    .master(\"local[*]\")\n",
    "\n",
    "    # Executor 메모리: 각 워커가 사용할 메모리\n",
    "    .config(\"spark.executor.memory\", \"2g\")\n",
    "\n",
    "    # 셔플 파티션 수: groupBy, join 등에서 사용\n",
    "    # 기본값 200은 작은 데이터에 과도함 → 줄이면 성능 향상\n",
    "    .config(\"spark.sql.shuffle.partitions\", 50)\n",
    "\n",
    "    # 세션 생성 또는 기존 세션 반환\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"설정된 SparkSession 생성 완료!\")\n",
    "print(f\"  셔플 파티션: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71b2cfd",
   "metadata": {},
   "source": [
    "Part 3: 데이터 읽기 (Read) 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e611920e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/app/path/to/file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mheader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath/to/file\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 또는 단축형\u001b[39;00m\n\u001b[32m      7\u001b[39m spark.read.csv(\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:307\u001b[39m, in \u001b[36mDataFrameReader.load\u001b[39m\u001b[34m(self, path, format, schema, **options)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mself\u001b[39m.options(**options)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) != \u001b[38;5;28mlist\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    181\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    184\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/app/path/to/file."
     ]
    }
   ],
   "source": [
    "spark.read\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", True)\\\n",
    "    .load(\"path/to/file\")\n",
    "\n",
    "# 또는 단축형\n",
    "spark.read.csv(\"path\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05e50199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 데이터 생성 완료!\n",
      "  행 수: 100\n",
      "  컬럼: ['emp_id', 'name', 'department', 'salary', 'hire_date', 'age', 'is_manager']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 테스트 데이터 생성 (Pandas로 먼저 생성 후 파일 저장)\n",
    "# -----------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 재현 가능한 랜덤 시드 설정\n",
    "np.random.seed(42)\n",
    "\n",
    "# 샘플 데이터 생성: 직원 정보\n",
    "sample_data = pd.DataFrame({\n",
    "    \"emp_id\": [f\"E{i:03d}\" for i in range(1, 101)],          # E001 ~ E100\n",
    "    \"name\": [f\"Employee_{i}\" for i in range(1, 101)],        # 이름\n",
    "    \"department\": np.random.choice(                           # 부서 (랜덤)\n",
    "        [\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\"],\n",
    "        100\n",
    "    ),\n",
    "    \"salary\": np.random.randint(40000, 120000, 100),         # 연봉 (랜덤)\n",
    "    \"hire_date\": pd.date_range(\"2020-01-01\", periods=100, freq=\"7D\"),  # 입사일\n",
    "    \"age\": np.random.randint(25, 55, 100),                   # 나이 (랜덤)\n",
    "    \"is_manager\": np.random.choice([True, False], 100, p=[0.2, 0.8]),  # 매니저 여부\n",
    "})\n",
    "\n",
    "# 결측치 일부 추가 (실무 데이터처럼)\n",
    "sample_data.loc[5:10, \"salary\"] = None\n",
    "sample_data.loc[15:18, \"department\"] = None\n",
    "\n",
    "# 데이터 디렉토리 생성\n",
    "os.makedirs(\"/tmp/spark_tutorial\", exist_ok=True)\n",
    "sample_data['hire_date'] = sample_data['hire_date'].astype(\"datetime64[us]\")\n",
    "# 다양한 포맷으로 저장\n",
    "sample_data.to_csv(\"/tmp/spark_tutorial/employees.csv\", index=False)\n",
    "sample_data.to_parquet(\"/tmp/spark_tutorial/employees.parquet\", index=False)\n",
    "sample_data.to_json(\"/tmp/spark_tutorial/employees.json\", orient=\"records\", lines=True)\n",
    "\n",
    "print(\"테스트 데이터 생성 완료!\")\n",
    "print(f\"  행 수: {len(sample_data)}\")\n",
    "print(f\"  컬럼: {list(sample_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d988f",
   "metadata": {},
   "source": [
    "3-1. CSV 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec0be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CSV 스키마 ===\n",
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CSV 파일 읽기\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 방법 1: 기본 읽기 (헤더 있음, 스키마 자동 추론)\n",
    "# - header=True: 첫 번째 줄을 컬럼명으로 사용\n",
    "# - inferSchema=True: 데이터 타입 자동 추론 (숫자, 문자열 등)\n",
    "#   주의: inferSchema는 데이터를 한 번 더 읽어서 느림 (대용량에서)\n",
    "df_csv = spark.read.csv(\n",
    "    \"/tmp/spark_tutorial/employees.csv\",  # 파일 경로\n",
    "    header=True,                           # 첫 줄이 헤더인지\n",
    "    inferSchema=True                       # 타입 자동 추론\n",
    ")\n",
    "\n",
    "# 스키마(구조) 확인\n",
    "print(\"=== CSV 스키마 ===\")\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad9dfd",
   "metadata": {},
   "source": [
    "3-2. Parquet 파일 읽기 (실무 표준)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d960514c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Parquet 스키마 (자동 추론됨) ===\n",
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: timestamp_ntz (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Parquet 파일 읽기\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Parquet은 스키마가 파일에 포함되어 있어 옵션이 거의 필요 없음\n",
    "# inferSchema도 불필요 (자동으로 스키마 읽음)\n",
    "df_parquet = spark.read.parquet(\"/tmp/spark_tutorial/employees.parquet\")\n",
    "\n",
    "print(\"=== Parquet 스키마 (자동 추론됨) ===\")\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5acbc",
   "metadata": {},
   "source": [
    "3-3. JSON 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e9c280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== JSON 스키마 ===\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- hire_date: long (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      "\n",
      "+---+----------+------+----------+----------+----------+-------+\n",
      "|age|department|emp_id| hire_date|is_manager|      name| salary|\n",
      "+---+----------+------+----------+----------+----------+-------+\n",
      "| 28|        HR|  E001|1577836800|     false|Employee_1|92251.0|\n",
      "| 43|   Finance|  E002|1578441600|     false|Employee_2|62662.0|\n",
      "| 50| Marketing|  E003|1579046400|     false|Employee_3|48392.0|\n",
      "+---+----------+------+----------+----------+----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# JSON 파일 읽기\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# JSON Lines 형식 (한 줄에 하나의 JSON 객체)\n",
    "# - 기본 JSON: [{\"a\":1}, {\"a\":2}]  → 배열\n",
    "# - JSON Lines: {\"a\":1}\\n{\"a\":2}   → 줄바꿈으로 구분\n",
    "df_json = spark.read.json(\"/tmp/spark_tutorial/employees.json\")\n",
    "\n",
    "print(\"=== JSON 스키마 ===\")\n",
    "df_json.printSchema()\n",
    "df_json.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de44a86",
   "metadata": {},
   "source": [
    "3-4. 스키마 직접 지정 (권장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cafa36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 스키마 직접 지정 결과 ===\n",
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 스키마 직접 정의 후 읽기\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# StructType: 전체 스키마 (테이블 구조)\n",
    "# StructField(이름, 타입, nullable): 개별 컬럼 정의\n",
    "#   - 이름: 컬럼명\n",
    "#   - 타입: StringType(), IntegerType(), etc.\n",
    "#   - nullable: NULL 허용 여부 (True/False)\n",
    "\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, DoubleType, BooleanType, DateType\n",
    ")\n",
    "\n",
    "# 스키마 정의\n",
    "employee_schema = StructType([\n",
    "    StructField(\"emp_id\", StringType(), False),       # 필수 (nullable=False)\n",
    "    StructField(\"name\", StringType(), True),          # NULL 허용\n",
    "    StructField(\"department\", StringType(), True),    # NULL 허용\n",
    "    StructField(\"salary\", IntegerType(), True),       # 정수형\n",
    "    StructField(\"hire_date\", DateType(), True),       # 날짜형\n",
    "    StructField(\"age\", IntegerType(), True),          # 정수형\n",
    "    StructField(\"is_manager\", BooleanType(), True),   # 불린형\n",
    "])\n",
    "\n",
    "# 정의한 스키마로 CSV 읽기 (inferSchema 대신)\n",
    "df_with_schema = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(employee_schema)                          # 스키마 지정\n",
    "    .csv(\"/tmp/spark_tutorial/employees.csv\")\n",
    ")\n",
    "\n",
    "print(\"=== 스키마 직접 지정 결과 ===\")\n",
    "df_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4ccbd",
   "metadata": {},
   "source": [
    "Part 4: 데이터 쓰기 (Write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62b0b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 데이터 쓰기: Parquet (실무 표준)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# mode(\"overwrite\"): 기존 파일 덮어쓰기\n",
    "# 주의: 실수로 데이터 날릴 수 있으니 경로 확인!\n",
    "df_parquet.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/spark_tutorial/output/employees_output.parquet\")\n",
    "\n",
    "print(\"Parquet 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "762dbf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파티셔닝 저장 완료!\n",
      "\n",
      "저장된 폴더 구조:\n",
      "/tmp/spark_tutorial/output/employees_partitioned\n",
      "/tmp/spark_tutorial/output/employees_partitioned/department=__HIVE_DEFAULT_PARTITION__\n",
      "/tmp/spark_tutorial/output/employees_partitioned/department=Engineering\n",
      "/tmp/spark_tutorial/output/employees_partitioned/department=Sales\n",
      "/tmp/spark_tutorial/output/employees_partitioned/department=HR\n",
      "/tmp/spark_tutorial/output/employees_partitioned/department=Finance\n",
      "/tmp/spark_tutorial/output/employees_partitioned/department=Marketing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 데이터 쓰기: 파티셔닝 (대용량 데이터 필수)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# partitionBy(): 지정한 컬럼 값으로 폴더를 나눠서 저장\n",
    "# 장점:\n",
    "#   - 쿼리 시 필요한 파티션만 읽음 (Partition Pruning)\n",
    "#   - department=\"Engineering\" 조건 시 해당 폴더만 읽음\n",
    "df_parquet.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"department\") \\\n",
    "    .parquet(\"/tmp/spark_tutorial/output/employees_partitioned\")\n",
    "\n",
    "print(\"파티셔닝 저장 완료!\")\n",
    "print(\"\\n저장된 폴더 구조:\")\n",
    "\n",
    "# 폴더 구조 확인 (Bash 명령)\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"find\", \"/tmp/spark_tutorial/output/employees_partitioned\", \"-type\", \"d\"],\n",
    "    capture_output=True, text=True\n",
    ")\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77257feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 파일로 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 데이터 쓰기: 파일 개수 조절\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 기본적으로 파티션 수만큼 파일이 생성됨\n",
    "# coalesce(n): 파일 개수를 n개로 줄임 (셔플 없음, 감소만 가능)\n",
    "# repartition(n): 파일 개수를 n개로 변경 (셔플 발생, 증가/감소 가능)\n",
    "\n",
    "# 작은 파일 여러 개 → 큰 파일 하나로 합치기\n",
    "df_parquet.coalesce(1) \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/tmp/spark_tutorial/output/employees_single\")\n",
    "\n",
    "print(\"단일 파일로 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe4b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 데이터 쓰기: CSV (다른 시스템 연동용)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# CSV로 저장 (다른 도구와 연동 시)\n",
    "df_parquet.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/tmp/spark_tutorial/output/employees_output.csv\")\n",
    "\n",
    "print(\"CSV 저장 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1a38d",
   "metadata": {},
   "source": [
    "Part 5: DataFrame 확인 (Inspection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98fb0d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 스키마 확인 ===\n",
      "root\n",
      " |-- emp_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: double (nullable = true)\n",
      " |-- hire_date: date (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- is_manager: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 스키마 확인: printSchema()\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# printSchema(): 컬럼명, 데이터 타입, nullable 여부를 트리 형태로 출력\n",
    "# - root: 최상위\n",
    "# - |-- 컬럼명: 타입 (nullable = true/false)\n",
    "print(\"=== 스키마 확인 ===\")\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c752cd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 기본 show() ===\n",
      "+------+----------+----------+--------+----------+---+----------+\n",
      "|emp_id|      name|department|  salary| hire_date|age|is_manager|\n",
      "+------+----------+----------+--------+----------+---+----------+\n",
      "|  E001|Employee_1|        HR| 92251.0|2020-01-01| 28|     false|\n",
      "|  E002|Employee_2|   Finance| 62662.0|2020-01-08| 43|     false|\n",
      "|  E003|Employee_3| Marketing| 48392.0|2020-01-15| 50|     false|\n",
      "|  E004|Employee_4|   Finance| 70535.0|2020-01-22| 27|     false|\n",
      "|  E005|Employee_5|   Finance|118603.0|2020-01-29| 43|     false|\n",
      "+------+----------+----------+--------+----------+---+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "=== truncate=False (잘림 없이) ===\n",
      "+------+----------+----------+-------+----------+---+----------+\n",
      "|emp_id|name      |department|salary |hire_date |age|is_manager|\n",
      "+------+----------+----------+-------+----------+---+----------+\n",
      "|E001  |Employee_1|HR        |92251.0|2020-01-01|28 |false     |\n",
      "|E002  |Employee_2|Finance   |62662.0|2020-01-08|43 |false     |\n",
      "|E003  |Employee_3|Marketing |48392.0|2020-01-15|50 |false     |\n",
      "+------+----------+----------+-------+----------+---+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "=== vertical=True (세로 출력) ===\n",
      "-RECORD 0----------------\n",
      " emp_id     | E001       \n",
      " name       | Employee_1 \n",
      " department | HR         \n",
      " salary     | 92251.0    \n",
      " hire_date  | 2020-01-01 \n",
      " age        | 28         \n",
      " is_manager | false      \n",
      "-RECORD 1----------------\n",
      " emp_id     | E002       \n",
      " name       | Employee_2 \n",
      " department | Finance    \n",
      " salary     | 62662.0    \n",
      " hire_date  | 2020-01-08 \n",
      " age        | 43         \n",
      " is_manager | false      \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 데이터 미리보기: show()\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# show(): 기본 20행 출력\n",
    "# show(n): n행 출력\n",
    "# show(n, truncate=False): 컬럼 내용 잘리지 않게 출력\n",
    "# show(n, truncate=10): 10자까지만 출력\n",
    "# show(vertical=True): 세로로 출력 (컬럼 많을 때)\n",
    "\n",
    "print(\"=== 기본 show() ===\")\n",
    "df_csv.show(5)  # 5행만 출력\n",
    "\n",
    "print(\"=== truncate=False (잘림 없이) ===\")\n",
    "df_csv.show(3, truncate=False)\n",
    "\n",
    "print(\"=== vertical=True (세로 출력) ===\")\n",
    "df_csv.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba94d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "행 수: 100\n",
      "컬럼 목록: ['emp_id', 'name', 'department', 'salary', 'hire_date', 'age', 'is_manager']\n",
      "컬럼 수: 7\n",
      "컬럼별 타입: [('emp_id', 'string'), ('name', 'string'), ('department', 'string'), ('salary', 'double'), ('hire_date', 'date'), ('age', 'int'), ('is_manager', 'boolean')]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 행/컬럼 수 확인\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# count(): 전체 행 수 반환 (Action이라 실행됨)\n",
    "row_count = df_csv.count()\n",
    "print(f\"행 수: {row_count:,}\")\n",
    "\n",
    "# columns: 컬럼명 리스트 반환 (속성)\n",
    "col_list = df_csv.columns\n",
    "print(f\"컬럼 목록: {col_list}\")\n",
    "print(f\"컬럼 수: {len(col_list)}\")\n",
    "\n",
    "# dtypes: (컬럼명, 타입) 튜플 리스트 반환\n",
    "print(f\"컬럼별 타입: {df_csv.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b93517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 통계 요약 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/19 07:40:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----------+-----------+------------------+-----------------+\n",
      "|summary|emp_id|       name| department|            salary|              age|\n",
      "+-------+------+-----------+-----------+------------------+-----------------+\n",
      "|  count|   100|        100|         96|                94|              100|\n",
      "|   mean|  NULL|       NULL|       NULL| 80864.46808510639|            38.83|\n",
      "| stddev|  NULL|       NULL|       NULL|22171.614668892573|9.084402216786948|\n",
      "|    min|  E001| Employee_1|Engineering|           40206.0|               25|\n",
      "|    max|  E100|Employee_99|      Sales|          119309.0|               54|\n",
      "+-------+------+-----------+-----------+------------------+-----------------+\n",
      "\n",
      "=== salary 컬럼 통계 ===\n",
      "+-------+------------------+-----------------+\n",
      "|summary|            salary|              age|\n",
      "+-------+------------------+-----------------+\n",
      "|  count|                94|              100|\n",
      "|   mean| 80864.46808510639|            38.83|\n",
      "| stddev|22171.614668892573|9.084402216786948|\n",
      "|    min|           40206.0|               25|\n",
      "|    max|          119309.0|               54|\n",
      "+-------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 통계 요약: describe()\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# describe(): 숫자형 컬럼의 기본 통계 (count, mean, stddev, min, max)\n",
    "# 반환값이 DataFrame이므로 show() 호출 필요\n",
    "print(\"=== 통계 요약 ===\")\n",
    "df_csv.describe().show()\n",
    "\n",
    "# 특정 컬럼만 통계\n",
    "print(\"=== salary 컬럼 통계 ===\")\n",
    "df_csv.describe(\"salary\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48476f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 행: [Row(emp_id='E001', name='Employee_1', department='HR', salary=92251.0, hire_date=datetime.date(2020, 1, 1), age=28, is_manager=False)]\n",
      "first(): Row(emp_id='E001', name='Employee_1', department='HR', salary=92251.0, hire_date=datetime.date(2020, 1, 1), age=28, is_manager=False)\n",
      "take(3): 3개 Row\n",
      "부서 종류: 6개\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 유용한 추가 메서드들\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# head(n): 처음 n행을 Row 객체 리스트로 반환\n",
    "first_row = df_csv.head(1)\n",
    "print(f\"첫 번째 행: {first_row}\")\n",
    "\n",
    "# first(): 첫 번째 행을 Row 객체로 반환\n",
    "first = df_csv.first()\n",
    "print(f\"first(): {first}\")\n",
    "\n",
    "# take(n): head(n)과 동일\n",
    "top3 = df_csv.take(3)\n",
    "print(f\"take(3): {len(top3)}개 Row\")\n",
    "\n",
    "# collect(): 전체 데이터를 Driver로 가져옴 (주의: 대용량 시 OOM!)\n",
    "# 작은 데이터에서만 사용\n",
    "# all_rows = df_csv.collect()\n",
    "\n",
    "# distinct(): 중복 제거된 행 수\n",
    "dept_count = df_csv.select(\"department\").distinct().count()\n",
    "print(f\"부서 종류: {dept_count}개\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
