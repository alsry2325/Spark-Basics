{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ae8ebe62",
      "metadata": {
        "id": "ae8ebe62"
      },
      "source": [
        "# 1교시 - PySpark DataFrame API 종합 과제\n",
        "\n",
        "---\n",
        "\n",
        "## 과제 개요\n",
        "\n",
        "이 과제에서는 실제 데이터를 활용하여 PySpark DataFrame API를 종합적으로 실습합니다.\n",
        "\n",
        "**사용 데이터:**\n",
        "- **Tips 데이터셋**: 레스토랑 팁 기록 데이터 (244건)\n",
        "- **Titanic 데이터셋**: 타이타닉 승객 생존 데이터 (891건)\n",
        "\n",
        "**학습 목표:**\n",
        "- 컬럼 선택, 추가, 수정 (`select`, `withColumn`)\n",
        "- 필터링 (`filter`, `where`, `isin`)\n",
        "- 조건부 값 설정 (`when`, `otherwise`)\n",
        "- 정렬, 중복 제거, 제한\n",
        "- 그룹별 집계 (`groupBy`, `agg`)\n",
        "- 피벗 테이블 (`pivot`)\n",
        "- 조인 (`join`)\n",
        "- NULL 처리 (`dropna`, `fillna`, `coalesce`)\n",
        "- Window 함수 (순위, 누적합, 이동평균)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f2f3394",
      "metadata": {
        "id": "8f2f3394"
      },
      "source": [
        "## 환경 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cc793e",
      "metadata": {
        "id": "08cc793e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "26/01/21 10:55:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "데이터 로드 완료!\n",
            "============================================================\n",
            "\n",
            "[Tips 데이터]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- 행 수: 244\n",
            "- 컬럼: ['total_bill', 'tip', 'sex', 'smoker', 'day', 'time', 'size']\n",
            "+----------+----+------+------+---+------+----+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|\n",
            "+----------+----+------+------+---+------+----+\n",
            "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
            "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
            "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
            "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
            "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
            "+----------+----+------+------+---+------+----+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "[Titanic 데이터]\n",
            "- 행 수: 891\n",
            "- 컬럼: ['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town', 'alive', 'alone']\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked|class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S|Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C|First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S|Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S|First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S|Third|  man|      true| NaN|Southampton|   no| true|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/21 10:55:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 환경 설정: PySpark 및 데이터 로드\n",
        "# -----------------------------------------------------------------------------\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, when, count, sum, avg, min, max,\n",
        "    round as spark_round, expr,\n",
        "    upper, lower, trim, concat, concat_ws, substring, length,\n",
        "    to_date, year, month, dayofmonth, datediff, current_date,\n",
        "    row_number, rank, dense_rank, lag, lead,\n",
        "    coalesce, countDistinct, first, last,\n",
        ")\n",
        "from pyspark.sql.window import Window\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# SparkSession 생성\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PySpark-DataFrame-Exercise\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 실제 데이터 로드: Seaborn 내장 데이터셋\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Tips 데이터셋: 레스토랑 팁 기록\n",
        "tips_pdf = sns.load_dataset(\"tips\")\n",
        "df_tips = spark.createDataFrame(tips_pdf)\n",
        "\n",
        "# Titanic 데이터셋: 타이타닉 승객 생존 기록\n",
        "titanic_pdf = sns.load_dataset(\"titanic\")\n",
        "df_titanic = spark.createDataFrame(titanic_pdf)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"데이터 로드 완료!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n[Tips 데이터]\")\n",
        "print(f\"- 행 수: {df_tips.count()}\")\n",
        "print(f\"- 컬럼: {df_tips.columns}\")\n",
        "df_tips.show(5)\n",
        "\n",
        "print(f\"\\n[Titanic 데이터]\")\n",
        "print(f\"- 행 수: {df_titanic.count()}\")\n",
        "print(f\"- 컬럼: {df_titanic.columns}\")\n",
        "df_titanic.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "19581c7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/site-packages (80.9.0)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/site-packages (from seaborn) (2.4.1)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/site-packages (from seaborn) (2.3.3)\n",
            "Collecting matplotlib!=3.6.1,>=3.4 (from seaborn)\n",
            "  Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (114 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Collecting pillow>=8 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting pyparsing>=3 (from matplotlib!=3.6.1,>=3.4->seaborn)\n",
            "  Downloading pyparsing-3.3.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Downloading matplotlib-3.10.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.61.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
            "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
            "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.1.0 pyparsing-3.3.2 seaborn-0.13.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install seaborn setuptools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c21d6d6",
      "metadata": {
        "id": "6c21d6d6"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 1: 데이터 탐색\n",
        "\n",
        "Tips 데이터의 스키마를 확인하고 기본 통계를 출력하세요.\n",
        "\n",
        "### 문제 1-1: 스키마 출력\n",
        "\n",
        "Tips 데이터의 스키마를 출력하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "DataFrame의 `printSchema()` 메서드를 호출하면 됩니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.printSchema()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d1f2238",
      "metadata": {
        "id": "4d1f2238"
      },
      "outputs": [],
      "source": [
        "# [문제 1-1] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be95f20",
      "metadata": {
        "id": "5be95f20"
      },
      "source": [
        "### 문제 1-2: 기본 통계 확인\n",
        "\n",
        "Tips 데이터의 기본 통계(count, mean, stddev, min, max)를 확인하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`describe()` 메서드는 숫자형 컬럼에 대한 통계를 반환합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.describe().show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95de046",
      "metadata": {
        "id": "b95de046"
      },
      "outputs": [],
      "source": [
        "# [문제 1-2] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aacf51c0",
      "metadata": {
        "id": "aacf51c0"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: 컬럼 선택 및 조작\n",
        "\n",
        "### 문제 2-1: 컬럼 선택\n",
        "\n",
        "Tips 데이터에서 `total_bill`, `tip`, `day` 컬럼만 선택하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`select(\"컬럼1\", \"컬럼2\", \"컬럼3\")` 형태로 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.select(\"total_bill\", \"tip\", \"day\").show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4612d9",
      "metadata": {
        "id": "5e4612d9"
      },
      "outputs": [],
      "source": [
        "# [문제 2-1] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69ce820e",
      "metadata": {
        "id": "69ce820e"
      },
      "source": [
        "### 문제 2-2: 새 컬럼 추가 (팁 비율)\n",
        "\n",
        "`tip`을 `total_bill`로 나눈 `tip_rate` 컬럼을 추가하세요.\n",
        "소수점 2자리로 반올림하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "- `withColumn()`으로 새 컬럼 추가\n",
        "- `spark_round(값, 자릿수)`로 반올림\n",
        "- `col(\"컬럼\") / col(\"컬럼\")`으로 나눗셈\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_with_rate = df_tips.withColumn(\n",
        "    \"tip_rate\",\n",
        "    spark_round(col(\"tip\") / col(\"total_bill\"), 2)\n",
        ")\n",
        "df_with_rate.select(\"total_bill\", \"tip\", \"tip_rate\").show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "940b0308",
      "metadata": {
        "id": "940b0308"
      },
      "outputs": [],
      "source": [
        "# [문제 2-2] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ab5783",
      "metadata": {
        "id": "a5ab5783"
      },
      "source": [
        "### 문제 2-3: 컬럼 이름 변경\n",
        "\n",
        "`total_bill` 컬럼을 `bill_amount`로, `tip` 컬럼을 `tip_amount`로 변경하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`withColumnRenamed(\"기존이름\", \"새이름\")`을 체이닝합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_renamed = df_tips \\\n",
        "    .withColumnRenamed(\"total_bill\", \"bill_amount\") \\\n",
        "    .withColumnRenamed(\"tip\", \"tip_amount\")\n",
        "df_renamed.show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b4366c0",
      "metadata": {
        "id": "5b4366c0"
      },
      "outputs": [],
      "source": [
        "# [문제 2-3] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a7e67f",
      "metadata": {
        "id": "53a7e67f"
      },
      "source": [
        "### 문제 2-4: 상수 컬럼 추가\n",
        "\n",
        "모든 행에 `currency` 컬럼을 추가하고 값을 `\"USD\"`로 설정하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`lit(\"값\")`을 사용하여 상수값을 Column으로 변환합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_with_currency = df_tips.withColumn(\"currency\", lit(\"USD\"))\n",
        "df_with_currency.select(\"total_bill\", \"currency\").show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615d6083",
      "metadata": {
        "id": "615d6083"
      },
      "outputs": [],
      "source": [
        "# [문제 2-4] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22804510",
      "metadata": {
        "id": "22804510"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: 필터링\n",
        "\n",
        "### 문제 3-1: 단순 필터링\n",
        "\n",
        "Tips 데이터에서 `total_bill`이 20 이상인 행만 필터링하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`filter(col(\"컬럼\") >= 값)` 또는 `filter(\"컬럼 >= 값\")` 형태로 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.filter(col(\"total_bill\") >= 20).show(5)\n",
        "# 또는\n",
        "df_tips.filter(\"total_bill >= 20\").show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a555ff01",
      "metadata": {
        "id": "a555ff01"
      },
      "outputs": [],
      "source": [
        "# [문제 3-1] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc298f6",
      "metadata": {
        "id": "dbc298f6"
      },
      "source": [
        "### 문제 3-2: AND 조건 필터링\n",
        "\n",
        "`total_bill`이 20 이상이고 `tip`이 3 이상인 행만 필터링하세요.\n",
        "\n",
        "**주의:** AND 조건에서는 각 조건을 괄호로 감싸야 합니다!\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`(조건1) & (조건2)` 형태로 AND 조건을 작성합니다.\n",
        "괄호를 반드시 사용해야 합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.filter(\n",
        "    (col(\"total_bill\") >= 20) & (col(\"tip\") >= 3)\n",
        ").show(5)\n",
        "# 또는 SQL 스타일\n",
        "df_tips.filter(\"total_bill >= 20 AND tip >= 3\").show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d36007e",
      "metadata": {
        "id": "6d36007e"
      },
      "outputs": [],
      "source": [
        "# [문제 3-2] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac10580",
      "metadata": {
        "id": "cac10580"
      },
      "source": [
        "### 문제 3-3: OR 조건 필터링\n",
        "\n",
        "`day`가 \"Sat\" 또는 \"Sun\"인 행만 필터링하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "방법 1: `(조건1) | (조건2)` 형태\n",
        "방법 2: `col(\"컬럼\").isin(값1, 값2)` 형태\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "# 방법 1: OR 연산자\n",
        "df_tips.filter(\n",
        "    (col(\"day\") == \"Sat\") | (col(\"day\") == \"Sun\")\n",
        ").show(5)\n",
        "\n",
        "# 방법 2: isin() 사용 (권장)\n",
        "df_tips.filter(col(\"day\").isin(\"Sat\", \"Sun\")).show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30225f9d",
      "metadata": {
        "id": "30225f9d"
      },
      "outputs": [],
      "source": [
        "# [문제 3-3] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a45271",
      "metadata": {
        "id": "a6a45271"
      },
      "source": [
        "### 문제 3-4: 문자열 필터링\n",
        "\n",
        "Titanic 데이터에서 `embark_town`이 \"South\"로 시작하는 승객을 필터링하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`col(\"컬럼\").startswith(\"문자열\")` 메서드를 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_titanic.filter(\n",
        "    col(\"embark_town\").startswith(\"South\")\n",
        ").select(\"survived\", \"name\", \"embark_town\").show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad68148c",
      "metadata": {
        "id": "ad68148c"
      },
      "outputs": [],
      "source": [
        "# [문제 3-4] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d17ec8ad",
      "metadata": {
        "id": "d17ec8ad"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: 조건부 값 설정 (when / otherwise)\n",
        "\n",
        "### 문제 4-1: 단순 조건 분기\n",
        "\n",
        "Tips 데이터에 `tip_quality` 컬럼을 추가하세요.\n",
        "- `tip`이 5 이상이면 \"Good\"\n",
        "- 그 외에는 \"Normal\"\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`when(조건, 값).otherwise(기본값)` 구조를 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_quality = df_tips.withColumn(\n",
        "    \"tip_quality\",\n",
        "    when(col(\"tip\") >= 5, \"Good\").otherwise(\"Normal\")\n",
        ")\n",
        "df_quality.select(\"tip\", \"tip_quality\").show(10)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f73ededd",
      "metadata": {
        "id": "f73ededd"
      },
      "outputs": [],
      "source": [
        "# [문제 4-1] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e1ad13a",
      "metadata": {
        "id": "8e1ad13a"
      },
      "source": [
        "### 문제 4-2: 다중 조건 분기\n",
        "\n",
        "Tips 데이터에 `bill_category` 컬럼을 추가하세요.\n",
        "- `total_bill` >= 30: \"High\"\n",
        "- `total_bill` >= 20: \"Medium\"\n",
        "- 그 외: \"Low\"\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`when().when().otherwise()` 형태로 체이닝합니다.\n",
        "첫 번째로 참인 조건에서 멈춥니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_category = df_tips.withColumn(\n",
        "    \"bill_category\",\n",
        "    when(col(\"total_bill\") >= 30, \"High\")\n",
        "    .when(col(\"total_bill\") >= 20, \"Medium\")\n",
        "    .otherwise(\"Low\")\n",
        ")\n",
        "df_category.select(\"total_bill\", \"bill_category\").show(10)\n",
        "\n",
        "# 카테고리별 건수 확인\n",
        "df_category.groupBy(\"bill_category\").count().show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be59cb68",
      "metadata": {
        "id": "be59cb68"
      },
      "outputs": [],
      "source": [
        "# [문제 4-2] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8568d2ee",
      "metadata": {
        "id": "8568d2ee"
      },
      "source": [
        "### 문제 4-3: 복합 조건 분기\n",
        "\n",
        "Titanic 데이터에 `priority` 컬럼을 추가하세요.\n",
        "- 1등석(`pclass` == 1)이면서 여성(`sex` == \"female\"): \"Highest\"\n",
        "- 여성: \"High\"\n",
        "- 어린이(`age` < 12): \"High\"\n",
        "- 그 외: \"Normal\"\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`when()` 안에서 AND 조건은 `&`를 사용하고 괄호로 감쌉니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_priority = df_titanic.withColumn(\n",
        "    \"priority\",\n",
        "    when((col(\"pclass\") == 1) & (col(\"sex\") == \"female\"), \"Highest\")\n",
        "    .when(col(\"sex\") == \"female\", \"High\")\n",
        "    .when(col(\"age\") < 12, \"High\")\n",
        "    .otherwise(\"Normal\")\n",
        ")\n",
        "df_priority.select(\"pclass\", \"sex\", \"age\", \"priority\").show(10)\n",
        "\n",
        "# 우선순위별 생존율 확인\n",
        "df_priority.groupBy(\"priority\").agg(\n",
        "    count(\"*\").alias(\"total\"),\n",
        "    sum(\"survived\").alias(\"survived\"),\n",
        "    spark_round(avg(\"survived\"), 2).alias(\"survival_rate\")\n",
        ").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04f0691",
      "metadata": {
        "id": "a04f0691"
      },
      "outputs": [],
      "source": [
        "# [문제 4-3] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03aabaa0",
      "metadata": {
        "id": "03aabaa0"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5: 정렬, 중복 제거, 제한\n",
        "\n",
        "### 문제 5-1: 정렬\n",
        "\n",
        "Tips 데이터를 `total_bill` 내림차순으로 정렬하고 상위 10개를 출력하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`orderBy(col(\"컬럼\").desc())`로 내림차순 정렬합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.orderBy(col(\"total_bill\").desc()).show(10)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "04c9924e",
      "metadata": {
        "id": "04c9924e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+------+------+----+------+----+\n",
            "|total_bill| tip|   sex|smoker| day|  time|size|\n",
            "+----------+----+------+------+----+------+----+\n",
            "|     50.81|10.0|  Male|   Yes| Sat|Dinner|   3|\n",
            "|     48.33| 9.0|  Male|    No| Sat|Dinner|   4|\n",
            "|     48.27|6.73|  Male|    No| Sat|Dinner|   4|\n",
            "|     48.17| 5.0|  Male|    No| Sun|Dinner|   6|\n",
            "|     45.35| 3.5|  Male|   Yes| Sun|Dinner|   3|\n",
            "|      44.3| 2.5|Female|   Yes| Sat|Dinner|   3|\n",
            "|     43.11| 5.0|Female|   Yes|Thur| Lunch|   4|\n",
            "|     41.19| 5.0|  Male|    No|Thur| Lunch|   5|\n",
            "|     40.55| 3.0|  Male|   Yes| Sun|Dinner|   2|\n",
            "|     40.17|4.73|  Male|   Yes| Fri|Dinner|   4|\n",
            "+----------+----+------+------+----+------+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 5-1] 여기에 코드를 작성하세요\n",
        "df_tips.orderBy(col(\"total_bill\").desc()).show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e07be9",
      "metadata": {
        "id": "76e07be9"
      },
      "source": [
        "### 문제 5-2: 여러 컬럼 정렬\n",
        "\n",
        "Tips 데이터를 `day` 오름차순, `total_bill` 내림차순으로 정렬하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`orderBy(col(\"컬럼1\").asc(), col(\"컬럼2\").desc())` 형태로 여러 컬럼 정렬을 지정합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.orderBy(\n",
        "    col(\"day\").asc(),\n",
        "    col(\"total_bill\").desc()\n",
        ").show(15)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0999d731",
      "metadata": {
        "id": "0999d731"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+------+------+---+------+----+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|\n",
            "+----------+----+------+------+---+------+----+\n",
            "|     40.17|4.73|  Male|   Yes|Fri|Dinner|   4|\n",
            "|     28.97| 3.0|  Male|   Yes|Fri|Dinner|   2|\n",
            "|     27.28| 4.0|  Male|   Yes|Fri|Dinner|   2|\n",
            "|     22.75|3.25|Female|    No|Fri|Dinner|   2|\n",
            "|     22.49| 3.5|  Male|    No|Fri|Dinner|   2|\n",
            "|     21.01| 3.0|  Male|   Yes|Fri|Dinner|   2|\n",
            "|     16.32| 4.3|Female|   Yes|Fri|Dinner|   2|\n",
            "|     16.27| 2.5|Female|   Yes|Fri| Lunch|   2|\n",
            "|     15.98| 3.0|Female|    No|Fri| Lunch|   3|\n",
            "|     15.38| 3.0|Female|   Yes|Fri|Dinner|   2|\n",
            "+----------+----+------+------+---+------+----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 5-2] 여기에 코드를 작성하세요\n",
        "df_tips.orderBy(\n",
        "    col(\"day\").asc(),   \n",
        "    col(\"total_bill\").desc()      \n",
        ").show(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d878cc68",
      "metadata": {
        "id": "d878cc68"
      },
      "source": [
        "### 문제 5-3: 고유값 추출\n",
        "\n",
        "Tips 데이터에서 `day`의 고유값 목록을 추출하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`select(\"컬럼\").distinct()`를 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.select(\"day\").distinct().show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "03eab208",
      "metadata": {
        "id": "03eab208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+\n",
            "| day|\n",
            "+----+\n",
            "| Sun|\n",
            "| Sat|\n",
            "|Thur|\n",
            "| Fri|\n",
            "+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 5-3] 여기에 코드를 작성하세요\n",
        "df_tips.select(\"day\").distinct().show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc129728",
      "metadata": {
        "id": "fc129728"
      },
      "source": [
        "### 문제 5-4: Top N 추출\n",
        "\n",
        "Tips 데이터에서 `tip`이 가장 높은 5건을 추출하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`orderBy().limit()` 조합을 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.orderBy(col(\"tip\").desc()).limit(5).show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "67a4545e",
      "metadata": {
        "id": "67a4545e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+----+------+----+------+----+\n",
            "|total_bill| tip| sex|smoker| day|  time|size|\n",
            "+----------+----+----+------+----+------+----+\n",
            "|     50.81|10.0|Male|   Yes| Sat|Dinner|   3|\n",
            "|     48.33| 9.0|Male|    No| Sat|Dinner|   4|\n",
            "|     39.42|7.58|Male|    No| Sat|Dinner|   4|\n",
            "|     48.27|6.73|Male|    No| Sat|Dinner|   4|\n",
            "|      34.3| 6.7|Male|    No|Thur| Lunch|   6|\n",
            "+----------+----+----+------+----+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 5-4] 여기에 코드를 작성하세요\n",
        "df_tips.orderBy(col(\"tip\").desc()).limit(5).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b56520",
      "metadata": {
        "id": "f0b56520"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: 그룹별 집계\n",
        "\n",
        "### 문제 6-1: 단일 컬럼 그룹 집계\n",
        "\n",
        "Tips 데이터를 `day`별로 그룹화하여 다음을 계산하세요:\n",
        "- 건수 (`count`)\n",
        "- 평균 팁 (`avg_tip`)\n",
        "- 총 매출 (`total_bill_sum`)\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`groupBy(\"컬럼\").agg(집계함수.alias(\"별칭\"), ...)`\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.groupBy(\"day\").agg(\n",
        "    count(\"*\").alias(\"count\"),\n",
        "    spark_round(avg(\"tip\"), 2).alias(\"avg_tip\"),\n",
        "    sum(\"total_bill\").alias(\"total_bill_sum\")\n",
        ").orderBy(\"day\").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ab911292",
      "metadata": {
        "id": "ab911292"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+------+------------------+\n",
            "| day|건수|평균팁|            총매출|\n",
            "+----+----+------+------------------+\n",
            "| Sun|  76|  3.26|1627.1599999999996|\n",
            "| Sat|  87|  2.99|1778.3999999999999|\n",
            "|Thur|  62|  2.77|           1096.33|\n",
            "| Fri|  19|  2.73|            325.88|\n",
            "+----+----+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 6-1] 여기에 코드를 작성하세요\n",
        "total_stats = df_tips.groupBy(\"day\").agg(\n",
        "    count(\"*\").alias(\"건수\"),                    \n",
        "    spark_round(avg(\"tip\"),2).alias(\"평균팁\"),               # 급여 평균\n",
        "    sum(\"total_bill\").alias(\"총매출\"),               # 최소 급여\n",
        "\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487b804e",
      "metadata": {
        "id": "487b804e"
      },
      "source": [
        "### 문제 6-2: 다중 컬럼 그룹 집계\n",
        "\n",
        "Tips 데이터를 `day`와 `time`으로 그룹화하여 건수와 평균 팁을 계산하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`groupBy(\"컬럼1\", \"컬럼2\")`로 여러 컬럼 그룹화가 가능합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.groupBy(\"day\", \"time\").agg(\n",
        "    count(\"*\").alias(\"count\"),\n",
        "    spark_round(avg(\"tip\"), 2).alias(\"avg_tip\")\n",
        ").orderBy(\"day\", \"time\").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b7b3b2",
      "metadata": {
        "id": "42b7b3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+----+------+\n",
            "| day|  time|건수|평균팁|\n",
            "+----+------+----+------+\n",
            "| Sun|Dinner|  76|  3.26|\n",
            "| Sat|Dinner|  87|  2.99|\n",
            "|Thur| Lunch|  61|  2.77|\n",
            "| Fri|Dinner|  12|  2.94|\n",
            "| Fri| Lunch|   7|  2.38|\n",
            "|Thur|Dinner|   1|   3.0|\n",
            "+----+------+----+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 6-2] 여기에 코드를 작성하세요\n",
        "df_tips.groupBy(\"day\", \"time\").agg(\n",
        "    count(\"*\").alias(\"건수\"),\n",
        "    spark_round(avg(\"tip\"), 2).alias(\"평균팁\"),\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad43f3df",
      "metadata": {
        "id": "ad43f3df"
      },
      "source": [
        "### 문제 6-3: 조건부 집계\n",
        "\n",
        "Tips 데이터에서 `day`별로 다음을 계산하세요:\n",
        "- 전체 건수\n",
        "- 흡연자(`smoker` == \"Yes\") 건수\n",
        "- 비흡연자 건수\n",
        "- 흡연자 비율 (%)\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`count(when(조건, 1))`으로 조건에 맞는 행만 카운트합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.groupBy(\"day\").agg(\n",
        "    count(\"*\").alias(\"total\"),\n",
        "    count(when(col(\"smoker\") == \"Yes\", 1)).alias(\"smoker_cnt\"),\n",
        "    count(when(col(\"smoker\") == \"No\", 1)).alias(\"non_smoker_cnt\"),\n",
        "    spark_round(\n",
        "        count(when(col(\"smoker\") == \"Yes\", 1)) / count(\"*\") * 100, 1\n",
        "    ).alias(\"smoker_rate\")\n",
        ").orderBy(\"day\").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "b0eba286",
      "metadata": {
        "id": "b0eba286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------+--------+----------+----------+\n",
            "| day|전체건수|흡연자수|비흡연자수|흡연자비율|\n",
            "+----+--------+--------+----------+----------+\n",
            "| Sun|      76|      19|        57|      25.0|\n",
            "| Sat|      87|      42|        45|      48.3|\n",
            "|Thur|      62|      17|        45|      27.4|\n",
            "| Fri|      19|      15|         4|      78.9|\n",
            "+----+--------+--------+----------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 6-3] 여기에 코드를 작성하세요\n",
        "\n",
        "df_tips.groupBy(\"day\").agg(\n",
        "    count(\"*\").alias(\"전체건수\"),\n",
        "    count(when(col(\"smoker\") == \"Yes\",1)).alias(\"흡연자수\"),\n",
        "    count(when(col(\"smoker\") == \"No\",1)).alias(\"비흡연자수\"),\n",
        "    spark_round(\n",
        "        count(when(col(\"smoker\") == \"Yes\",1)) / count(\"*\")* 100, 1\n",
        "        ).alias(\"흡연자비율\")\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af5ae9c1",
      "metadata": {
        "id": "af5ae9c1"
      },
      "source": [
        "### 문제 6-4: Titanic 생존 분석\n",
        "\n",
        "Titanic 데이터에서 `pclass`(객실 등급)별로 다음을 계산하세요:\n",
        "- 총 승객 수\n",
        "- 생존자 수\n",
        "- 사망자 수\n",
        "- 생존율 (%)\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`survived` 컬럼은 0(사망) 또는 1(생존)입니다.\n",
        "`sum(\"survived\")`는 생존자 수가 됩니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_titanic.groupBy(\"pclass\").agg(\n",
        "    count(\"*\").alias(\"total\"),\n",
        "    sum(\"survived\").alias(\"survived\"),\n",
        "    count(when(col(\"survived\") == 0, 1)).alias(\"died\"),\n",
        "    spark_round(avg(\"survived\") * 100, 1).alias(\"survival_rate\")\n",
        ").orderBy(\"pclass\").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "142c6803",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked| class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S| Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C| First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S| Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S| First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S| Third|  man|      true| NaN|Southampton|   no| true|\n",
            "|       0|     3|  male| NaN|    0|    0| 8.4583|       Q| Third|  man|      true| NaN| Queenstown|   no| true|\n",
            "|       0|     1|  male|54.0|    0|    0|51.8625|       S| First|  man|      true|   E|Southampton|   no| true|\n",
            "|       0|     3|  male| 2.0|    3|    1| 21.075|       S| Third|child|     false| NaN|Southampton|   no|false|\n",
            "|       1|     3|female|27.0|    0|    2|11.1333|       S| Third|woman|     false| NaN|Southampton|  yes|false|\n",
            "|       1|     2|female|14.0|    1|    0|30.0708|       C|Second|child|     false| NaN|  Cherbourg|  yes|false|\n",
            "+--------+------+------+----+-----+-----+-------+--------+------+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_titanic.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "56889e8b",
      "metadata": {
        "id": "56889e8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------+--------+--------+----------+\n",
            "|pclass|총승객수|생존자수|사망자수|생존자비율|\n",
            "+------+--------+--------+--------+----------+\n",
            "|     3|     491|     119|     372|      24.2|\n",
            "|     2|     184|      87|      97|      47.3|\n",
            "|     1|     216|     136|      80|      63.0|\n",
            "+------+--------+--------+--------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 6-4] 여기에 코드를 작성하세요\n",
        "df_titanic.groupBy(\"pclass\").agg(\n",
        "    count(\"*\").alias(\"총승객수\"),\n",
        "    count(when(col(\"alive\") == \"yes\",1)).alias(\"생존자수\"),\n",
        "    count(when(col(\"alive\") == \"no\",1)).alias(\"사망자수\"),\n",
        "    spark_round(\n",
        "        count(when(col(\"alive\") == \"yes\",1)) / count(\"*\")* 100, 1\n",
        "        ).alias(\"생존자비율\")\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7fac9bb",
      "metadata": {
        "id": "f7fac9bb"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 7: 피벗 테이블\n",
        "\n",
        "### 문제 7-1: 기본 피벗\n",
        "\n",
        "Tips 데이터로 `day`(행) × `time`(열) 형태의 피벗 테이블을 만들고,\n",
        "각 셀에는 건수를 표시하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`groupBy(\"행컬럼\").pivot(\"열컬럼\").count()`\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.groupBy(\"day\").pivot(\"time\").count().show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "91a13bf5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+-----+\n",
            "| day|Dinner|Lunch|\n",
            "+----+------+-----+\n",
            "| Sun|    76| NULL|\n",
            "| Fri|    12|    7|\n",
            "|Thur|     1|   61|\n",
            "| Sat|    87| NULL|\n",
            "+----+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_tips.groupBy(\"day\").pivot(\"time\").count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71cf7b8d",
      "metadata": {
        "id": "71cf7b8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------+-----+\n",
            "| day|Dinner|Lunch|\n",
            "+----+------+-----+\n",
            "| Sun|    76| NULL|\n",
            "| Fri|    12|    7|\n",
            "|Thur|     1|   61|\n",
            "| Sat|    87| NULL|\n",
            "+----+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 7-1] 여기에 코드를 작성하세요\n",
        "df_tips.groupBy(\"day\").pivot(\"time\").count().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e48de04",
      "metadata": {
        "id": "8e48de04"
      },
      "source": [
        "### 문제 7-2: 집계와 함께 피벗\n",
        "\n",
        "Tips 데이터로 `day`(행) × `smoker`(열) 형태의 피벗 테이블을 만들고,\n",
        "각 셀에는 평균 팁을 표시하세요. 소수점 2자리로 반올림하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`pivot()` 후 `agg(spark_round(avg(\"컬럼\"), 2))`를 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_tips.groupBy(\"day\").pivot(\"smoker\").agg(\n",
        "    spark_round(avg(\"tip\"), 2)\n",
        ").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de319073",
      "metadata": {
        "id": "de319073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+----+\n",
            "| day|  No| Yes|\n",
            "+----+----+----+\n",
            "| Sun|3.17|3.52|\n",
            "| Fri|2.81|2.71|\n",
            "|Thur|2.67|3.03|\n",
            "| Sat| 3.1|2.88|\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 7-2] 여기에 코드를 작성하세요\n",
        "df_tips.groupBy(\"day\").pivot(\"smoker\").agg(\n",
        "            spark_round(avg(\"tip\"),2)\n",
        ").show()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2582698d",
      "metadata": {
        "id": "2582698d"
      },
      "source": [
        "### 문제 7-3: Titanic 피벗 분석\n",
        "\n",
        "Titanic 데이터로 `pclass`(행) × `sex`(열) 형태의 피벗 테이블을 만들고,\n",
        "각 셀에는 생존율(%)을 표시하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`survived`의 평균 × 100이 생존율입니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_titanic.groupBy(\"pclass\").pivot(\"sex\").agg(\n",
        "    spark_round(avg(\"survived\") * 100, 1)\n",
        ").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "77f7a819",
      "metadata": {
        "id": "77f7a819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+------+----+\n",
            "|pclass|female|male|\n",
            "+------+------+----+\n",
            "|     3|  50.0|13.5|\n",
            "|     2|  92.1|15.7|\n",
            "|     1|  96.8|36.9|\n",
            "+------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 7-3] 여기에 코드를 작성하세요\n",
        "df_titanic.groupBy(\"pclass\").pivot(\"sex\").agg(\n",
        "            spark_round(avg(\"survived\") *100,1).alias(\"생존율\")\n",
        ").show()\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab04c2b",
      "metadata": {
        "id": "1ab04c2b"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 8: 조인\n",
        "\n",
        "### 데이터 준비\n",
        "\n",
        "조인 실습을 위해 Tips 데이터에서 파생 테이블을 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "35ef620e",
      "metadata": {
        "id": "35ef620e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 요일별 통계 테이블 ===\n",
            "+----+--------+-------+-----------+\n",
            "| day|avg_bill|avg_tip|visit_count|\n",
            "+----+--------+-------+-----------+\n",
            "| Sun|   21.41|   3.26|         76|\n",
            "| Sat|   20.44|   2.99|         87|\n",
            "|Thur|   17.68|   2.77|         62|\n",
            "| Fri|   17.15|   2.73|         19|\n",
            "+----+--------+-------+-----------+\n",
            "\n",
            "=== 시간대별 통계 테이블 ===\n",
            "+------+--------+-----+\n",
            "|  time|avg_bill|count|\n",
            "+------+--------+-----+\n",
            "|Dinner|    20.8|  176|\n",
            "| Lunch|   17.17|   68|\n",
            "+------+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 조인용 데이터 준비\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# 요일별 평균 정보 테이블 생성\n",
        "df_day_stats = df_tips.groupBy(\"day\").agg(\n",
        "    spark_round(avg(\"total_bill\"), 2).alias(\"avg_bill\"),\n",
        "    spark_round(avg(\"tip\"), 2).alias(\"avg_tip\"),\n",
        "    count(\"*\").alias(\"visit_count\")\n",
        ")\n",
        "\n",
        "print(\"=== 요일별 통계 테이블 ===\")\n",
        "df_day_stats.show()\n",
        "\n",
        "# 시간대별 정보 테이블 생성\n",
        "df_time_stats = df_tips.groupBy(\"time\").agg(\n",
        "    spark_round(avg(\"total_bill\"), 2).alias(\"avg_bill\"),\n",
        "    count(\"*\").alias(\"count\")\n",
        ")\n",
        "\n",
        "print(\"=== 시간대별 통계 테이블 ===\")\n",
        "df_time_stats.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1727943b",
      "metadata": {
        "id": "1727943b"
      },
      "source": [
        "### 문제 8-1: Inner Join\n",
        "\n",
        "Tips 데이터와 `df_day_stats`를 `day` 컬럼으로 Inner Join하세요.\n",
        "결과에서 `total_bill`, `tip`, `day`, `avg_bill`, `avg_tip`을 선택하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`df1.join(df2, \"공통컬럼\", \"inner\")`\n",
        "같은 컬럼명으로 조인하면 문자열로 지정할 수 있습니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_joined = df_tips.join(df_day_stats, \"day\", \"inner\")\n",
        "df_joined.select(\"total_bill\", \"tip\", \"day\", \"avg_bill\", \"avg_tip\").show(10)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "66ff10f3",
      "metadata": {
        "id": "66ff10f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+---+---+--------+-------+\n",
            "|total_bill|tip|day|avg_bill|avg_tip|\n",
            "+----------+---+---+--------+-------+\n",
            "|     15.69|1.5|Sun|   21.41|   3.26|\n",
            "|      23.1|4.0|Sun|   21.41|   3.26|\n",
            "|     18.15|3.5|Sun|   21.41|   3.26|\n",
            "|     30.46|2.0|Sun|   21.41|   3.26|\n",
            "|      20.9|3.5|Sun|   21.41|   3.26|\n",
            "+----------+---+---+--------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 8-1] 여기에 코드를 작성하세요\n",
        "df_tips_join = df_tips.join(\n",
        "    df_day_stats,\n",
        "    \"day\",\n",
        "    \"inner\"\n",
        ")\n",
        "\n",
        "df_tips_join.select(\n",
        "    \"total_bill\",\"tip\",\"day\",\"avg_bill\",\"avg_tip\"\n",
        ").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43b896de",
      "metadata": {
        "id": "43b896de"
      },
      "source": [
        "### 문제 8-2: 조인 후 파생 컬럼 추가\n",
        "\n",
        "문제 8-1의 결과에서 `tip_vs_avg` 컬럼을 추가하세요.\n",
        "이 컬럼은 `tip - avg_tip` (개인 팁 - 평균 팁)을 계산합니다.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "조인 결과에 `withColumn()`을 체이닝합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_with_diff = df_tips.join(df_day_stats, \"day\", \"inner\").withColumn(\n",
        "    \"tip_vs_avg\",\n",
        "    spark_round(col(\"tip\") - col(\"avg_tip\"), 2)\n",
        ")\n",
        "df_with_diff.select(\"day\", \"tip\", \"avg_tip\", \"tip_vs_avg\").show(10)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2d599a5c",
      "metadata": {
        "id": "2d599a5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----------+---+------+------+------+----+--------+-------+-----------+------------------+\n",
            "|day|total_bill|tip|   sex|smoker|  time|size|avg_bill|avg_tip|visit_count|        tip_vs_avg|\n",
            "+---+----------+---+------+------+------+----+--------+-------+-----------+------------------+\n",
            "|Sun|     15.69|1.5|  Male|   Yes|Dinner|   2|   21.41|   3.26|         76|0.4601226993865031|\n",
            "|Sun|      23.1|4.0|  Male|   Yes|Dinner|   3|   21.41|   3.26|         76|1.2269938650306749|\n",
            "|Sun|     18.15|3.5|Female|   Yes|Dinner|   3|   21.41|   3.26|         76|1.0736196319018405|\n",
            "|Sun|     30.46|2.0|  Male|   Yes|Dinner|   5|   21.41|   3.26|         76|0.6134969325153374|\n",
            "|Sun|      20.9|3.5|Female|   Yes|Dinner|   3|   21.41|   3.26|         76|1.0736196319018405|\n",
            "+---+----------+---+------+------+------+----+--------+-------+-----------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 8-2] 여기에 코드를 작성하세요\n",
        "df_tips_join.withColumn(\n",
        "    \"tip_vs_avg\",\n",
        "    col(\"tip\") / col(\"avg_tip\")\n",
        ").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7ac75c",
      "metadata": {
        "id": "bc7ac75c"
      },
      "source": [
        "### 문제 8-3: Left Join과 NULL 확인\n",
        "\n",
        "아래 코드로 새로운 요일 데이터를 만들고, `df_day_stats`와 Left Join하세요.\n",
        "매칭되지 않는 행이 있는지 확인하세요.\n",
        "\n",
        "```python\n",
        "# 새로운 요일 포함 데이터\n",
        "df_new_days = spark.createDataFrame([\n",
        "    (\"Mon\", 25.0),\n",
        "    (\"Sat\", 30.0),\n",
        "    (\"Wed\", 20.0),\n",
        "], [\"day\", \"bill\"])\n",
        "```\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "Left Join 후 `avg_bill`이 NULL인 행을 필터링하면 매칭되지 않는 행을 찾을 수 있습니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_new_days = spark.createDataFrame([\n",
        "    (\"Mon\", 25.0),\n",
        "    (\"Sat\", 30.0),\n",
        "    (\"Wed\", 20.0),\n",
        "], [\"day\", \"bill\"])\n",
        "\n",
        "df_left = df_new_days.join(df_day_stats, \"day\", \"left\")\n",
        "df_left.show()\n",
        "\n",
        "# 매칭되지 않는 행 확인\n",
        "print(\"매칭되지 않는 요일:\")\n",
        "df_left.filter(col(\"avg_bill\").isNull()).show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b20a7b7",
      "metadata": {
        "id": "2b20a7b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+--------+-------+-----------+\n",
            "|day|bill|avg_bill|avg_tip|visit_count|\n",
            "+---+----+--------+-------+-----------+\n",
            "|Mon|25.0|    NULL|   NULL|       NULL|\n",
            "|Sat|30.0|   20.44|   2.99|         87|\n",
            "|Wed|20.0|    NULL|   NULL|       NULL|\n",
            "+---+----+--------+-------+-----------+\n",
            "\n",
            "+---+----+--------+-------+-----------+\n",
            "|day|bill|avg_bill|avg_tip|visit_count|\n",
            "+---+----+--------+-------+-----------+\n",
            "|Mon|25.0|    NULL|   NULL|       NULL|\n",
            "|Wed|20.0|    NULL|   NULL|       NULL|\n",
            "+---+----+--------+-------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 8-3] 여기에 코드를 작성하세요\n",
        "df_new_days = spark.createDataFrame([\n",
        "    (\"Mon\", 25.0),\n",
        "    (\"Sat\", 30.0),\n",
        "    (\"Wed\", 20.0),\n",
        "], [\"day\", \"bill\"])\n",
        "# df_new_days를 기준으로 해서,df_day_stats에 day가 같은 행이 있으면 그 통계를 붙여서 보여주고, 없으면 df_new_days의 행은 그대로 두고 통계 컬럼은 NULL로 둔다.”\n",
        "df_left =df_new_days.join(\n",
        "    df_day_stats,\n",
        "    \"day\",\n",
        "    \"left\"\n",
        ")\n",
        "df_left.show()\n",
        "\n",
        "df_left.filter(col(\"avg_bill\").isNull()).show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d78a74",
      "metadata": {
        "id": "28d78a74"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 9: NULL 처리\n",
        "\n",
        "### 데이터 준비\n",
        "\n",
        "NULL이 포함된 데이터를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ff16b955",
      "metadata": {
        "id": "ff16b955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Titanic 데이터 NULL 현황 ===\n",
            "+--------+---------+-------------+----------------+\n",
            "|age_null|deck_null|embarked_null|embark_town_null|\n",
            "+--------+---------+-------------+----------------+\n",
            "|       0|        0|            0|               0|\n",
            "+--------+---------+-------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# NULL 처리용 데이터 준비\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Titanic 데이터의 NULL 확인\n",
        "print(\"=== Titanic 데이터 NULL 현황 ===\")\n",
        "df_titanic.agg(\n",
        "    count(when(col(\"age\").isNull(), 1)).alias(\"age_null\"),\n",
        "    count(when(col(\"deck\").isNull(), 1)).alias(\"deck_null\"),\n",
        "    count(when(col(\"embarked\").isNull(), 1)).alias(\"embarked_null\"),\n",
        "    count(when(col(\"embark_town\").isNull(), 1)).alias(\"embark_town_null\"),\n",
        ").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9b421fc",
      "metadata": {
        "id": "c9b421fc"
      },
      "source": [
        "### 문제 9-1: NULL 행 제거\n",
        "\n",
        "Titanic 데이터에서 `age`가 NULL인 행을 제거하세요.\n",
        "제거 전후의 행 수를 비교하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`dropna(subset=[\"컬럼\"])`을 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "print(f\"원본 행 수: {df_titanic.count()}\")\n",
        "\n",
        "df_no_null_age = df_titanic.dropna(subset=[\"age\"])\n",
        "print(f\"age NULL 제거 후: {df_no_null_age.count()}\")\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "3463ce22",
      "metadata": {
        "id": "3463ce22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "891\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "714"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# [문제 9-1] 여기에 코드를 작성하세요\n",
        "print(df_titanic.count())\n",
        "df_no_age_null = df_titanic.dropna(subset=[\"age\"])\n",
        "df_no_age_null.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d735db90",
      "metadata": {
        "id": "d735db90"
      },
      "source": [
        "### 문제 9-2: NULL 값 채우기\n",
        "\n",
        "Titanic 데이터에서:\n",
        "- `age`의 NULL을 0으로 채우기\n",
        "- `deck`의 NULL을 \"Unknown\"으로 채우기\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`fillna({\"컬럼1\": 값1, \"컬럼2\": 값2})`로 컬럼별 다른 값을 지정합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_filled = df_titanic.fillna({\n",
        "    \"age\": 0,\n",
        "    \"deck\": \"Unknown\"\n",
        "})\n",
        "\n",
        "# NULL이 채워졌는지 확인\n",
        "df_filled.agg(\n",
        "    count(when(col(\"age\").isNull(), 1)).alias(\"age_null\"),\n",
        "    count(when(col(\"deck\").isNull(), 1)).alias(\"deck_null\"),\n",
        ").show()\n",
        "\n",
        "# deck이 Unknown인 행 확인\n",
        "df_filled.filter(col(\"deck\") == \"Unknown\").select(\"survived\", \"pclass\", \"deck\").show(5)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7790efa5",
      "metadata": {
        "id": "7790efa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---------+\n",
            "|age_null|deck_null|\n",
            "+--------+---------+\n",
            "|       0|        0|\n",
            "+--------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 9-2] 여기에 코드를 작성하세요\n",
        "df_filled = df_titanic.fillna({\n",
        "    \"age\": 0,                    \n",
        "    \"deck\": \"Unknown\"     \n",
        "})\n",
        "df_filled.agg(\n",
        "    count(when(col(\"age\").isNull(),1)).alias(\"age_null\"),\n",
        "    count(when(col(\"deck\").isNull(),1)).alias(\"deck_null\")\n",
        ").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9db80bbe",
      "metadata": {
        "id": "9db80bbe"
      },
      "source": [
        "### 문제 9-3: coalesce로 대체값 설정\n",
        "\n",
        "Titanic 데이터에서 `embarked` 컬럼의 NULL을 다음 우선순위로 채우세요:\n",
        "1. `embarked` 값이 있으면 그대로\n",
        "2. NULL이면 \"S\" (Southampton, 가장 많은 승선지)\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`coalesce(col(\"컬럼\"), lit(\"기본값\"))`을 사용합니다.\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_coalesced = df_titanic.withColumn(\n",
        "    \"embarked_filled\",\n",
        "    coalesce(col(\"embarked\"), lit(\"S\"))\n",
        ")\n",
        "\n",
        "# 확인\n",
        "df_coalesced.filter(col(\"embarked\").isNull()) \\\n",
        "    .select(\"embarked\", \"embarked_filled\").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "47508b4a",
      "metadata": {
        "id": "47508b4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "+--------+---------------+\n",
            "|embarked|embarked_filled|\n",
            "+--------+---------------+\n",
            "+--------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 9-3] 여기에 코드를 작성하세요\n",
        "test = df_titanic.filter(col(\"embarked\").isNull()).count()\n",
        "print(test)\n",
        "\n",
        "df_coalesced = df_titanic.withColumn(\n",
        "    \"embarked_filled\",\n",
        "    coalesce(\n",
        "        col(\"embarked\"),\n",
        "        lit(\"S\")\n",
        "    )\n",
        ")\n",
        "\n",
        "df_coalesced.filter(col(\"embarked\").isNull()) \\\n",
        "    .select(\"embarked\", \"embarked_filled\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4cb14d1",
      "metadata": {
        "id": "c4cb14d1"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 10: Window 함수\n",
        "\n",
        "### 문제 10-1: 그룹 내 순위 (row_number)\n",
        "\n",
        "Tips 데이터에서 `day`별로 `total_bill` 내림차순 순위를 부여하세요.\n",
        "컬럼명은 `bill_rank`로 하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "1. `Window.partitionBy(\"그룹컬럼\").orderBy(col(\"정렬컬럼\").desc())`로 윈도우 정의\n",
        "2. `row_number().over(윈도우)`로 순위 부여\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "window_day = Window.partitionBy(\"day\").orderBy(col(\"total_bill\").desc())\n",
        "\n",
        "df_ranked = df_tips.withColumn(\n",
        "    \"bill_rank\",\n",
        "    row_number().over(window_day)\n",
        ")\n",
        "\n",
        "df_ranked.select(\"day\", \"total_bill\", \"tip\", \"bill_rank\") \\\n",
        "    .orderBy(\"day\", \"bill_rank\").show(20)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852db651",
      "metadata": {
        "id": "852db651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+----+------+------+---+------+----+---------+\n",
            "|total_bill| tip|   sex|smoker|day|  time|size|bill_rank|\n",
            "+----------+----+------+------+---+------+----+---------+\n",
            "|     40.17|4.73|  Male|   Yes|Fri|Dinner|   4|        1|\n",
            "|     28.97| 3.0|  Male|   Yes|Fri|Dinner|   2|        2|\n",
            "|     27.28| 4.0|  Male|   Yes|Fri|Dinner|   2|        3|\n",
            "|     22.75|3.25|Female|    No|Fri|Dinner|   2|        4|\n",
            "|     22.49| 3.5|  Male|    No|Fri|Dinner|   2|        5|\n",
            "+----------+----+------+------+---+------+----+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 10-1] 여기에 코드를 작성하세요\n",
        "window_rank = Window.partitionBy(\"day\").orderBy(col(\"total_bill\").desc())\n",
        "#total_bill 값이 NULL이 아닌 행만 남긴 뒤,그 행들에 대해서만 window_rank 기준으로 순위를 매긴다.\n",
        "df_ranked = df_tips.filter(col(\"total_bill\").isNotNull()).withColumn(\n",
        "    \"bill_rank\", row_number().over(window_rank))\n",
        "\n",
        "df_ranked.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e94240e",
      "metadata": {
        "id": "2e94240e"
      },
      "source": [
        "### 문제 10-2: 그룹별 Top N 추출\n",
        "\n",
        "문제 10-1의 결과를 사용하여 각 요일별 `total_bill` Top 3를 추출하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "순위를 부여한 후 `filter(col(\"bill_rank\") <= 3)`\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "window_day = Window.partitionBy(\"day\").orderBy(col(\"total_bill\").desc())\n",
        "\n",
        "df_top3 = df_tips.withColumn(\n",
        "    \"bill_rank\", row_number().over(window_day)\n",
        ").filter(col(\"bill_rank\") <= 3)\n",
        "\n",
        "df_top3.select(\"day\", \"total_bill\", \"tip\", \"bill_rank\") \\\n",
        "    .orderBy(\"day\", \"bill_rank\").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "0aabb4f7",
      "metadata": {
        "id": "0aabb4f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+----+---------+\n",
            "| day|total_bill| tip|bill_rank|\n",
            "+----+----------+----+---------+\n",
            "| Fri|     40.17|4.73|        1|\n",
            "| Fri|     28.97| 3.0|        2|\n",
            "| Fri|     27.28| 4.0|        3|\n",
            "| Sat|     50.81|10.0|        1|\n",
            "| Sat|     48.33| 9.0|        2|\n",
            "| Sat|     48.27|6.73|        3|\n",
            "| Sun|     48.17| 5.0|        1|\n",
            "| Sun|     45.35| 3.5|        2|\n",
            "| Sun|     40.55| 3.0|        3|\n",
            "|Thur|     43.11| 5.0|        1|\n",
            "|Thur|     41.19| 5.0|        2|\n",
            "|Thur|     34.83|5.17|        3|\n",
            "+----+----------+----+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# [문제 10-2] 여기에 코드를 작성하세요\n",
        "window_rank = Window.partitionBy(\"day\").orderBy(col(\"total_bill\").desc())\n",
        "\n",
        "df_top3 = df_tips.filter(col(\"total_bill\").isNotNull()).withColumn(\n",
        "    \"bill_rank\", row_number().over(window_rank)\n",
        ").filter(\n",
        "    col(\"bill_rank\") <= 3\n",
        ")\n",
        "df_top3.select(\"day\", \"total_bill\", \"tip\", \"bill_rank\") \\\n",
        "    .orderBy(\"day\", \"bill_rank\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d8cf8b0",
      "metadata": {
        "id": "8d8cf8b0"
      },
      "source": [
        "### 문제 10-3: 그룹 내 비율 계산\n",
        "\n",
        "Tips 데이터에서 `day`별로 각 건의 `total_bill`이 그 날 전체 매출에서 차지하는 비율(%)을 계산하세요.\n",
        "컬럼명은 `bill_pct`로 하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "1. `Window.partitionBy(\"day\")`로 그룹 윈도우 정의 (정렬 없음)\n",
        "2. `sum(\"total_bill\").over(윈도우)`로 그룹 합계\n",
        "3. 개별 값 / 그룹 합계 * 100\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "window_day = Window.partitionBy(\"day\")\n",
        "\n",
        "df_pct = df_tips.withColumn(\n",
        "    \"day_total\", sum(\"total_bill\").over(window_day)\n",
        ").withColumn(\n",
        "    \"bill_pct\", spark_round(col(\"total_bill\") / col(\"day_total\") * 100, 2)\n",
        ")\n",
        "\n",
        "df_pct.select(\"day\", \"total_bill\", \"day_total\", \"bill_pct\") \\\n",
        "    .orderBy(\"day\", col(\"bill_pct\").desc()).show(15)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c31dc8f",
      "metadata": {
        "id": "5c31dc8f"
      },
      "outputs": [],
      "source": [
        "# [문제 10-3] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "831c6a72",
      "metadata": {
        "id": "831c6a72"
      },
      "source": [
        "### 문제 10-4: lag/lead - 이전/다음 행 비교\n",
        "\n",
        "Tips 데이터를 `total_bill` 순으로 정렬하고,\n",
        "이전 행의 `total_bill`과 현재 행의 차이(`bill_diff`)를 계산하세요.\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "1. `Window.orderBy(\"total_bill\")`로 전체 정렬 윈도우 정의\n",
        "2. `lag(\"컬럼\", 1).over(윈도우)`로 이전 행 값\n",
        "3. 현재 값 - 이전 값\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "window_order = Window.orderBy(\"total_bill\")\n",
        "\n",
        "df_with_lag = df_tips.withColumn(\n",
        "    \"prev_bill\", lag(\"total_bill\", 1).over(window_order)\n",
        ").withColumn(\n",
        "    \"bill_diff\", spark_round(col(\"total_bill\") - col(\"prev_bill\"), 2)\n",
        ")\n",
        "\n",
        "df_with_lag.select(\"total_bill\", \"prev_bill\", \"bill_diff\").show(15)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27bcc8b",
      "metadata": {
        "id": "e27bcc8b"
      },
      "outputs": [],
      "source": [
        "# [문제 10-4] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "208abc23",
      "metadata": {
        "id": "208abc23"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 11: 종합 문제\n",
        "\n",
        "### 종합 문제 1: Tips 데이터 분석 파이프라인\n",
        "\n",
        "Tips 데이터를 사용하여 다음을 한 번에 수행하는 파이프라인을 작성하세요:\n",
        "\n",
        "1. `tip_rate` 컬럼 추가 (tip / total_bill, 소수점 2자리)\n",
        "2. `tip_rate`가 0.2(20%) 이상이면 \"good_tipper\"를, 아니면 \"normal\"을 `tipper_type` 컬럼에 저장\n",
        "3. `day`와 `tipper_type`으로 그룹화하여 건수 집계\n",
        "4. `day` 오름차순, 건수 내림차순 정렬\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "`withColumn().withColumn().groupBy().agg().orderBy()` 체이닝\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "result = (\n",
        "    df_tips\n",
        "    .withColumn(\"tip_rate\", spark_round(col(\"tip\") / col(\"total_bill\"), 2))\n",
        "    .withColumn(\n",
        "        \"tipper_type\",\n",
        "        when(col(\"tip_rate\") >= 0.2, \"good_tipper\").otherwise(\"normal\")\n",
        "    )\n",
        "    .groupBy(\"day\", \"tipper_type\")\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "    .orderBy(col(\"day\").asc(), col(\"count\").desc())\n",
        ")\n",
        "\n",
        "result.show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49220955",
      "metadata": {
        "id": "49220955"
      },
      "outputs": [],
      "source": [
        "# [종합 문제 1] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "272eac0b",
      "metadata": {
        "id": "272eac0b"
      },
      "source": [
        "### 종합 문제 2: Titanic 생존 분석\n",
        "\n",
        "Titanic 데이터를 사용하여 다음을 수행하세요:\n",
        "\n",
        "1. `age`의 NULL을 평균 나이로 채우기\n",
        "2. `age_group` 컬럼 추가:\n",
        "   - 12세 미만: \"child\"\n",
        "   - 12~60세: \"adult\"\n",
        "   - 60세 이상: \"senior\"\n",
        "3. `pclass`와 `age_group`으로 피벗 테이블 생성 (값: 생존율 %)\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "1. 먼저 평균 나이를 계산 (`df.agg(avg(\"age\")).collect()[0][0]`)\n",
        "2. `fillna()`로 채우기\n",
        "3. `when().when().otherwise()`로 분류\n",
        "4. `pivot()`으로 피벗\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "# 1. 평균 나이 계산\n",
        "avg_age = df_titanic.agg(avg(\"age\")).collect()[0][0]\n",
        "print(f\"평균 나이: {avg_age:.1f}\")\n",
        "\n",
        "# 2. NULL 채우기 + age_group 추가\n",
        "df_processed = (\n",
        "    df_titanic\n",
        "    .fillna({\"age\": avg_age})\n",
        "    .withColumn(\n",
        "        \"age_group\",\n",
        "        when(col(\"age\") < 12, \"child\")\n",
        "        .when(col(\"age\") < 60, \"adult\")\n",
        "        .otherwise(\"senior\")\n",
        "    )\n",
        ")\n",
        "\n",
        "# 3. 피벗 테이블 (생존율 %)\n",
        "pivot_result = df_processed.groupBy(\"pclass\").pivot(\"age_group\", [\"child\", \"adult\", \"senior\"]).agg(\n",
        "    spark_round(avg(\"survived\") * 100, 1)\n",
        ")\n",
        "\n",
        "print(\"=== 객실 등급 × 연령대별 생존율 (%) ===\")\n",
        "pivot_result.orderBy(\"pclass\").show()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "740c8a5d",
      "metadata": {
        "id": "740c8a5d"
      },
      "outputs": [],
      "source": [
        "# [종합 문제 2] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28d33010",
      "metadata": {
        "id": "28d33010"
      },
      "source": [
        "### 종합 문제 3: Window 함수 종합\n",
        "\n",
        "Titanic 데이터를 사용하여 다음을 수행하세요:\n",
        "\n",
        "1. `pclass`별로 `fare` 내림차순 순위 부여 (`fare_rank`)\n",
        "2. `pclass`별 `fare` 합계 대비 개인 비율(%) 계산 (`fare_pct`)\n",
        "3. 각 등급별 Top 3 승객만 추출\n",
        "4. 결과에서 `pclass`, `name`, `fare`, `fare_rank`, `fare_pct` 출력\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "- 순위용 Window: `partitionBy().orderBy()`\n",
        "- 합계용 Window: `partitionBy()` (정렬 없음)\n",
        "- Top 3: `filter(col(\"fare_rank\") <= 3)`\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "# Window 정의\n",
        "window_rank = Window.partitionBy(\"pclass\").orderBy(col(\"fare\").desc())\n",
        "window_sum = Window.partitionBy(\"pclass\")\n",
        "\n",
        "# 분석\n",
        "df_fare_analysis = (\n",
        "    df_titanic\n",
        "    .withColumn(\"fare_rank\", row_number().over(window_rank))\n",
        "    .withColumn(\"class_fare_sum\", sum(\"fare\").over(window_sum))\n",
        "    .withColumn(\"fare_pct\", spark_round(col(\"fare\") / col(\"class_fare_sum\") * 100, 2))\n",
        "    .filter(col(\"fare_rank\") <= 3)\n",
        "    .select(\"pclass\", \"name\", \"fare\", \"fare_rank\", \"fare_pct\")\n",
        "    .orderBy(\"pclass\", \"fare_rank\")\n",
        ")\n",
        "\n",
        "df_fare_analysis.show(truncate=False)\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb6f44e",
      "metadata": {
        "id": "dfb6f44e"
      },
      "outputs": [],
      "source": [
        "# [종합 문제 3] 여기에 코드를 작성하세요\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f7fe1e3",
      "metadata": {
        "id": "4f7fe1e3"
      },
      "source": [
        "---\n",
        "\n",
        "## 과제 완료\n",
        "\n",
        "축하합니다! PySpark DataFrame API 종합 과제를 완료했습니다.\n",
        "\n",
        "### 학습 내용 정리\n",
        "\n",
        "| 주제 | 주요 함수/메서드 |\n",
        "|------|------------------|\n",
        "| 컬럼 선택/조작 | `select()`, `withColumn()`, `withColumnRenamed()`, `drop()` |\n",
        "| 필터링 | `filter()`, `where()`, `isin()`, `startswith()` |\n",
        "| 조건부 값 | `when()`, `otherwise()` |\n",
        "| 정렬/제한 | `orderBy()`, `limit()`, `distinct()` |\n",
        "| 집계 | `groupBy()`, `agg()`, `count()`, `sum()`, `avg()` |\n",
        "| 피벗 | `pivot()` |\n",
        "| 조인 | `join()` (inner, left, right, outer) |\n",
        "| NULL 처리 | `dropna()`, `fillna()`, `coalesce()` |\n",
        "| Window 함수 | `row_number()`, `rank()`, `lag()`, `lead()`, `sum().over()` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19065b61",
      "metadata": {
        "id": "19065b61"
      },
      "outputs": [],
      "source": [
        "# 세션 종료\n",
        "# spark.stop()\n",
        "print(\"\\n과제 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0665f6",
      "metadata": {
        "id": "ef0665f6"
      },
      "source": [
        "# 2교시: Spark Structured Streaming\n",
        "\n",
        "![Spark Structured Streaming](https://moons08.github.io/assets/img/post/spark/streaming-arch.png)\n",
        "\n",
        "---\n",
        "\n",
        "## 수업 목표\n",
        "\n",
        "이 교시를 마치면 다음을 할 수 있습니다:\n",
        "\n",
        "- Spark Structured Streaming의 개념과 특징을 이해할 수 있다\n",
        "- Kafka에서 실시간으로 데이터를 읽어올 수 있다\n",
        "- 윈도우 기반 집계(시간 윈도우)를 구현할 수 있다\n",
        "- 스트리밍 결과를 콘솔/파일/Kafka로 출력할 수 있다\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67a18905",
      "metadata": {
        "id": "67a18905"
      },
      "source": [
        "## 핵심 개념 1: 배치 처리 vs 스트리밍 처리\n",
        "\n",
        "### 두 가지 데이터 처리 방식\n",
        "\n",
        "데이터 처리 방식은 크게 **배치(Batch)**와 **스트리밍(Streaming)**으로 나뉩니다.\n",
        "\n",
        "![](https://k21academy.com/wp-content/uploads/2020/11/BatchProcessingStreamProcessing_Diagram-02.png)\n",
        "\n",
        "### 배치 처리 (Batch Processing)\n",
        "\n",
        "**흐름**: 어제의 데이터 → 처리 (한번에) → 결과 (다음날 확인)\n",
        "\n",
        "**예시**:\n",
        "- 매일 밤 12시에 전날 판매 리포트 생성\n",
        "- 매주 월요일에 주간 사용자 통계 계산\n",
        "- 매월 1일에 월간 정산 처리\n",
        "\n",
        "**특징**:\n",
        "- 데이터가 모두 준비된 후 처리\n",
        "- 높은 처리량 (throughput) 가능\n",
        "- 지연 시간(latency)이 김 (시간~일)\n",
        "\n",
        "### 스트리밍 처리 (Stream Processing)\n",
        "\n",
        "**흐름**: 실시간 데이터 → 처리 → 결과 → 처리 → 결과 (끊임없이 반복)\n",
        "\n",
        "**예시**:\n",
        "- 실시간 대시보드 (트래픽 모니터링)\n",
        "- 이상 거래 탐지 (1초 내 알림)\n",
        "- 실시간 추천 시스템\n",
        "\n",
        "**특징**:\n",
        "- 데이터가 도착하는 즉시 처리\n",
        "- 낮은 지연 시간(latency) (밀리초~초)\n",
        "- 24시간 계속 실행\n",
        "\n",
        "### 비교 요약\n",
        "\n",
        "| 구분 | 배치 처리 | 스트리밍 처리 |\n",
        "|------|----------|--------------|\n",
        "| 처리 시점 | 데이터 축적 후 일괄 처리 | 데이터 도착 즉시 처리 |\n",
        "| 지연 시간 | 시간 ~ 일 | 밀리초 ~ 초 |\n",
        "| 처리량 | 높음 | 상대적으로 낮음 |\n",
        "| 실행 방식 | 스케줄 기반 (주기적) | 24시간 상시 실행 |\n",
        "| 사용 사례 | 리포트, 정산, 분석 | 모니터링, 알림, 실시간 대시보드 |\n",
        "\n",
        "### 언제 어떤 방식을 사용하나요?\n",
        "\n",
        "| 상황 | 배치 | 스트리밍 |\n",
        "|------|------|----------|\n",
        "| 데이터 분석 리포트 | O | |\n",
        "| 실시간 대시보드 | | O |\n",
        "| 월간 정산 | O | |\n",
        "| 이상 거래 탐지 | | O |\n",
        "| 머신러닝 모델 학습 | O | |\n",
        "| 실시간 추천 | | O |\n",
        "| ETL 파이프라인 | O | O (둘 다 가능) |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e29ca84",
      "metadata": {
        "id": "3e29ca84"
      },
      "source": [
        "## 핵심 개념 2: Micro-batch 처리 방식\n",
        "\n",
        "### Spark Structured Streaming의 동작 원리\n",
        "\n",
        "Spark Structured Streaming은 **Micro-batch** 방식으로 스트리밍을 처리합니다.\n",
        "데이터를 작은 배치 단위로 나누어 처리하는 방식입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f398c0a8",
      "metadata": {
        "id": "f398c0a8"
      },
      "source": [
        "**처리 흐름**:\n",
        "1. 데이터 도착 → 배치 단위로 분할\n",
        "2. 각 배치별 처리 실행\n",
        "3. 결과 출력\n",
        "4. 다음 배치 반복\n",
        "\n",
        "예: `trigger: processingTime=\"5 seconds\"` → 5초마다 배치 처리\n",
        "\n",
        "### Micro-batch의 장점\n",
        "\n",
        "| 장점 | 설명 |\n",
        "|------|------|\n",
        "| **Exactly-once 보장** | 데이터 중복/손실 없이 정확히 한 번 처리 |\n",
        "| **배치 코드 재사용** | DataFrame API를 그대로 사용 |\n",
        "| **장애 복구 용이** | 체크포인트로 상태 저장/복구 |\n",
        "| **간편한 개발** | 스트리밍 복잡성을 추상화 |\n",
        "\n",
        "### 트리거(Trigger) 옵션\n",
        "\n",
        "| 트리거 | 설명 | 사용 예시 |\n",
        "|--------|------|----------|\n",
        "| `processingTime=\"5 seconds\"` | 5초마다 배치 처리 | 실시간 대시보드 |\n",
        "| `processingTime=\"1 minute\"` | 1분마다 배치 처리 | 준실시간 집계 |\n",
        "| `once=True` | 한 번만 처리 | 배치 스타일 스트리밍 |\n",
        "| `availableNow=True` | 현재까지 데이터 한 번 처리 | 백필(backfill) |\n",
        "\n",
        "### 리얼타임 모드(Real-Time Mode, RTM)\n",
        "\n",
        "Spark 4.1부터 도입된 **초저지연 연속 처리** 모드입니다.\n",
        "Micro-batch의 배치 간격으로 인한 지연을 제거하여 **p99 레이턴시를 한 자릿수 밀리초**까지 낮출 수 있습니다.\n",
        "\n",
        "**Micro-batch vs Real-Time Mode**:\n",
        "\n",
        "![Code Diff btw rtm simple change](https://www.databricks.com/sites/default/files/inline-images/image5_36.png)\n",
        "\n",
        "| 구분 | Micro-batch | Real-Time Mode |\n",
        "|------|-------------|----------------|\n",
        "| **처리 방식** | 주기적 배치 처리 | 연속(Continuous) 처리 |\n",
        "| **레이턴시** | 초~분 단위 | 밀리초 단위 |\n",
        "| **적용 방식** | 기본 모드 | 설정 변경만으로 활성화 |\n",
        "\n",
        "**RTM 지원 범위 (Spark 4.1 기준)**:\n",
        "\n",
        "| 항목 | 지원 내용 |\n",
        "|------|-----------|\n",
        "| **언어** | Scala |\n",
        "| **쿼리 타입** | Stateless / 단일 스테이지 |\n",
        "| **Source** | Kafka |\n",
        "| **Sink** | Kafka, ForeachSink |\n",
        "| **출력 모드** | Update 모드만 |\n",
        "\n",
        "> 💡 **선택 기준**: 대부분의 스트리밍 워크로드는 Micro-batch로 충분합니다.\n",
        "> 밀리초 단위 응답이 필수인 경우에만 RTM을 고려하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee8b6d8",
      "metadata": {
        "id": "2ee8b6d8"
      },
      "source": [
        "## 핵심 개념 3: Watermark (워터마크)\n",
        "\n",
        "### 지연 데이터 문제\n",
        "\n",
        "실시간 시스템에서는 네트워크 지연 등으로 데이터가 늦게 도착할 수 있습니다.\n",
        "Watermark는 \"얼마나 늦은 데이터까지 허용할 것인가\"를 정의합니다.\n",
        "\n",
        "**예시**: Watermark = 10분 설정\n",
        "\n",
        "| 실제 시간 | 데이터 | 이벤트 시간 | 처리 여부 |\n",
        "|-----------|--------|-------------|-----------|\n",
        "| 10:00 | A | 10:00 | 처리 |\n",
        "| 10:05 | B | 10:04 | 처리 |\n",
        "| 10:10 | C | 10:08 | 처리 |\n",
        "| 10:15 | D | 10:12 | 처리 |\n",
        "| 10:20 | E | 10:03 (늦음!) | **버림** |\n",
        "\n",
        "**계산**: 현재 시간 10:15일 때, Watermark = 10:15 - 10분 = 10:05\n",
        "- 이벤트 시간 10:03인 (E)는 10:05보다 이전 → **버림**\n",
        "- 결론: 10분 이상 늦은 데이터는 무시됨\n",
        "\n",
        "### Watermark 설정 방법\n",
        "\n",
        "```python\n",
        "df.withWatermark(\"event_time\", \"10 minutes\")\n",
        "```\n",
        "\n",
        "| 파라미터 | 의미 |\n",
        "|----------|------|\n",
        "| `\"event_time\"` | 이벤트 시간 컬럼명 |\n",
        "| `\"10 minutes\"` | 허용할 최대 지연 시간 |\n",
        "\n",
        "### Watermark 설정 기준\n",
        "\n",
        "| 값 | 상황 |\n",
        "|----|------|\n",
        "| 작은 값 (1분) | 네트워크가 안정적, 빠른 결과 필요 |\n",
        "| 큰 값 (1시간) | 네트워크가 불안정, 데이터 손실 최소화 |\n",
        "| 중간 값 (10분) | 일반적인 상황 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bae047eb",
      "metadata": {
        "id": "bae047eb"
      },
      "source": [
        "## 핵심 개념 4: 윈도우 집계 (Window Aggregation)\n",
        "\n",
        "### 시간 윈도우란?\n",
        "\n",
        "![](https://www.databricks.com/wp-content/uploads/2021/10/Native-Support-4-Session-Window-in-Spark-Streaming-blog-img-1.jpg)\n",
        "\n",
        "스트리밍 데이터를 시간 단위로 묶어서 집계하는 방법입니다.\n",
        "\n",
        "### Tumbling Window (텀블링 윈도우)\n",
        "\n",
        "윈도우가 **겹치지 않음** - 각 데이터는 하나의 윈도우에만 속함\n",
        "\n",
        "| 윈도우 | 시간 범위 | 설명 |\n",
        "|--------|----------|------|\n",
        "| Window 1 | 0~5분 | 집계 결과 1 |\n",
        "| Window 2 | 5~10분 | 집계 결과 2 |\n",
        "| Window 3 | 10~15분 | 집계 결과 3 |\n",
        "\n",
        "**특징**: 가장 일반적으로 사용\n",
        "\n",
        "### Sliding Window (슬라이딩 윈도우)\n",
        "\n",
        "윈도우가 **겹침** - 하나의 데이터가 여러 윈도우에 속할 수 있음\n",
        "\n",
        "| 윈도우 | 시간 범위 |\n",
        "|--------|----------|\n",
        "| Window 1 | 0~5분 |\n",
        "| Window 2 | 2~7분 |\n",
        "| Window 3 | 4~9분 |\n",
        "| Window 4 | 6~11분 |\n",
        "\n",
        "**특징**: 더 세밀한 분석 가능\n",
        "\n",
        "### 윈도우 설정 방법\n",
        "\n",
        "```python\n",
        "# Tumbling Window (5분)\n",
        "window(col(\"event_time\"), \"5 minutes\")\n",
        "\n",
        "# Sliding Window (5분 윈도우, 1분 슬라이드)\n",
        "window(col(\"event_time\"), \"5 minutes\", \"1 minute\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eed10b22",
      "metadata": {
        "id": "eed10b22"
      },
      "source": [
        "## Structured Streaming이란?\n",
        "\n",
        "### 개념\n",
        "\n",
        "**배치 처리 (기존 방식)**: 데이터 전체 로드 → 처리 → 결과 출력\n",
        "\n",
        "**스트리밍 처리 (Structured Streaming)**: 데이터 조금씩 → 처리 → 결과 지속 업데이트 (micro-batch 반복)\n",
        "\n",
        "### 핵심 아이디어: 무한 테이블\n",
        "\n",
        "스트리밍 데이터를 \"무한히 늘어나는 테이블\"처럼 취급합니다.\n",
        "\n",
        "| 시점 | 테이블 상태 |\n",
        "|------|------------|\n",
        "| 초기 | row 1, row 2 |\n",
        "| 새 데이터 도착 | row 1, row 2, **row 3 (new)**, **row 4 (new)** |\n",
        "| 또 새 데이터 | row 1, row 2, row 3, row 4, **row 5 (new)** |\n",
        "\n",
        "**장점**: 같은 DataFrame API로 배치/스트리밍 모두 처리 가능\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817bde1a",
      "metadata": {
        "id": "817bde1a"
      },
      "source": [
        "# 3교시 Spark Structured Streaming 패턴 실습\n",
        "\n",
        "---\n",
        "\n",
        "## 수업 목표\n",
        "\n",
        "이 교안을 마치면 다음을 할 수 있습니다:\n",
        "\n",
        "- 다양한 스트리밍 소스(rate, socket, kafka, file)를 활용할 수 있다\n",
        "- 스트림 데이터에 스키마를 정의하고 JSON을 파싱할 수 있다\n",
        "- 스트림에 filter, select, withColumn 등 변환 연산을 적용할 수 있다\n",
        "- Output Mode(append, update, complete)를 상황에 맞게 선택할 수 있다\n",
        "- 다양한 Sink(console, file, kafka, foreachBatch)로 결과를 출력할 수 있다\n",
        "- Trigger를 설정하여 처리 주기를 제어할 수 있다\n",
        "- Tumbling/Sliding 윈도우 집계를 구현할 수 있다\n",
        "- Watermark로 지연 데이터를 처리할 수 있다\n",
        "- 체크포인트로 장애 복구를 구현할 수 있다\n",
        "\n",
        "---\n",
        "\n",
        "## 실습 환경 구성\n",
        "\n",
        "### 필요한 파일\n",
        "\n",
        "이 교안은 `spark-kafka-compose` 환경에서 실행합니다.\n",
        "아래 파일들이 `/app` 디렉토리에 있어야 합니다.\n",
        "\n",
        "#### 1. `producer.py` - Kafka 테스트 데이터 생성기\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "Kafka Producer - API 이벤트 생성기\n",
        "\n",
        "실행 방법:\n",
        "    docker exec -it python-dev python producer.py --rate 10\n",
        "\"\"\"\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import uuid\n",
        "import argparse\n",
        "from datetime import datetime\n",
        "from confluent_kafka import Producer\n",
        "\n",
        "KAFKA_BOOTSTRAP_SERVERS = \"kafka:9092\"\n",
        "TOPIC_NAME = \"api-events\"\n",
        "\n",
        "ENDPOINTS = [\"/api/users\", \"/api/orders\", \"/api/products\", \"/api/payments\", \"/api/search\"]\n",
        "METHODS = [\"GET\", \"POST\", \"PUT\", \"DELETE\"]\n",
        "STATUS_CODES = [\n",
        "    (200, 60), (201, 10), (400, 8), (401, 5),\n",
        "    (404, 7), (500, 6), (502, 2), (503, 2),\n",
        "]\n",
        "\n",
        "def weighted_choice(choices):\n",
        "    total = sum(weight for _, weight in choices)\n",
        "    r = random.uniform(0, total)\n",
        "    cumulative = 0\n",
        "    for item, weight in choices:\n",
        "        cumulative += weight\n",
        "        if r <= cumulative:\n",
        "            return item\n",
        "    return choices[-1][0]\n",
        "\n",
        "def generate_event():\n",
        "    status_code = weighted_choice(STATUS_CODES)\n",
        "    if status_code >= 500:\n",
        "        response_time = random.randint(500, 2000)\n",
        "    elif status_code >= 400:\n",
        "        response_time = random.randint(100, 500)\n",
        "    else:\n",
        "        response_time = random.randint(10, 300)\n",
        "    return {\n",
        "        \"request_id\": str(uuid.uuid4())[:8],\n",
        "        \"user_id\": f\"user-{random.randint(1, 100):03d}\",\n",
        "        \"endpoint\": random.choice(ENDPOINTS),\n",
        "        \"method\": random.choice(METHODS),\n",
        "        \"status_code\": status_code,\n",
        "        \"response_time_ms\": response_time,\n",
        "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%dT%H:%M:%S\"),\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--rate\", type=int, default=10)\n",
        "    parser.add_argument(\"--duration\", type=int, default=0)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    producer = Producer({\"bootstrap.servers\": KAFKA_BOOTSTRAP_SERVERS})\n",
        "    interval = 1.0 / args.rate\n",
        "    total_sent = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            if args.duration > 0 and (time.time() - start_time) >= args.duration:\n",
        "                break\n",
        "            event = generate_event()\n",
        "            producer.produce(TOPIC_NAME, key=event[\"endpoint\"], value=json.dumps(event))\n",
        "            total_sent += 1\n",
        "            if total_sent % 10 == 0:\n",
        "                print(f\"전송: {total_sent}건\")\n",
        "            producer.poll(0)\n",
        "            time.sleep(interval)\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "    finally:\n",
        "        producer.flush()\n",
        "        print(f\"총 전송: {total_sent}건\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 환경 시작\n",
        "\n",
        "```bash\n",
        "cd docker\n",
        "docker compose up -d\n",
        "```\n",
        "\n",
        "### 실습 진행 방법\n",
        "\n",
        "**터미널 1: Spark 실습 코드 실행**\n",
        "```bash\n",
        "docker exec -it python-dev bash\n",
        "cd /app\n",
        "python streaming_lesson.py\n",
        "```\n",
        "\n",
        "**터미널 2: Kafka Producer 실행 (Kafka 예제 시)**\n",
        "```bash\n",
        "docker exec -it python-dev python producer.py --rate 10\n",
        "```\n",
        "\n",
        "> **핵심**: 각 셀을 실행하면 **Console에 결과가 출력**됩니다!\n",
        "> 스트리밍 쿼리를 멈추려면 `Ctrl+C`를 누르세요."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f8d12e",
      "metadata": {
        "id": "a2f8d12e"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: 첫 스트리밍 실행 - 바로 결과 보기\n",
        "\n",
        "복잡한 설정 없이 **바로 스트리밍 결과를 확인**해봅시다.\n",
        "Rate 소스는 테스트용 더미 데이터를 자동 생성하므로 외부 의존성이 없습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefbaa74",
      "metadata": {
        "id": "eefbaa74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c363a27f-7fb2-494d-90e1-c6299a08268f;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.8 in central\n",
            "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.8 in central\n",
            "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
            "\tfound org.lz4#lz4-java;1.8.0 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
            "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
            "\tfound commons-logging#commons-logging;1.1.3 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.8/spark-sql-kafka-0-10_2.12-3.5.8.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.8!spark-sql-kafka-0-10_2.12.jar (61ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.8/spark-token-provider-kafka-0-10_2.12-3.5.8.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.8!spark-token-provider-kafka-0-10_2.12.jar (46ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (230ms)\n",
            "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
            "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (40ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (51ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (1421ms)\n",
            "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
            "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (64ms)\n",
            "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.5/snappy-java-1.1.10.5.jar ...\n",
            "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.5!snappy-java.jar(bundle) (146ms)\n",
            "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
            "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (39ms)\n",
            "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
            "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (983ms)\n",
            "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
            "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (36ms)\n",
            ":: resolution report :: resolve 6370ms :: artifacts dl 3124ms\n",
            "\t:: modules in use:\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
            "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
            "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
            "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.8 from central in [default]\n",
            "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.8 from central in [default]\n",
            "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
            "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-c363a27f-7fb2-494d-90e1-c6299a08268f\n",
            "\tconfs: [default]\n",
            "\t11 artifacts copied, 0 already retrieved (57002kB/43ms)\n",
            "26/01/21 07:38:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "26/01/21 07:38:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SparkSession 생성 완료!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/21 07:39:15 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, lit, when, count, sum as spark_sum, avg, min, max,\n",
        "    from_json, to_json, struct, explode, split,\n",
        "    window, current_timestamp, expr, to_timestamp, round as spark_round\n",
        ")\n",
        "from pyspark.sql.types import (\n",
        "    StructType, StructField, StringType, IntegerType,\n",
        "    LongType, DoubleType, TimestampType, ArrayType, BooleanType\n",
        ")\n",
        "import time\n",
        "\n",
        "# SparkSession 생성 (로컬 모드)\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"Structured-Streaming-Lesson\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.8\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", 4)\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "print(\"SparkSession 생성 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef1b19cb",
      "metadata": {
        "id": "ef1b19cb"
      },
      "source": [
        "### 0.1 Rate 소스 원본 데이터 확인\n",
        "\n",
        "먼저 Rate 소스가 **어떤 데이터를 생성하는지** 직접 확인해봅시다.\n",
        "변환 없이 원본 그대로 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c81324cd",
      "metadata": {
        "id": "c81324cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Rate 소스 원본 데이터\n",
            "- Rate 소스가 자동 생성하는 데이터를 확인합니다\n",
            "- timestamp: 생성 시간\n",
            "- value: 0부터 시작하는 순차 번호\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/21 07:41:53 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7dddbfc4-f8d6-4bd4-bc4d-1cd39e531311. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "26/01/21 07:41:53 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---------+-----+\n",
            "|timestamp|value|\n",
            "+---------+-----+\n",
            "+---------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+-----------------------+-----+\n",
            "|timestamp              |value|\n",
            "+-----------------------+-----+\n",
            "|2026-01-21 07:41:54.415|0    |\n",
            "|2026-01-21 07:41:54.748|1    |\n",
            "|2026-01-21 07:41:55.082|2    |\n",
            "|2026-01-21 07:41:55.415|3    |\n",
            "|2026-01-21 07:41:55.748|4    |\n",
            "|2026-01-21 07:41:56.082|5    |\n",
            "+-----------------------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+-----------------------+-----+\n",
            "|timestamp              |value|\n",
            "+-----------------------+-----+\n",
            "|2026-01-21 07:41:56.415|6    |\n",
            "|2026-01-21 07:41:56.748|7    |\n",
            "|2026-01-21 07:41:57.082|8    |\n",
            "|2026-01-21 07:41:57.415|9    |\n",
            "|2026-01-21 07:41:57.748|10   |\n",
            "|2026-01-21 07:41:58.082|11   |\n",
            "|2026-01-21 07:41:58.415|12   |\n",
            "|2026-01-21 07:41:58.748|13   |\n",
            "|2026-01-21 07:41:59.082|14   |\n",
            "+-----------------------+-----+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 3\n",
            "-------------------------------------------\n",
            "+-----------------------+-----+\n",
            "|timestamp              |value|\n",
            "+-----------------------+-----+\n",
            "|2026-01-21 07:41:59.415|15   |\n",
            "|2026-01-21 07:41:59.748|16   |\n",
            "|2026-01-21 07:42:00.082|17   |\n",
            "|2026-01-21 07:42:00.415|18   |\n",
            "|2026-01-21 07:42:00.748|19   |\n",
            "|2026-01-21 07:42:01.082|20   |\n",
            "|2026-01-21 07:42:01.415|21   |\n",
            "|2026-01-21 07:42:01.748|22   |\n",
            "|2026-01-21 07:42:02.082|23   |\n",
            "+-----------------------+-----+\n",
            "\n",
            "\n",
            "원본 데이터 확인 완료!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Rate 소스 원본 데이터 확인\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Rate 소스 원본 데이터\")\n",
        "print(\"- Rate 소스가 자동 생성하는 데이터를 확인합니다\")\n",
        "print(\"- timestamp: 생성 시간\")\n",
        "print(\"- value: 0부터 시작하는 순차 번호\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate_raw = (\n",
        "    spark.readStream\n",
        "    .format(\"rate\")\n",
        "    .option(\"rowsPerSecond\", 3)  # 초당 3개 생성\n",
        "    .load()\n",
        ")\n",
        "\n",
        "# 원본 그대로 출력\n",
        "query = (\n",
        "    df_rate_raw.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(10)\n",
        "query.stop()\n",
        "print(\"\\n원본 데이터 확인 완료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b88ab76",
      "metadata": {
        "id": "2b88ab76"
      },
      "source": [
        "**Rate 소스 원본 출력**:\n",
        "```\n",
        "-------------------------------------------\n",
        "Batch: 0\n",
        "-------------------------------------------\n",
        "+--------------------+-----+\n",
        "|timestamp           |value|\n",
        "+--------------------+-----+\n",
        "|2026-01-21 10:00:00 |0    |\n",
        "|2026-01-21 10:00:00 |1    |\n",
        "|2026-01-21 10:00:00 |2    |\n",
        "+--------------------+-----+\n",
        "\n",
        "-------------------------------------------\n",
        "Batch: 1\n",
        "-------------------------------------------\n",
        "+--------------------+-----+\n",
        "|timestamp           |value|\n",
        "+--------------------+-----+\n",
        "|2026-01-21 10:00:03 |3    |\n",
        "|2026-01-21 10:00:03 |4    |\n",
        "|2026-01-21 10:00:03 |5    |\n",
        "|2026-01-21 10:00:04 |6    |\n",
        "|2026-01-21 10:00:04 |7    |\n",
        "|2026-01-21 10:00:04 |8    |\n",
        "+--------------------+-----+\n",
        "```\n",
        "\n",
        "**핵심 포인트**:\n",
        "- `timestamp`: Spark가 데이터를 생성한 시간\n",
        "- `value`: 0부터 시작하여 **순차적으로 증가**하는 long 타입 숫자\n",
        "- 각 배치에 새로운 데이터만 포함됨 (append 모드)\n",
        "- Rate 소스는 **외부 의존성 없이** 스트리밍 테스트 가능!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b1885d0",
      "metadata": {
        "id": "8b1885d0"
      },
      "source": [
        "### 0.2 Rate 소스에 변환 적용\n",
        "\n",
        "이제 원본 데이터에 변환을 적용해봅시다.\n",
        "**실행 후 콘솔 출력을 확인하세요!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9d095f83",
      "metadata": {
        "id": "9d095f83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Rate 소스 + 변환\n",
            "- 원본 value에 짝수/홀수 분류 추가\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "26/01/21 07:44:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0e30e3c3-65f7-4b8b-9b40-0951ea9843e5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
            "26/01/21 07:44:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------\n",
            "Batch: 0\n",
            "-------------------------------------------\n",
            "+---------+-----+-------+--------+\n",
            "|timestamp|value|is_even|category|\n",
            "+---------+-----+-------+--------+\n",
            "+---------+-----+-------+--------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 1\n",
            "-------------------------------------------\n",
            "+-----------------------+-----+-------+--------+\n",
            "|timestamp              |value|is_even|category|\n",
            "+-----------------------+-----+-------+--------+\n",
            "|2026-01-21 07:44:19.216|0    |true   |짝수    |\n",
            "|2026-01-21 07:44:19.416|1    |false  |홀수    |\n",
            "|2026-01-21 07:44:19.616|2    |true   |짝수    |\n",
            "|2026-01-21 07:44:19.816|3    |false  |홀수    |\n",
            "|2026-01-21 07:44:20.016|4    |true   |짝수    |\n",
            "+-----------------------+-----+-------+--------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 2\n",
            "-------------------------------------------\n",
            "+-----------------------+-----+-------+--------+\n",
            "|timestamp              |value|is_even|category|\n",
            "+-----------------------+-----+-------+--------+\n",
            "|2026-01-21 07:44:20.216|5    |false  |홀수    |\n",
            "|2026-01-21 07:44:20.416|6    |true   |짝수    |\n",
            "|2026-01-21 07:44:20.616|7    |false  |홀수    |\n",
            "|2026-01-21 07:44:20.816|8    |true   |짝수    |\n",
            "|2026-01-21 07:44:21.016|9    |false  |홀수    |\n",
            "|2026-01-21 07:44:21.216|10   |true   |짝수    |\n",
            "|2026-01-21 07:44:21.416|11   |false  |홀수    |\n",
            "|2026-01-21 07:44:21.616|12   |true   |짝수    |\n",
            "|2026-01-21 07:44:21.816|13   |false  |홀수    |\n",
            "|2026-01-21 07:44:22.016|14   |true   |짝수    |\n",
            "|2026-01-21 07:44:22.216|15   |false  |홀수    |\n",
            "|2026-01-21 07:44:22.416|16   |true   |짝수    |\n",
            "|2026-01-21 07:44:22.616|17   |false  |홀수    |\n",
            "|2026-01-21 07:44:22.816|18   |true   |짝수    |\n",
            "|2026-01-21 07:44:23.016|19   |false  |홀수    |\n",
            "+-----------------------+-----+-------+--------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 3\n",
            "-------------------------------------------\n",
            "+-----------------------+-----+-------+--------+\n",
            "|timestamp              |value|is_even|category|\n",
            "+-----------------------+-----+-------+--------+\n",
            "|2026-01-21 07:44:23.216|20   |true   |짝수    |\n",
            "|2026-01-21 07:44:23.416|21   |false  |홀수    |\n",
            "|2026-01-21 07:44:23.616|22   |true   |짝수    |\n",
            "|2026-01-21 07:44:23.816|23   |false  |홀수    |\n",
            "|2026-01-21 07:44:24.016|24   |true   |짝수    |\n",
            "|2026-01-21 07:44:24.216|25   |false  |홀수    |\n",
            "|2026-01-21 07:44:24.416|26   |true   |짝수    |\n",
            "|2026-01-21 07:44:24.616|27   |false  |홀수    |\n",
            "|2026-01-21 07:44:24.816|28   |true   |짝수    |\n",
            "|2026-01-21 07:44:25.016|29   |false  |홀수    |\n",
            "|2026-01-21 07:44:25.216|30   |true   |짝수    |\n",
            "|2026-01-21 07:44:25.416|31   |false  |홀수    |\n",
            "|2026-01-21 07:44:25.616|32   |true   |짝수    |\n",
            "|2026-01-21 07:44:25.816|33   |false  |홀수    |\n",
            "|2026-01-21 07:44:26.016|34   |true   |짝수    |\n",
            "+-----------------------+-----+-------+--------+\n",
            "\n",
            "-------------------------------------------\n",
            "Batch: 4\n",
            "-------------------------------------------\n",
            "+-----------------------+-----+-------+--------+\n",
            "|timestamp              |value|is_even|category|\n",
            "+-----------------------+-----+-------+--------+\n",
            "|2026-01-21 07:44:26.216|35   |false  |홀수    |\n",
            "|2026-01-21 07:44:26.416|36   |true   |짝수    |\n",
            "|2026-01-21 07:44:26.616|37   |false  |홀수    |\n",
            "|2026-01-21 07:44:26.816|38   |true   |짝수    |\n",
            "|2026-01-21 07:44:27.016|39   |false  |홀수    |\n",
            "|2026-01-21 07:44:27.216|40   |true   |짝수    |\n",
            "|2026-01-21 07:44:27.416|41   |false  |홀수    |\n",
            "|2026-01-21 07:44:27.616|42   |true   |짝수    |\n",
            "|2026-01-21 07:44:27.816|43   |false  |홀수    |\n",
            "|2026-01-21 07:44:28.016|44   |true   |짝수    |\n",
            "|2026-01-21 07:44:28.216|45   |false  |홀수    |\n",
            "|2026-01-21 07:44:28.416|46   |true   |짝수    |\n",
            "|2026-01-21 07:44:28.616|47   |false  |홀수    |\n",
            "|2026-01-21 07:44:28.816|48   |true   |짝수    |\n",
            "|2026-01-21 07:44:29.016|49   |false  |홀수    |\n",
            "+-----------------------+-----+-------+--------+\n",
            "\n",
            "\n",
            "변환 적용 스트리밍 종료!\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Rate 소스에 변환 적용\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Rate 소스 + 변환\")\n",
        "print(\"- 원본 value에 짝수/홀수 분류 추가\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = (\n",
        "    spark.readStream\n",
        "    .format(\"rate\")\n",
        "    .option(\"rowsPerSecond\", 5)\n",
        "    .load()\n",
        ")\n",
        "\n",
        "# 간단한 변환: 짝수/홀수 구분\n",
        "df_transformed = (\n",
        "    df_rate\n",
        "    .withColumn(\"is_even\", col(\"value\") % 2 == 0)\n",
        "    .withColumn(\"category\",\n",
        "        when(col(\"value\") % 2 == 0, \"짝수\").otherwise(\"홀수\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_transformed.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(12)\n",
        "query.stop()\n",
        "print(\"\\n변환 적용 스트리밍 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e8131f",
      "metadata": {
        "id": "87e8131f"
      },
      "source": [
        "**변환 적용 후 출력**:\n",
        "```\n",
        "-------------------------------------------\n",
        "Batch: 1\n",
        "-------------------------------------------\n",
        "+--------------------+-----+-------+--------+\n",
        "|timestamp           |value|is_even|category|\n",
        "+--------------------+-----+-------+--------+\n",
        "|2026-01-21 10:00:00 |0    |true   |짝수    |\n",
        "|2026-01-21 10:00:00 |1    |false  |홀수    |\n",
        "|2026-01-21 10:00:00 |2    |true   |짝수    |\n",
        "|2026-01-21 10:00:00 |3    |false  |홀수    |\n",
        "+--------------------+-----+-------+--------+\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d2bfd9",
      "metadata": {
        "id": "70d2bfd9"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 1: 스트리밍 소스 패턴\n",
        "\n",
        "Structured Streaming은 다양한 소스에서 데이터를 읽을 수 있습니다.\n",
        "\n",
        "| 소스 | 용도 | 사용 상황 |\n",
        "|------|------|----------|\n",
        "| `rate` | 테스트용 더미 데이터 | 개발/디버깅 |\n",
        "| `socket` | TCP 소켓 | 간단한 테스트 |\n",
        "| `kafka` | Kafka 토픽 | 프로덕션 실시간 처리 |\n",
        "| `file` | 파일 디렉토리 | 배치 스타일 스트리밍 |\n",
        "\n",
        "### 1.1 Rate 소스 (테스트용)\n",
        "\n",
        "초당 지정된 수의 행을 자동 생성합니다. **외부 의존성 없이** 스트리밍을 테스트할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8252e6a4",
      "metadata": {
        "id": "8252e6a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rate 소스 스키마:\n",
            "root\n",
            " |-- timestamp: timestamp (nullable = true)\n",
            " |-- value: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Rate 소스: 테스트용 더미 데이터 자동 생성\n",
        "# =============================================================================\n",
        "df_rate = (\n",
        "    spark.readStream\n",
        "    .format(\"rate\")\n",
        "    .option(\"rowsPerSecond\", 10)   # 초당 10개\n",
        "    .load()\n",
        ")\n",
        "\n",
        "print(\"Rate 소스 스키마:\")\n",
        "df_rate.printSchema()\n",
        "# root\n",
        "#  |-- timestamp: timestamp\n",
        "#  |-- value: long"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "607e3e8e",
      "metadata": {
        "id": "607e3e8e"
      },
      "source": [
        "**Rate 소스 옵션**:\n",
        "\n",
        "| 옵션 | 설명 | 기본값 |\n",
        "|------|------|--------|\n",
        "| `rowsPerSecond` | 초당 생성 행 수 | 1 |\n",
        "| `rampUpTime` | 목표 속도까지 도달 시간 | 0s |\n",
        "| `numPartitions` | 파티션 수 | Spark 기본값 |\n",
        "\n",
        "### 1.2 Socket 소스 (간단한 테스트)\n",
        "\n",
        "TCP 소켓에서 텍스트 데이터를 읽습니다.\n",
        "터미널에서 `nc -lk 9999` 실행 후 텍스트 입력하면 수신됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdfcaaa9",
      "metadata": {
        "id": "bdfcaaa9"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Socket 소스: TCP 소켓에서 텍스트 읽기 (참고용, 실행 시 nc 필요)\n",
        "# =============================================================================\n",
        "# df_socket = (\n",
        "#     spark.readStream\n",
        "#     .format(\"socket\")\n",
        "#     .option(\"host\", \"localhost\")\n",
        "#     .option(\"port\", 9999)\n",
        "#     .load()\n",
        "# )\n",
        "# 스키마: value (StringType) 하나의 컬럼\n",
        "print(\"Socket 소스는 프로덕션에서 사용하지 않습니다 (장애 복구 불가)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa33937",
      "metadata": {
        "id": "efa33937"
      },
      "source": [
        "### 1.3 Kafka 소스 (프로덕션)\n",
        "\n",
        "가장 많이 사용하는 프로덕션 소스입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4e89dc50",
      "metadata": {
        "id": "4e89dc50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이벤트 스키마 정의 완료!\n",
            "StructType([StructField('request_id', StringType(), True), StructField('user_id', StringType(), True), StructField('endpoint', StringType(), True), StructField('method', StringType(), True), StructField('status_code', IntegerType(), True), StructField('response_time_ms', IntegerType(), True), StructField('timestamp', StringType(), True)])\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Kafka 메시지 스키마\n",
        "# =============================================================================\n",
        "# Kafka에서 읽은 메시지는 다음 컬럼을 포함합니다:\n",
        "# - key: binary\n",
        "# - value: binary        ← 실제 데이터 (JSON 등)\n",
        "# - topic: string\n",
        "# - partition: integer\n",
        "# - offset: long\n",
        "# - timestamp: timestamp\n",
        "# - timestampType: integer\n",
        "\n",
        "# API 이벤트 JSON 스키마 정의\n",
        "event_schema = StructType([\n",
        "    StructField(\"request_id\", StringType(), True),\n",
        "    StructField(\"user_id\", StringType(), True),\n",
        "    StructField(\"endpoint\", StringType(), True),\n",
        "    StructField(\"method\", StringType(), True),\n",
        "    StructField(\"status_code\", IntegerType(), True),\n",
        "    StructField(\"response_time_ms\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", StringType(), True),\n",
        "])\n",
        "\n",
        "print(\"이벤트 스키마 정의 완료!\")\n",
        "print(event_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0bb920",
      "metadata": {
        "id": "4d0bb920"
      },
      "source": [
        "**Kafka 소스 주요 옵션**:\n",
        "\n",
        "| 옵션 | 설명 | 예시 |\n",
        "|------|------|------|\n",
        "| `subscribe` | 단일/여러 토픽 구독 | `\"topic1,topic2\"` |\n",
        "| `subscribePattern` | 패턴으로 토픽 구독 | `\"api-.*\"` |\n",
        "| `startingOffsets` | 시작 위치 | `\"earliest\"`, `\"latest\"` |\n",
        "| `maxOffsetsPerTrigger` | 트리거당 최대 오프셋 | `10000` |\n",
        "| `kafka.group.id` | Consumer Group ID | `\"my-group\"` |\n",
        "\n",
        "### 1.4 File 소스 (배치 스타일)\n",
        "\n",
        "디렉토리에 새로 추가되는 파일을 스트림으로 처리합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d99e810",
      "metadata": {
        "id": "2d99e810"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# File 소스: 새 파일 자동 감지 및 처리 (참고용)\n",
        "# =============================================================================\n",
        "file_schema = StructType([\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"value\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", StringType(), True),\n",
        "])\n",
        "\n",
        "# df_file = (\n",
        "#     spark.readStream\n",
        "#     .format(\"json\")                      # 또는 \"csv\", \"parquet\"\n",
        "#     .schema(file_schema)                 # 스키마 필수!\n",
        "#     .option(\"maxFilesPerTrigger\", 10)    # 트리거당 최대 파일 수\n",
        "#     .load(\"/data/input/\")                # 감시할 디렉토리\n",
        "# )\n",
        "print(\"File 소스는 스키마 지정이 필수입니다!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49746b27",
      "metadata": {
        "id": "49746b27"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: 스키마 정의와 JSON 파싱\n",
        "\n",
        "Kafka 메시지는 바이너리로 전송되므로 JSON 파싱이 필요합니다.\n",
        "\n",
        "### 2.1 주요 데이터 타입\n",
        "\n",
        "| 타입 | 설명 | 예시 |\n",
        "|------|------|------|\n",
        "| `StringType()` | 문자열 | `\"hello\"` |\n",
        "| `IntegerType()` | 32비트 정수 | `123` |\n",
        "| `LongType()` | 64비트 정수 | `123456789` |\n",
        "| `DoubleType()` | 64비트 실수 | `3.14` |\n",
        "| `BooleanType()` | 불리언 | `True` |\n",
        "| `TimestampType()` | 타임스탬프 | `2024-01-01 00:00:00` |\n",
        "| `ArrayType(elem)` | 배열 | `[1, 2, 3]` |\n",
        "| `MapType(k, v)` | 맵 | `{\"a\": 1}` |\n",
        "\n",
        "### 2.2 JSON 파싱 패턴"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48c1a804",
      "metadata": {
        "id": "48c1a804"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# JSON 파싱 시뮬레이션 (Rate 소스로)\n",
        "# =============================================================================\n",
        "# 실제 Kafka 파싱 흐름:\n",
        "# 1) col(\"value\").cast(\"string\"): 바이너리 → 문자열\n",
        "# 2) from_json(문자열, 스키마): JSON 문자열 → 구조체\n",
        "# 3) .alias(\"data\"): 구조체에 별칭 부여\n",
        "# 4) select(\"data.*\"): 구조체 내부 필드를 개별 컬럼으로 펼침\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"JSON 파싱 패턴 (Rate 소스로 시뮬레이션)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Rate 소스로 JSON 파싱 시뮬레이션\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 3).load()\n",
        "\n",
        "# value를 JSON처럼 변환하여 파싱 연습\n",
        "df_simulated = (\n",
        "    df_rate\n",
        "    .withColumn(\"json_str\",\n",
        "        struct(\n",
        "            col(\"value\").alias(\"id\"),\n",
        "            (col(\"value\") * 10).alias(\"amount\"),\n",
        "            col(\"timestamp\").alias(\"event_time\")\n",
        "        )\n",
        "    )\n",
        "    .select(\n",
        "        col(\"json_str.id\"),\n",
        "        col(\"json_str.amount\"),\n",
        "        col(\"json_str.event_time\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_simulated.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(10)\n",
        "query.stop()\n",
        "print(\"\\nJSON 파싱 시뮬레이션 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7496d6a8",
      "metadata": {
        "id": "7496d6a8"
      },
      "source": [
        "### 2.3 중첩 JSON 파싱 (실습)\n",
        "\n",
        "실제로 중첩된 JSON을 파싱해보겠습니다.\n",
        "시뮬레이션 데이터로 **중첩 구조 접근 방법**을 익힙니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "809d6f93",
      "metadata": {
        "id": "809d6f93"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 중첩 JSON 파싱 실습\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"중첩 JSON 파싱 실습\")\n",
        "print(\"- 중첩된 user 객체에서 필드 추출\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 중첩 JSON 스키마 정의\n",
        "# JSON 예시: {\"user\": {\"id\": \"U001\", \"name\": \"Alice\"}, \"event\": \"click\", \"amount\": 100}\n",
        "nested_schema = StructType([\n",
        "    StructField(\"user\", StructType([\n",
        "        StructField(\"id\", StringType(), True),\n",
        "        StructField(\"name\", StringType(), True),\n",
        "    ]), True),\n",
        "    StructField(\"event\", StringType(), True),\n",
        "    StructField(\"amount\", IntegerType(), True),\n",
        "])\n",
        "\n",
        "# 테스트용 JSON 데이터 생성 (배치 DataFrame으로 시뮬레이션)\n",
        "nested_json_data = [\n",
        "    ('{\"user\": {\"id\": \"U001\", \"name\": \"Alice\"}, \"event\": \"click\", \"amount\": 100}',),\n",
        "    ('{\"user\": {\"id\": \"U002\", \"name\": \"Bob\"}, \"event\": \"purchase\", \"amount\": 250}',),\n",
        "    ('{\"user\": {\"id\": \"U003\", \"name\": \"Charlie\"}, \"event\": \"view\", \"amount\": 0}',),\n",
        "]\n",
        "df_nested_raw = spark.createDataFrame(nested_json_data, [\"json_str\"])\n",
        "\n",
        "print(\"\\n[1단계] 원본 JSON 문자열:\")\n",
        "df_nested_raw.show(truncate=False)\n",
        "\n",
        "# JSON 파싱\n",
        "df_nested_parsed = (\n",
        "    df_nested_raw\n",
        "    .select(from_json(col(\"json_str\"), nested_schema).alias(\"data\"))\n",
        ")\n",
        "\n",
        "print(\"[2단계] from_json 적용 후 (struct 타입):\")\n",
        "df_nested_parsed.show(truncate=False)\n",
        "df_nested_parsed.printSchema()\n",
        "\n",
        "# 중첩 필드 접근\n",
        "df_nested_flat = (\n",
        "    df_nested_parsed\n",
        "    .select(\n",
        "        col(\"data.user.id\").alias(\"user_id\"),       # 중첩 접근: data.user.id\n",
        "        col(\"data.user.name\").alias(\"user_name\"),   # 중첩 접근: data.user.name\n",
        "        col(\"data.event\"),\n",
        "        col(\"data.amount\")\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"[3단계] 중첩 필드를 개별 컬럼으로 추출:\")\n",
        "df_nested_flat.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6ec4982",
      "metadata": {
        "id": "f6ec4982"
      },
      "source": [
        "**중첩 JSON 파싱 핵심**:\n",
        "```python\n",
        "# 1. 스키마 정의 - StructType 안에 StructType\n",
        "nested_schema = StructType([\n",
        "    StructField(\"user\", StructType([        # 중첩 객체\n",
        "        StructField(\"id\", StringType()),\n",
        "        StructField(\"name\", StringType()),\n",
        "    ])),\n",
        "    StructField(\"event\", StringType()),\n",
        "])\n",
        "\n",
        "# 2. 파싱 후 중첩 접근\n",
        "df.select(\n",
        "    col(\"data.user.id\").alias(\"user_id\"),   # 점(.) 표기법으로 접근\n",
        "    col(\"data.user.name\").alias(\"user_name\"),\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44562223",
      "metadata": {
        "id": "44562223"
      },
      "source": [
        "### 2.4 배열 타입 파싱 (실습)\n",
        "\n",
        "배열이 포함된 JSON을 파싱하고, **explode로 행 단위로 펼치는** 방법을 익힙니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b67eac33",
      "metadata": {
        "id": "b67eac33"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 배열 타입 JSON 파싱 실습\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"배열 타입 JSON 파싱 실습\")\n",
        "print(\"- 배열 필드를 explode로 펼치기\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 배열 포함 JSON 스키마\n",
        "# JSON 예시: {\"order_id\": \"O001\", \"items\": [\"laptop\", \"mouse\", \"keyboard\"]}\n",
        "array_schema = StructType([\n",
        "    StructField(\"order_id\", StringType(), True),\n",
        "    StructField(\"items\", ArrayType(StringType()), True),\n",
        "])\n",
        "\n",
        "# 테스트용 JSON 데이터\n",
        "array_json_data = [\n",
        "    ('{\"order_id\": \"O001\", \"items\": [\"laptop\", \"mouse\", \"keyboard\"]}',),\n",
        "    ('{\"order_id\": \"O002\", \"items\": [\"phone\", \"charger\"]}',),\n",
        "    ('{\"order_id\": \"O003\", \"items\": [\"tablet\"]}',),\n",
        "]\n",
        "df_array_raw = spark.createDataFrame(array_json_data, [\"json_str\"])\n",
        "\n",
        "print(\"\\n[1단계] 원본 JSON 문자열:\")\n",
        "df_array_raw.show(truncate=False)\n",
        "\n",
        "# JSON 파싱\n",
        "df_array_parsed = (\n",
        "    df_array_raw\n",
        "    .select(from_json(col(\"json_str\"), array_schema).alias(\"data\"))\n",
        "    .select(\"data.*\")\n",
        ")\n",
        "\n",
        "print(\"[2단계] 파싱 후 (items는 배열 타입):\")\n",
        "df_array_parsed.show(truncate=False)\n",
        "df_array_parsed.printSchema()\n",
        "\n",
        "# 배열을 개별 행으로 펼치기\n",
        "df_exploded = (\n",
        "    df_array_parsed\n",
        "    .select(\n",
        "        col(\"order_id\"),\n",
        "        explode(col(\"items\")).alias(\"item\")  # 배열 → 개별 행\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"[3단계] explode 적용 후 (각 아이템이 별도 행):\")\n",
        "df_exploded.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0938bc",
      "metadata": {
        "id": "8e0938bc"
      },
      "source": [
        "**배열 JSON 파싱 핵심**:\n",
        "```python\n",
        "# 1. 스키마 정의 - ArrayType 사용\n",
        "array_schema = StructType([\n",
        "    StructField(\"order_id\", StringType()),\n",
        "    StructField(\"items\", ArrayType(StringType())),  # 문자열 배열\n",
        "])\n",
        "\n",
        "# 2. explode로 배열을 개별 행으로\n",
        "df.select(\n",
        "    col(\"order_id\"),\n",
        "    explode(col(\"items\")).alias(\"item\")  # [\"a\",\"b\",\"c\"] → 3개 행\n",
        ")\n",
        "\n",
        "# 참고: 빈 배열이면 해당 행 제거됨\n",
        "# 빈 배열도 유지하려면 explode_outer() 사용\n",
        "```\n",
        "\n",
        "| 함수 | 빈 배열 처리 |\n",
        "|------|-------------|\n",
        "| `explode()` | 행 제거 |\n",
        "| `explode_outer()` | NULL로 유지 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97318b13",
      "metadata": {
        "id": "97318b13"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: 스트림 변환 패턴\n",
        "\n",
        "배치 DataFrame에서 사용하던 변환 연산을 스트림에도 동일하게 적용할 수 있습니다.\n",
        "\n",
        "### 3.1 지원되는 연산 vs 지원되지 않는 연산\n",
        "\n",
        "| 지원됨 | 지원 안 됨 |\n",
        "|--------|-----------|\n",
        "| `select`, `filter`, `where` | `count()`, `collect()` (직접 호출) |\n",
        "| `withColumn`, `drop` | `distinct()` (스트림 전체에 대해) |\n",
        "| `groupBy` + 집계 | `sort()` (전체 정렬) |\n",
        "| `join` (특정 조건) | `limit()`, `take()` |\n",
        "\n",
        "> 💡 **핵심**: 무한 데이터를 전제로 하므로, \"전체\" 개념이 필요한 연산은 지원되지 않음"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b574b6a",
      "metadata": {
        "id": "1b574b6a"
      },
      "source": [
        "### 3.2 filter / where"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f4961d6",
      "metadata": {
        "id": "5f4961d6"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# filter / where: 조건에 맞는 행만 선택\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"filter 변환 실습\")\n",
        "print(\"- value > 10 필터링\")\n",
        "print(\"- 짝수만 필터링\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\n",
        "\n",
        "# 여러 조건 AND\n",
        "df_filtered = df_rate.filter(\n",
        "    (col(\"value\") > 10) &\n",
        "    (col(\"value\") % 2 == 0)  # 10보다 크고 짝수\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_filtered.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(12)\n",
        "query.stop()\n",
        "print(\"\\nfilter 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53fecfe3",
      "metadata": {
        "id": "53fecfe3"
      },
      "source": [
        "### 3.3 select / withColumn / drop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05f546e",
      "metadata": {
        "id": "e05f546e"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# withColumn: 새 컬럼 추가 또는 기존 컬럼 변환\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"withColumn 변환 실습\")\n",
        "print(\"- 제곱값 계산\")\n",
        "print(\"- 상태 분류 (when/otherwise)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "df_enhanced = (\n",
        "    df_rate\n",
        "    # 제곱값 계산\n",
        "    .withColumn(\"squared\", col(\"value\") * col(\"value\"))\n",
        "    # 상태 분류\n",
        "    .withColumn(\n",
        "        \"category\",\n",
        "        when(col(\"value\") < 10, \"small\")\n",
        "        .when(col(\"value\") < 20, \"medium\")\n",
        "        .otherwise(\"large\")\n",
        "    )\n",
        "    # 필요한 컬럼만 선택\n",
        "    .select(\"timestamp\", \"value\", \"squared\", \"category\")\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_enhanced.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(12)\n",
        "query.stop()\n",
        "print(\"\\nwithColumn 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c06cd05d",
      "metadata": {
        "id": "c06cd05d"
      },
      "source": [
        "### 3.4 스트림 변환 체이닝"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b26f49f1",
      "metadata": {
        "id": "b26f49f1"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 변환 체이닝: 여러 변환을 파이프라인으로 연결\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"변환 체이닝 실습\")\n",
        "print(\"- 파생 컬럼 추가 → 필터링 → 선택\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\n",
        "\n",
        "df_processed = (\n",
        "    df_rate\n",
        "    # 1. 파생 컬럼 추가\n",
        "    .withColumn(\"is_even\", col(\"value\") % 2 == 0)\n",
        "    .withColumn(\"tens\", (col(\"value\") / 10).cast(\"int\"))\n",
        "    # 2. 필터링\n",
        "    .filter(col(\"is_even\") == True)\n",
        "    # 3. 필요한 컬럼만 선택\n",
        "    .select(\"timestamp\", \"value\", \"tens\")\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_processed.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(12)\n",
        "query.stop()\n",
        "print(\"\\n변환 체이닝 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8231af6",
      "metadata": {
        "id": "e8231af6"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Output Mode\n",
        "\n",
        "스트리밍 결과를 어떻게 출력할지 결정합니다.\n",
        "\n",
        "| 모드 | 설명 | 사용 상황 |\n",
        "|------|------|----------|\n",
        "| `append` | 새로 추가된 행만 출력 | 집계 없는 단순 변환 |\n",
        "| `update` | 변경된 행만 출력 | 집계 결과 중 업데이트된 것만 |\n",
        "| `complete` | 전체 결과 출력 | 전체 집계 결과가 필요할 때 |\n",
        "\n",
        "### Output Mode 선택 가이드\n",
        "\n",
        "```\n",
        "집계(groupBy)가 있나요?\n",
        "│\n",
        "├─ No → append 모드 (기본)\n",
        "│\n",
        "└─ Yes\n",
        "    │\n",
        "    ├─ 전체 결과가 필요한가?\n",
        "    │   ├─ Yes → complete 모드\n",
        "    │   └─ No → update 모드 (권장)\n",
        "    │\n",
        "    └─ 파일 저장이 목적인가?\n",
        "        └─ Yes → append + Watermark\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b193e53e",
      "metadata": {
        "id": "b193e53e"
      },
      "source": [
        "### 4.1 Append 모드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ed1890",
      "metadata": {
        "id": "90ed1890"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Append 모드: 새 행만 출력 (집계 없는 변환)\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Append 모드: 새 행만 출력\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "df_filtered = df_rate.filter(col(\"value\") > 5)\n",
        "\n",
        "query = (\n",
        "    df_filtered.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(10)\n",
        "query.stop()\n",
        "print(\"\\nAppend 모드 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a85cdaf8",
      "metadata": {
        "id": "a85cdaf8"
      },
      "source": [
        "### 4.2 Update 모드\n",
        "\n",
        "**시나리오**: 상품별 주문 수를 실시간으로 모니터링합니다.\n",
        "Update 모드는 **변경된 그룹만** 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f9112e",
      "metadata": {
        "id": "d6f9112e"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Update 모드: 변경된 집계 결과만 출력\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Update 모드: 상품별 주문 수\")\n",
        "print(\"- 새 주문이 들어온 상품만 출력됩니다\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "# 시뮬레이션: value를 상품 코드로 변환\n",
        "PRODUCTS = [\"노트북\", \"스마트폰\", \"태블릿\", \"이어폰\", \"충전기\"]\n",
        "\n",
        "df_orders = (\n",
        "    df_rate\n",
        "    .withColumn(\"product\",\n",
        "        when(col(\"value\") % 5 == 0, lit(\"노트북\"))\n",
        "        .when(col(\"value\") % 5 == 1, lit(\"스마트폰\"))\n",
        "        .when(col(\"value\") % 5 == 2, lit(\"태블릿\"))\n",
        "        .when(col(\"value\") % 5 == 3, lit(\"이어폰\"))\n",
        "        .otherwise(lit(\"충전기\"))\n",
        "    )\n",
        "    .groupBy(\"product\")\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"order_count\"),\n",
        "        spark_sum(\"value\").alias(\"total_value\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_orders.writeStream\n",
        "    .outputMode(\"update\")  # 변경된 그룹만!\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(15)\n",
        "query.stop()\n",
        "print(\"\\nUpdate 모드 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "809b2cb7",
      "metadata": {
        "id": "809b2cb7"
      },
      "source": [
        "**Update 모드 출력 예시**:\n",
        "```\n",
        "-------------------------------------------\n",
        "Batch: 1\n",
        "-------------------------------------------\n",
        "+--------+-----------+-----------+\n",
        "|product |order_count|total_value|\n",
        "+--------+-----------+-----------+\n",
        "|노트북  |3          |30         |   ← 새로 집계된 3개만 출력\n",
        "|스마트폰|2          |17         |\n",
        "+--------+-----------+-----------+\n",
        "\n",
        "-------------------------------------------\n",
        "Batch: 2\n",
        "-------------------------------------------\n",
        "+--------+-----------+-----------+\n",
        "|product |order_count|total_value|\n",
        "+--------+-----------+-----------+\n",
        "|노트북  |5          |80         |   ← count가 3→5로 변경됨\n",
        "|태블릿  |4          |52         |   ← 새로 변경된 그룹\n",
        "+--------+-----------+-----------+\n",
        "```\n",
        "\n",
        "**핵심**: 이번 배치에서 **값이 변경된 그룹만** 출력됩니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e996c7d",
      "metadata": {
        "id": "8e996c7d"
      },
      "source": [
        "### 4.3 Complete 모드\n",
        "\n",
        "**시나리오**: 동일한 상품별 집계를 Complete 모드로 실행합니다.\n",
        "Complete 모드는 **매번 전체 결과**를 출력합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17861aef",
      "metadata": {
        "id": "17861aef"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Complete 모드: 전체 집계 결과 출력\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Complete 모드: 매번 전체 상품 집계\")\n",
        "print(\"- 변경 유무와 관계없이 전체 출력\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "df_orders = (\n",
        "    df_rate\n",
        "    .withColumn(\"product\",\n",
        "        when(col(\"value\") % 5 == 0, lit(\"노트북\"))\n",
        "        .when(col(\"value\") % 5 == 1, lit(\"스마트폰\"))\n",
        "        .when(col(\"value\") % 5 == 2, lit(\"태블릿\"))\n",
        "        .when(col(\"value\") % 5 == 3, lit(\"이어폰\"))\n",
        "        .otherwise(lit(\"충전기\"))\n",
        "    )\n",
        "    .groupBy(\"product\")\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"order_count\"),\n",
        "        spark_sum(\"value\").alias(\"total_value\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_orders.writeStream\n",
        "    .outputMode(\"complete\")  # 전체 결과!\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(15)\n",
        "query.stop()\n",
        "print(\"\\nComplete 모드 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec207266",
      "metadata": {
        "id": "ec207266"
      },
      "source": [
        "**Complete 모드 출력 예시**:\n",
        "```\n",
        "-------------------------------------------\n",
        "Batch: 2\n",
        "-------------------------------------------\n",
        "+--------+-----------+-----------+\n",
        "|product |order_count|total_value|\n",
        "+--------+-----------+-----------+\n",
        "|노트북  |5          |80         |   ← 전체 5개 상품 모두 출력\n",
        "|스마트폰|4          |61         |\n",
        "|태블릿  |4          |52         |\n",
        "|이어폰  |3          |42         |\n",
        "|충전기  |3          |39         |\n",
        "+--------+-----------+-----------+\n",
        "```\n",
        "\n",
        "**핵심**: **항상 전체 집계 결과**가 출력됩니다!\n",
        "\n",
        "---\n",
        "\n",
        "### Update vs Complete 비교 정리\n",
        "\n",
        "| 구분 | Update | Complete |\n",
        "|------|--------|----------|\n",
        "| 출력 내용 | 변경된 그룹만 | 전체 그룹 |\n",
        "| 출력량 | 적음 | 많음 (그룹 수만큼) |\n",
        "| 메모리 사용 | 낮음 | 높음 |\n",
        "| 사용 상황 | 실시간 알림, 모니터링 | 대시보드, 전체 현황 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eec18a4",
      "metadata": {
        "id": "3eec18a4"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 5: Sink 패턴\n",
        "\n",
        "스트리밍 결과를 어디로 출력할지 결정합니다.\n",
        "\n",
        "| Sink | 용도 | Output Mode | 비고 |\n",
        "|------|------|-------------|------|\n",
        "| `console` | 개발/디버깅 | append, update, complete | 프로덕션 X |\n",
        "| `file` | 데이터 저장 | append만 | 체크포인트 필수 |\n",
        "| `kafka` | 이벤트 전달 | append, update | to_json 변환 필요 |\n",
        "| `foreachBatch` | 커스텀 처리 | append, update, complete | 자유도 높음 |\n",
        "| `memory` | 테스트 | append, complete | 임시 테이블로 쿼리 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1da7071",
      "metadata": {
        "id": "f1da7071"
      },
      "source": [
        "### 5.1 Console Sink (개발/디버깅)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "827f5c3f",
      "metadata": {
        "id": "827f5c3f"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Console Sink: 터미널에 결과 출력\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Console Sink: 디버깅용 출력\")\n",
        "print(\"- truncate=False: 긴 문자열 자르지 않음\")\n",
        "print(\"- numRows=20: 출력할 행 수\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "query = (\n",
        "    df_rate.writeStream\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .option(\"numRows\", 20)\n",
        "    .outputMode(\"append\")\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(10)\n",
        "query.stop()\n",
        "print(\"\\nConsole Sink 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114319c9",
      "metadata": {
        "id": "114319c9"
      },
      "source": [
        "### 5.2 foreachBatch Sink (커스텀 처리)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7704da64",
      "metadata": {
        "id": "7704da64"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# foreachBatch: 각 마이크로배치에 커스텀 로직 적용\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"foreachBatch: 배치별 통계 계산\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "def process_batch(batch_df, batch_id):\n",
        "    \"\"\"각 마이크로배치 처리 함수\"\"\"\n",
        "    count = batch_df.count()\n",
        "    if count > 0:\n",
        "        sum_val = batch_df.agg(spark_sum(\"value\")).collect()[0][0]\n",
        "        avg_val = batch_df.agg(avg(\"value\")).collect()[0][0]\n",
        "        print(f\"배치 {batch_id}: {count}건, 합계={sum_val}, 평균={avg_val:.2f}\")\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\n",
        "\n",
        "query = (\n",
        "    df_rate.writeStream\n",
        "    .foreachBatch(process_batch)\n",
        "    .trigger(processingTime=\"3 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(12)\n",
        "query.stop()\n",
        "print(\"\\nforeachBatch 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04e0e3c5",
      "metadata": {
        "id": "04e0e3c5"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Trigger 설정\n",
        "\n",
        "스트리밍 처리 주기를 결정합니다.\n",
        "\n",
        "| Trigger | 설명 | 사용 상황 |\n",
        "|---------|------|----------|\n",
        "| `processingTime` | 고정 주기로 처리 | 일반적인 실시간 처리 |\n",
        "| `once` | 한 번만 처리 후 종료 | 배치 스타일 처리 |\n",
        "| `availableNow` | 현재까지 데이터 한 번 처리 | 백필(backfill) |\n",
        "| `continuous` | 연속 처리 (밀리초 지연) | 초저지연 필요시 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "784579d4",
      "metadata": {
        "id": "784579d4"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 다양한 Trigger 비교\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Trigger 비교\")\n",
        "print(\"- processingTime: 주기적 처리\")\n",
        "print(\"- once=True: 한 번만 처리\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# processingTime Trigger\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "query = (\n",
        "    df_rate.writeStream\n",
        "    .format(\"console\")\n",
        "    .trigger(processingTime=\"2 seconds\")  # 2초마다 처리\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(8)\n",
        "query.stop()\n",
        "print(\"\\nTrigger 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f2da4b",
      "metadata": {
        "id": "c7f2da4b"
      },
      "source": [
        "### Trigger 선택 가이드\n",
        "\n",
        "| 상황 | 권장 Trigger | 주기 |\n",
        "|------|-------------|------|\n",
        "| 실시간 대시보드 | `processingTime` | 5~30초 |\n",
        "| 준실시간 분석 | `processingTime` | 1~5분 |\n",
        "| 알림/모니터링 | `processingTime` | 10~60초 |\n",
        "| 스케줄링된 배치 | `once` | - |\n",
        "| 대량 백필 | `availableNow` | - |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067564d7",
      "metadata": {
        "id": "067564d7"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 7: Tumbling Window 집계\n",
        "\n",
        "시간 윈도우를 사용한 집계입니다. 스트리밍의 핵심 패턴입니다.\n",
        "\n",
        "### Tumbling Window란?\n",
        "\n",
        "**겹치지 않는 고정 크기 윈도우**입니다. 각 이벤트는 정확히 하나의 윈도우에 속합니다.\n",
        "\n",
        "```\n",
        "시간축: 0분 ──── 5분 ──── 10분 ──── 15분\n",
        "\n",
        "윈도우1: [0분 ~ 5분)  ← 이벤트 A, B\n",
        "윈도우2:       [5분 ~ 10분)  ← 이벤트 C, D\n",
        "윈도우3:            [10분 ~ 15분)  ← 이벤트 E\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1612fa93",
      "metadata": {
        "id": "1612fa93"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Tumbling Window 집계\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Tumbling Window: 10초 윈도우\")\n",
        "print(\"- 10초 단위로 count, sum 집계\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "df_windowed = (\n",
        "    df_rate\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"10 seconds\")  # 10초 윈도우\n",
        "    )\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"count\"),\n",
        "        spark_sum(\"value\").alias(\"sum_value\"),\n",
        "        avg(\"value\").alias(\"avg_value\")\n",
        "    )\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"count\"),\n",
        "        col(\"sum_value\"),\n",
        "        spark_round(col(\"avg_value\"), 2).alias(\"avg_value\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_windowed.writeStream\n",
        "    .outputMode(\"update\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"5 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(25)\n",
        "query.stop()\n",
        "print(\"\\nTumbling Window 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07fae7a1",
      "metadata": {
        "id": "07fae7a1"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 8: Sliding Window 집계\n",
        "\n",
        "**윈도우가 겹칩니다**. 하나의 이벤트가 여러 윈도우에 속할 수 있습니다.\n",
        "이동 평균 계산 등에 활용됩니다.\n",
        "\n",
        "```\n",
        "윈도우 크기: 20초, 슬라이드 간격: 10초\n",
        "\n",
        "시간축: 0초 ──── 10초 ──── 20초 ──── 30초\n",
        "\n",
        "윈도우1: [0초 ──────────── 20초)\n",
        "윈도우2:      [10초 ──────────── 30초)\n",
        "\n",
        "이벤트 (15초): 윈도우1, 윈도우2 모두에 포함!\n",
        "```\n",
        "\n",
        "### Tumbling vs Sliding 비교\n",
        "\n",
        "| 구분 | Tumbling | Sliding |\n",
        "|------|----------|---------|\n",
        "| 윈도우 겹침 | X | O |\n",
        "| 이벤트 소속 | 1개 윈도우 | 여러 윈도우 |\n",
        "| 문법 | `window(col, size)` | `window(col, size, slide)` |\n",
        "| 용도 | 일반 집계 | 이동 평균, 트렌드 분석 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a60fd969",
      "metadata": {
        "id": "a60fd969"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Sliding Window: 이동 평균 계산\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Sliding Window: 20초 윈도우, 10초 슬라이드\")\n",
        "print(\"- 이동 평균 계산\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "df_sliding = (\n",
        "    df_rate\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"20 seconds\", \"10 seconds\")  # 윈도우 크기, 슬라이드 간격\n",
        "    )\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"count\"),\n",
        "        avg(\"value\").alias(\"moving_avg\")\n",
        "    )\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"count\"),\n",
        "        spark_round(col(\"moving_avg\"), 2).alias(\"moving_avg\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_sliding.writeStream\n",
        "    .outputMode(\"update\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"5 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(35)\n",
        "query.stop()\n",
        "print(\"\\nSliding Window 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7847c3f",
      "metadata": {
        "id": "d7847c3f"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 9: Watermark와 지연 데이터\n",
        "\n",
        "### Watermark란?\n",
        "\n",
        "실시간 시스템에서는 네트워크 지연 등으로 데이터가 늦게 도착할 수 있습니다.\n",
        "**Watermark**는 \"얼마나 늦은 데이터까지 기다릴 것인가\"를 정의합니다.\n",
        "\n",
        "```\n",
        "현재 처리 시간: 10:15\n",
        "\n",
        "[정상 데이터] 이벤트 시간 10:14 → 처리됨 ✓\n",
        "[지연 데이터] 이벤트 시간 10:05 → 10분 지연! 처리할까?\n",
        "```\n",
        "\n",
        "**Watermark 동작 원리**:\n",
        "```\n",
        "Watermark = max(event_time) - 허용지연시간\n",
        "\n",
        "예: max(event_time) = 10:15, 허용지연 = 10분\n",
        "    Watermark = 10:05\n",
        "    → 이벤트 시간 10:05 이전의 데이터는 무시됨 (윈도우 closed)\n",
        "    → 이벤트 시간 10:05 이후의 데이터는 처리됨\n",
        "```\n",
        "\n",
        "### Watermark의 두 가지 역할\n",
        "\n",
        "| 역할 | 설명 |\n",
        "|------|------|\n",
        "| **지연 데이터 처리** | 허용 시간 내 도착한 늦은 데이터 포함 |\n",
        "| **상태 정리** | 오래된 윈도우 상태를 메모리에서 제거 |\n",
        "\n",
        "### Watermark와 Output Mode\n",
        "\n",
        "| Output Mode | Watermark 역할 | 출력 타이밍 |\n",
        "|-------------|---------------|------------|\n",
        "| `update` | 상태 정리용 (선택) | 매 배치마다 변경분 출력 |\n",
        "| `append` | **필수** | 윈도우가 닫힌 후에만 출력 |\n",
        "| `complete` | 상태 정리용 (선택) | 매 배치마다 전체 출력 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be48e011",
      "metadata": {
        "id": "be48e011"
      },
      "source": [
        "### 9.1 Update 모드 + Watermark\n",
        "\n",
        "**특징**: 윈도우가 닫히기 전에도 중간 결과를 볼 수 있습니다.\n",
        "Watermark는 주로 **상태 정리**를 위해 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7878285",
      "metadata": {
        "id": "b7878285"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Update 모드 + Watermark\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Update 모드 + Watermark (10초)\")\n",
        "print(\"- 매 배치마다 변경된 윈도우 출력\")\n",
        "print(\"- Watermark 지나면 윈도우 상태 정리\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "df_update_watermark = (\n",
        "    df_rate\n",
        "    .withWatermark(\"timestamp\", \"10 seconds\")\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"10 seconds\")\n",
        "    )\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"count\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_update_watermark.writeStream\n",
        "    .outputMode(\"update\")  # 변경된 윈도우만 출력\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"5 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(30)\n",
        "query.stop()\n",
        "print(\"\\nUpdate + Watermark 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4ae787",
      "metadata": {
        "id": "bf4ae787"
      },
      "source": [
        "**Update 모드 + Watermark 출력 예시**:\n",
        "```\n",
        "-------------------------------------------\n",
        "Batch: 1 (시간: 10:00:05)\n",
        "-------------------------------------------\n",
        "+-------------------+-------------------+-----+\n",
        "|window_start       |window_end         |count|\n",
        "+-------------------+-------------------+-----+\n",
        "|2026-01-21 10:00:00|2026-01-21 10:00:10|25   |  ← 아직 열린 윈도우\n",
        "+-------------------+-------------------+-----+\n",
        "\n",
        "-------------------------------------------\n",
        "Batch: 2 (시간: 10:00:10)\n",
        "-------------------------------------------\n",
        "+-------------------+-------------------+-----+\n",
        "|window_start       |window_end         |count|\n",
        "+-------------------+-------------------+-----+\n",
        "|2026-01-21 10:00:00|2026-01-21 10:00:10|50   |  ← count 증가 (25→50)\n",
        "|2026-01-21 10:00:10|2026-01-21 10:00:20|15   |  ← 새 윈도우 시작\n",
        "+-------------------+-------------------+-----+\n",
        "\n",
        "-------------------------------------------\n",
        "Batch: 3 (시간: 10:00:25) - Watermark = 10:00:15\n",
        "-------------------------------------------\n",
        "+-------------------+-------------------+-----+\n",
        "|window_start       |window_end         |count|\n",
        "+-------------------+-------------------+-----+\n",
        "|2026-01-21 10:00:10|2026-01-21 10:00:20|50   |  ← 첫 번째 윈도우 사라짐 (closed)\n",
        "|2026-01-21 10:00:20|2026-01-21 10:00:30|25   |\n",
        "+-------------------+-------------------+-----+\n",
        "```\n",
        "\n",
        "**핵심 포인트**:\n",
        "- 윈도우가 닫히기 **전**에도 중간 결과 출력\n",
        "- Watermark를 지난 윈도우는 더 이상 출력되지 않음 (상태 정리됨)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "755ae337",
      "metadata": {
        "id": "755ae337"
      },
      "source": [
        "### 9.2 Append 모드 + Watermark (필수!)\n",
        "\n",
        "**특징**: 윈도우가 **확정된 후에만** 출력됩니다.\n",
        "윈도우 집계에서 append 모드를 사용하려면 **Watermark 필수**입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc23eb67",
      "metadata": {
        "id": "cc23eb67"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Append 모드 + Watermark (필수 조합)\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"Append 모드 + Watermark\")\n",
        "print(\"- 윈도우가 완전히 닫힌 후에만 출력\")\n",
        "print(\"- 파일 저장에 적합 (중복 없음)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "df_append_watermark = (\n",
        "    df_rate\n",
        "    .withWatermark(\"timestamp\", \"10 seconds\")  # append 모드에서는 필수!\n",
        "    .groupBy(\n",
        "        window(col(\"timestamp\"), \"10 seconds\")\n",
        "    )\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"count\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_append_watermark.writeStream\n",
        "    .outputMode(\"append\")  # 닫힌 윈도우만 출력\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"5 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(35)\n",
        "query.stop()\n",
        "print(\"\\nAppend + Watermark 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4174cc06",
      "metadata": {
        "id": "4174cc06"
      },
      "source": [
        "**Append 모드 + Watermark 출력 예시**:\n",
        "```\n",
        "-------------------------------------------\n",
        "Batch: 1~3 (시간: 10:00:00 ~ 10:00:15)\n",
        "-------------------------------------------\n",
        "(출력 없음 - 아직 닫힌 윈도우가 없음)\n",
        "\n",
        "-------------------------------------------\n",
        "Batch: 4 (시간: 10:00:22) - Watermark = 10:00:12\n",
        "-------------------------------------------\n",
        "+-------------------+-------------------+-----+\n",
        "|window_start       |window_end         |count|\n",
        "+-------------------+-------------------+-----+\n",
        "|2026-01-21 10:00:00|2026-01-21 10:00:10|50   |  ← 윈도우 닫힘! 한 번만 출력\n",
        "+-------------------+-------------------+-----+\n",
        "\n",
        "-------------------------------------------\n",
        "Batch: 5 (시간: 10:00:32) - Watermark = 10:00:22\n",
        "-------------------------------------------\n",
        "+-------------------+-------------------+-----+\n",
        "|window_start       |window_end         |count|\n",
        "+-------------------+-------------------+-----+\n",
        "|2026-01-21 10:00:10|2026-01-21 10:00:20|50   |  ← 다음 윈도우 닫힘\n",
        "+-------------------+-------------------+-----+\n",
        "```\n",
        "\n",
        "**핵심 포인트**:\n",
        "- 윈도우가 **완전히 닫힌 후** 한 번만 출력\n",
        "- 출력이 **늦게** 나옴 (Watermark 대기 시간만큼)\n",
        "- 파일 저장 시 **중복 없이** 안전하게 저장 가능\n",
        "\n",
        "---\n",
        "\n",
        "### Update vs Append 비교 (Watermark 있을 때)\n",
        "\n",
        "| 구분 | Update | Append |\n",
        "|------|--------|--------|\n",
        "| 출력 타이밍 | 즉시 (중간 결과) | 윈도우 닫힌 후 |\n",
        "| 같은 윈도우 출력 | 여러 번 (값 변경 시) | 한 번만 |\n",
        "| 용도 | 실시간 모니터링 | 파일 저장, 정확한 결과 |\n",
        "| Watermark | 선택 (상태 정리용) | **필수** |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be26ad9",
      "metadata": {
        "id": "5be26ad9"
      },
      "source": [
        "### Watermark 설정 기준\n",
        "\n",
        "| 상황 | 권장 Watermark | 이유 |\n",
        "|------|---------------|------|\n",
        "| 네트워크 안정적 | 1~5분 | 빠른 결과 출력 |\n",
        "| 네트워크 불안정 | 10~30분 | 지연 데이터 처리 |\n",
        "| 데이터 정확성 중요 | 30분~1시간 | 손실 최소화 |\n",
        "| 빠른 응답 중요 | 1~2분 | 즉시 결과 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe62ccc7",
      "metadata": {
        "id": "fe62ccc7"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 10: 체크포인트와 장애 복구\n",
        "\n",
        "스트리밍 애플리케이션의 상태를 저장하고 장애 시 복구합니다.\n",
        "\n",
        "**체크포인트 저장 내용**:\n",
        "- 현재 처리 오프셋 (어디까지 읽었는지)\n",
        "- 집계 상태 (윈도우별 중간 결과)\n",
        "- 메타데이터 (쿼리 설정 등)\n",
        "\n",
        "**체크포인트 디렉토리 구조**:\n",
        "```\n",
        "/data/checkpoints/my-query/\n",
        "├── commits/          # 커밋된 배치 정보\n",
        "├── offsets/          # 소스 오프셋 (Kafka 등)\n",
        "├── sources/          # 소스 메타데이터\n",
        "├── state/            # 집계 상태\n",
        "└── metadata          # 쿼리 메타데이터\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f50f41b",
      "metadata": {
        "id": "7f50f41b"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 체크포인트 설정 예제\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"체크포인트: /data/checkpoints/lesson\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "df_rate = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
        "\n",
        "df_windowed = (\n",
        "    df_rate\n",
        "    .groupBy(window(col(\"timestamp\"), \"10 seconds\"))\n",
        "    .agg(count(\"*\").alias(\"count\"))\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_windowed.writeStream\n",
        "    .outputMode(\"update\")\n",
        "    .format(\"console\")\n",
        "    .option(\"checkpointLocation\", \"/data/checkpoints/lesson\")  # 체크포인트!\n",
        "    .trigger(processingTime=\"5 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(15)\n",
        "query.stop()\n",
        "print(\"\\n체크포인트 실습 종료!\")\n",
        "print(\"체크포인트 저장됨: /data/checkpoints/lesson\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dccf5c9",
      "metadata": {
        "id": "1dccf5c9"
      },
      "source": [
        "### 체크포인트 주의사항\n",
        "\n",
        "| 주의사항 | 설명 |\n",
        "|---------|------|\n",
        "| 경로 고유성 | 쿼리마다 다른 체크포인트 경로 사용 |\n",
        "| 삭제 금지 | 체크포인트 삭제 시 처음부터 재처리 |\n",
        "| 스키마 변경 | 스키마 변경 시 새 체크포인트 필요 |\n",
        "| 저장소 신뢰성 | HDFS, S3 등 신뢰할 수 있는 저장소 사용 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f8400b7",
      "metadata": {
        "id": "7f8400b7"
      },
      "source": [
        "---\n",
        "\n",
        "## 실전 팁: 스트리밍 코드 개발 시 중간 결과 확인하기\n",
        "\n",
        "Kafka 과제를 진행하기 전에, **스트리밍 코드를 단계별로 개발하는 방법**을 익혀봅시다.\n",
        "한 번에 전체 코드를 작성하지 말고, **중간 결과를 확인**하며 진행하세요!\n",
        "\n",
        "### 팁 1: DataFrame 타입과 스키마 확인\n",
        "\n",
        "```python\n",
        "# 스트리밍 DataFrame인지 확인\n",
        "print(f\"isStreaming: {df.isStreaming}\")\n",
        "\n",
        "# 스키마 확인 (컬럼명, 타입)\n",
        "df.printSchema()\n",
        "```\n",
        "\n",
        "### 팁 2: 배치 DataFrame으로 먼저 테스트\n",
        "\n",
        "스트리밍 코드를 작성하기 전에, **배치 모드**로 로직을 검증하세요.\n",
        "\n",
        "```python\n",
        "# 방법 1: 샘플 데이터로 배치 테스트\n",
        "sample_data = [\n",
        "    ('{\"user_id\": \"U001\", \"amount\": 100}',),\n",
        "    ('{\"user_id\": \"U002\", \"amount\": 200}',),\n",
        "]\n",
        "df_batch = spark.createDataFrame(sample_data, [\"value\"])\n",
        "df_parsed = df_batch.select(from_json(col(\"value\"), schema).alias(\"data\"))\n",
        "df_parsed.show()  # 바로 결과 확인!\n",
        "\n",
        "# 방법 2: Kafka에서 배치로 읽기 (startingOffsets + endingOffsets)\n",
        "df_batch = (\n",
        "    spark.read  # readStream이 아닌 read!\n",
        "    .format(\"kafka\")\n",
        "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
        "    .option(\"subscribe\", \"api-events\")\n",
        "    .option(\"startingOffsets\", \"earliest\")\n",
        "    .option(\"endingOffsets\", \"latest\")\n",
        "    .load()\n",
        ")\n",
        "df_batch.select(col(\"value\").cast(\"string\")).show(5, truncate=False)\n",
        "```\n",
        "\n",
        "### 팁 3: 단계별 스트리밍 개발 순서\n",
        "\n",
        "```python\n",
        "# Step 1: 원본 데이터 확인 (JSON 파싱 전)\n",
        "query = (\n",
        "    df_kafka\n",
        "    .select(col(\"value\").cast(\"string\").alias(\"raw_json\"))\n",
        "    .writeStream.format(\"console\").start()\n",
        ")\n",
        "# 출력 확인 후 query.stop()\n",
        "\n",
        "# Step 2: JSON 파싱 결과 확인\n",
        "query = (\n",
        "    df_kafka\n",
        "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
        "    .select(\"data.*\")\n",
        "    .writeStream.format(\"console\").start()\n",
        ")\n",
        "# 모든 컬럼이 제대로 파싱되는지 확인\n",
        "\n",
        "# Step 3: 변환 적용 후 확인\n",
        "query = (\n",
        "    df_parsed\n",
        "    .filter(col(\"status_code\") >= 400)\n",
        "    .writeStream.format(\"console\").start()\n",
        ")\n",
        "\n",
        "# Step 4: 최종 집계 추가\n",
        "# ...\n",
        "```\n",
        "\n",
        "### 팁 4: 디버깅용 Console Sink 옵션\n",
        "\n",
        "```python\n",
        ".format(\"console\")\n",
        ".option(\"truncate\", False)    # 긴 문자열 자르지 않음\n",
        ".option(\"numRows\", 50)        # 출력 행 수 늘리기\n",
        ".trigger(processingTime=\"5 seconds\")  # 빠른 피드백\n",
        "```\n",
        "\n",
        "### 팁 5: 스트리밍 쿼리 상태 확인\n",
        "\n",
        "```python\n",
        "# 쿼리 실행 중에\n",
        "print(query.status)           # 현재 상태\n",
        "print(query.lastProgress)     # 마지막 배치 정보\n",
        "print(query.recentProgress)   # 최근 배치들 정보\n",
        "\n",
        "# 에러 확인\n",
        "print(query.exception())      # 예외 발생 시 확인\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 실전 예시: Kafka 데이터 단계별 확인\n",
        "\n",
        "```python\n",
        "# 1단계: Kafka 원본 확인\n",
        "df_kafka = spark.readStream.format(\"kafka\")...load()\n",
        "df_kafka.select(col(\"value\").cast(\"string\")).writeStream.format(\"console\").start()\n",
        "# → {\"user_id\": \"U001\", \"status_code\": 200, ...} 확인\n",
        "\n",
        "# 2단계: 스키마 정의 후 파싱\n",
        "schema = StructType([...])\n",
        "df_parsed = df_kafka.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
        "df_parsed.writeStream.format(\"console\").start()\n",
        "# → 모든 필드가 정상 파싱되는지 확인 (NULL이면 스키마 문제!)\n",
        "\n",
        "# 3단계: 필드 펼치기\n",
        "df_flat = df_parsed.select(\"data.*\")\n",
        "df_flat.printSchema()  # 스키마 확인\n",
        "df_flat.writeStream.format(\"console\").start()\n",
        "\n",
        "# 4단계: 필터/변환 추가\n",
        "df_filtered = df_flat.filter(col(\"status_code\") >= 500)\n",
        "df_filtered.writeStream.format(\"console\").start()\n",
        "\n",
        "# 5단계: 집계 추가 (최종)\n",
        "df_agg = df_filtered.groupBy(window(...)).agg(...)\n",
        "df_agg.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "```\n",
        "\n",
        "> **핵심**: 한 단계씩 확인하며 진행하면 에러 원인을 쉽게 찾을 수 있습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be93490",
      "metadata": {
        "id": "7be93490"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 11: Kafka 종합 실습\n",
        "\n",
        "지금까지 배운 내용을 종합하여 실제 API 이벤트를 처리해봅시다.\n",
        "\n",
        "> **실행 전**: Producer를 먼저 실행하세요!\n",
        "> ```bash\n",
        "> docker exec -it python-dev python producer.py --rate 10\n",
        "> ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fcc94c",
      "metadata": {
        "id": "24fcc94c"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 종합 실습: Kafka → 윈도우 집계 → Console\n",
        "# =============================================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"종합 실습: Kafka API 이벤트 처리\")\n",
        "print(\"- 30초 윈도우로 endpoint별 통계\")\n",
        "print(\"- Producer 실행 필요: python producer.py --rate 10\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Kafka에서 읽기\n",
        "df_kafka = (\n",
        "    spark.readStream\n",
        "    .format(\"kafka\")\n",
        "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
        "    .option(\"subscribe\", \"api-events\")\n",
        "    .option(\"startingOffsets\", \"latest\")\n",
        "    .load()\n",
        ")\n",
        "\n",
        "# JSON 파싱\n",
        "df_parsed = (\n",
        "    df_kafka\n",
        "    .select(from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\"))\n",
        "    .select(\"data.*\")\n",
        "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
        ")\n",
        "\n",
        "# 30초 윈도우 집계\n",
        "df_windowed = (\n",
        "    df_parsed\n",
        "    .withWatermark(\"event_time\", \"1 minute\")\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"30 seconds\"),\n",
        "        col(\"endpoint\")\n",
        "    )\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"request_count\"),\n",
        "        spark_round(avg(\"response_time_ms\"), 2).alias(\"avg_latency\"),\n",
        "        spark_sum(when(col(\"status_code\") >= 400, 1).otherwise(0)).alias(\"error_count\")\n",
        "    )\n",
        "    .withColumn(\"error_rate\", spark_round(col(\"error_count\") / col(\"request_count\") * 100, 1))\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"endpoint\"),\n",
        "        col(\"request_count\"),\n",
        "        col(\"avg_latency\"),\n",
        "        col(\"error_count\"),\n",
        "        col(\"error_rate\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_windowed.writeStream\n",
        "    .outputMode(\"update\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"10 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "time.sleep(45)\n",
        "query.stop()\n",
        "print(\"\\nKafka 종합 실습 종료!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf4e371",
      "metadata": {
        "id": "4bf4e371"
      },
      "source": [
        "---\n",
        "\n",
        "## 퀴즈\n",
        "\n",
        "**Q1. Structured Streaming에서 윈도우 집계 시 Watermark의 역할은?**\n",
        "\n",
        "- A) 데이터 압축률을 높인다\n",
        "- B) 지연 데이터를 얼마나 기다릴지 결정한다\n",
        "- C) 출력 파일 크기를 제한한다\n",
        "- D) 네트워크 대역폭을 조절한다\n",
        "\n",
        "<details>\n",
        "<summary>정답</summary>\n",
        "B) 지연 데이터를 얼마나 기다릴지 결정한다\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. 다음 중 append 모드에서 사용 가능한 쿼리는?**\n",
        "\n",
        "- A) `df.groupBy(\"col\").count()` (Watermark 없음)\n",
        "- B) `df.filter(col(\"x\") > 10)`\n",
        "- C) `df.distinct()`\n",
        "- D) `df.sort(\"col\")`\n",
        "\n",
        "<details>\n",
        "<summary>정답</summary>\n",
        "B) filter는 집계 없는 변환이므로 append 가능\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. Sliding Window `window(col(\"time\"), \"10 minutes\", \"2 minutes\")`의 의미는?**\n",
        "\n",
        "- A) 10분 윈도우, 10분마다 새 윈도우\n",
        "- B) 10분 윈도우, 2분마다 새 윈도우\n",
        "- C) 2분 윈도우, 10분마다 새 윈도우\n",
        "- D) 2분 윈도우, 2분마다 새 윈도우\n",
        "\n",
        "<details>\n",
        "<summary>정답</summary>\n",
        "B) 10분 윈도우, 2분마다 새 윈도우 (겹침)\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Q4. Kafka 메시지를 Spark에서 처리할 때 JSON 파싱 순서는?**\n",
        "\n",
        "- A) from_json → cast(\"string\") → select(\"data.*\")\n",
        "- B) cast(\"string\") → from_json → select(\"data.*\")\n",
        "- C) select(\"data.*\") → cast(\"string\") → from_json\n",
        "- D) from_json → select(\"data.*\") → cast(\"string\")\n",
        "\n",
        "<details>\n",
        "<summary>정답</summary>\n",
        "B) cast(\"string\") → from_json → select(\"data.*\")\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "**Q5. 체크포인트의 역할이 아닌 것은?**\n",
        "\n",
        "- A) 장애 시 처리 상태 복구\n",
        "- B) 중복 처리 방지\n",
        "- C) 쿼리 성능 최적화\n",
        "- D) 집계 중간 상태 저장\n",
        "\n",
        "<details>\n",
        "<summary>정답</summary>\n",
        "C) 체크포인트는 성능 최적화가 아닌 장애 복구용\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a2cbcc",
      "metadata": {
        "id": "41a2cbcc"
      },
      "source": [
        "---\n",
        "\n",
        "## 트러블슈팅 가이드\n",
        "\n",
        "### 에러 1: Output Mode 불일치\n",
        "\n",
        "**에러 메시지**:\n",
        "```\n",
        "AnalysisException: Append output mode not supported when there are streaming\n",
        "aggregations on streaming DataFrames/DataSets without watermark\n",
        "```\n",
        "\n",
        "**원인**: 집계 쿼리에 append 모드를 사용했으나 Watermark가 없음\n",
        "\n",
        "**해결**:\n",
        "```python\n",
        "# 방법 1: Watermark 추가\n",
        "df.withWatermark(\"event_time\", \"10 minutes\")\n",
        "  .groupBy(window(...))\n",
        "  .agg(...)\n",
        "  .writeStream.outputMode(\"append\")\n",
        "\n",
        "# 방법 2: update 또는 complete 모드 사용\n",
        "df.groupBy(...).agg(...)\n",
        "  .writeStream.outputMode(\"update\")  # Watermark 없이도 가능\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 에러 2: JSON 파싱 실패\n",
        "\n",
        "**증상**: 모든 컬럼이 null로 나옴\n",
        "\n",
        "**원인들**:\n",
        "1. 스키마와 실제 JSON 구조 불일치\n",
        "2. `cast(\"string\")` 누락\n",
        "3. 필드명 대소문자 불일치\n",
        "\n",
        "**디버깅 방법**:\n",
        "```python\n",
        "# 1. 원본 데이터 확인 (바이너리)\n",
        "df_kafka.select(\"value\").writeStream.format(\"console\").start()\n",
        "\n",
        "# 2. 문자열 변환 후 확인\n",
        "df_kafka.select(col(\"value\").cast(\"string\").alias(\"json_str\")) \\\n",
        "  .writeStream.format(\"console\").start()\n",
        "\n",
        "# 3. 실제 JSON 구조 확인 후 스키마 수정\n",
        "```\n",
        "\n",
        "**해결**: 스키마를 실제 JSON과 일치시키기\n",
        "```python\n",
        "# 실제 JSON: {\"user_id\": \"U001\", \"timestamp\": \"2024-01-01T10:00:00\"}\n",
        "schema = StructType([\n",
        "    StructField(\"user_id\", StringType(), True),     # 필드명 정확히\n",
        "    StructField(\"timestamp\", StringType(), True),   # 타입 맞추기\n",
        "])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 에러 3: Kafka 연결 실패\n",
        "\n",
        "**에러 메시지**:\n",
        "```\n",
        "KafkaException: Failed to connect to kafka:9092\n",
        "```\n",
        "\n",
        "**해결**:\n",
        "```bash\n",
        "# 1. Kafka 상태 확인\n",
        "docker compose ps\n",
        "\n",
        "# 2. Kafka 로그 확인\n",
        "docker logs kafka\n",
        "\n",
        "# 3. 컨테이너 내부에서 연결 테스트\n",
        "docker exec -it python-dev bash\n",
        "nc -zv kafka 9092\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 에러 4: 체크포인트 충돌\n",
        "\n",
        "**에러 메시지**:\n",
        "```\n",
        "Cannot update the query with the given checkpoint\n",
        "StreamingQueryException: Cannot recover from a checkpoint with different query\n",
        "```\n",
        "\n",
        "**원인**: 쿼리 변경 후 기존 체크포인트 사용\n",
        "\n",
        "**해결**:\n",
        "```bash\n",
        "# 체크포인트 삭제 후 재시작\n",
        "rm -rf /data/checkpoints/query-name\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 에러 5: 윈도우 집계에서 데이터가 안 보임\n",
        "\n",
        "**증상**: 윈도우 집계 결과가 출력되지 않음\n",
        "\n",
        "**원인들**:\n",
        "1. Watermark + append 모드: 윈도우가 닫힐 때까지 대기\n",
        "2. Trigger 주기가 너무 김\n",
        "3. 데이터 유입 없음\n",
        "\n",
        "**해결**:\n",
        "```python\n",
        "# 방법 1: update 모드로 즉시 확인\n",
        ".outputMode(\"update\")\n",
        "\n",
        "# 방법 2: Trigger 주기 단축\n",
        ".trigger(processingTime=\"5 seconds\")\n",
        "\n",
        "# 방법 3: Watermark 지연 시간 단축 (테스트용)\n",
        ".withWatermark(\"event_time\", \"30 seconds\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 에러 6: 메모리 부족\n",
        "\n",
        "**에러 메시지**:\n",
        "```\n",
        "java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
        "```\n",
        "\n",
        "**해결**:\n",
        "```python\n",
        "# 1. 파티션 수 줄이기\n",
        ".config(\"spark.sql.shuffle.partitions\", 4)\n",
        "\n",
        "# 2. 드라이버/Executor 메모리 증가\n",
        ".config(\"spark.driver.memory\", \"2g\")\n",
        ".config(\"spark.executor.memory\", \"2g\")\n",
        "\n",
        "# 3. Watermark 설정으로 상태 정리 (집계 쿼리)\n",
        ".withWatermark(\"event_time\", \"10 minutes\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 빠른 디버깅 체크리스트\n",
        "\n",
        "| 증상 | 확인 사항 |\n",
        "|------|----------|\n",
        "| 데이터 안 나옴 | Producer 실행 중? / Kafka 연결? |\n",
        "| 모두 null | 스키마 일치? / cast(\"string\")? |\n",
        "| 집계 안 됨 | Output Mode? / Watermark? |\n",
        "| 에러 메시지 | 로그 레벨을 DEBUG로 |\n",
        "| OOM | 파티션 수 / Watermark |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03674d0b",
      "metadata": {
        "id": "03674d0b"
      },
      "source": [
        "---\n",
        "\n",
        "## 과제: 실시간 API 모니터링\n",
        "\n",
        "### 시나리오\n",
        "\n",
        "운영팀에서 **실시간 API 모니터링**을 요청했습니다.\n",
        "Kafka에서 API 이벤트를 수신하여 다양한 지표를 실시간으로 계산해야 합니다.\n",
        "\n",
        "> **사전 준비**: Producer 실행\n",
        "> ```bash\n",
        "> docker exec -it python-dev python producer.py --rate 10\n",
        "> ```\n",
        "\n",
        "---\n",
        "\n",
        "### 과제 1: 서버 에러 실시간 모니터링\n",
        "\n",
        "**목표**: status_code가 500 이상인 서버 에러를 실시간으로 콘솔에 출력하세요.\n",
        "\n",
        "**조건**:\n",
        "- 출력 컬럼: event_time, endpoint, status_code, response_time_ms\n",
        "- 5초마다 처리\n",
        "- append 모드\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "- `filter(col(\"status_code\") >= 500)`\n",
        "- `outputMode(\"append\")` (집계 없으므로)\n",
        "- `trigger(processingTime=\"5 seconds\")`\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_kafka = (\n",
        "    spark.readStream\n",
        "    .format(\"kafka\")\n",
        "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
        "    .option(\"subscribe\", \"api-events\")\n",
        "    .option(\"startingOffsets\", \"latest\")\n",
        "    .load()\n",
        ")\n",
        "\n",
        "df_parsed = (\n",
        "    df_kafka\n",
        "    .select(from_json(col(\"value\").cast(\"string\"), event_schema).alias(\"data\"))\n",
        "    .select(\"data.*\")\n",
        "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\")))\n",
        ")\n",
        "\n",
        "df_errors = (\n",
        "    df_parsed\n",
        "    .filter(col(\"status_code\") >= 500)\n",
        "    .select(\"event_time\", \"endpoint\", \"status_code\", \"response_time_ms\")\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_errors.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"5 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### 과제 2: 5분 윈도우 집계 분석\n",
        "\n",
        "**목표**: 5분 윈도우로 endpoint별 다음 지표를 계산하여 콘솔에 출력하세요.\n",
        "\n",
        "**계산 지표**:\n",
        "- total_requests: 총 요청 수\n",
        "- avg_latency: 평균 응답시간\n",
        "- error_count: 에러 수 (status_code >= 400)\n",
        "- error_rate: 에러율 (%)\n",
        "\n",
        "**조건**:\n",
        "- Watermark: 2분 (테스트용으로 짧게)\n",
        "- 30초마다 출력\n",
        "- update 모드\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "- `withWatermark(\"event_time\", \"2 minutes\")`\n",
        "- `outputMode(\"update\")` (콘솔 출력)\n",
        "- `sum(when(..., 1).otherwise(0))`: 조건부 합계\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_windowed = (\n",
        "    df_parsed\n",
        "    .withWatermark(\"event_time\", \"2 minutes\")\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"5 minutes\"),\n",
        "        col(\"endpoint\")\n",
        "    )\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_requests\"),\n",
        "        avg(\"response_time_ms\").alias(\"avg_latency\"),\n",
        "        spark_sum(when(col(\"status_code\") >= 400, 1).otherwise(0)).alias(\"error_count\")\n",
        "    )\n",
        "    .withColumn(\"error_rate\", col(\"error_count\") / col(\"total_requests\") * 100)\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"endpoint\"),\n",
        "        col(\"total_requests\"),\n",
        "        spark_round(col(\"avg_latency\"), 2).alias(\"avg_latency\"),\n",
        "        col(\"error_count\"),\n",
        "        spark_round(col(\"error_rate\"), 1).alias(\"error_rate\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_windowed.writeStream\n",
        "    .outputMode(\"update\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"30 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### 과제 3: 응답시간 이동 평균 모니터링\n",
        "\n",
        "**목표**: 10분 윈도우, 2분 슬라이드로 endpoint별 응답시간 이동 평균을 계산하세요.\n",
        "\n",
        "**출력 지표**:\n",
        "- moving_avg: 10분 이동 평균\n",
        "- min_latency: 최소 응답시간\n",
        "- max_latency: 최대 응답시간\n",
        "- sample_count: 샘플 수\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "- Sliding Window: `window(col, \"10 minutes\", \"2 minutes\")`\n",
        "- `outputMode(\"update\")` (콘솔 출력)\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_moving_avg = (\n",
        "    df_parsed\n",
        "    .withWatermark(\"event_time\", \"15 minutes\")\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"10 minutes\", \"2 minutes\"),\n",
        "        col(\"endpoint\")\n",
        "    )\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"sample_count\"),\n",
        "        avg(\"response_time_ms\").alias(\"moving_avg\"),\n",
        "        min(\"response_time_ms\").alias(\"min_latency\"),\n",
        "        max(\"response_time_ms\").alias(\"max_latency\")\n",
        "    )\n",
        "    .select(\n",
        "        col(\"window.start\").alias(\"window_start\"),\n",
        "        col(\"window.end\").alias(\"window_end\"),\n",
        "        col(\"endpoint\"),\n",
        "        col(\"sample_count\"),\n",
        "        spark_round(col(\"moving_avg\"), 2).alias(\"moving_avg\"),\n",
        "        col(\"min_latency\"),\n",
        "        col(\"max_latency\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_moving_avg.writeStream\n",
        "    .outputMode(\"update\")\n",
        "    .format(\"console\")\n",
        "    .option(\"truncate\", False)\n",
        "    .trigger(processingTime=\"30 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### 과제 4: foreachBatch로 커스텀 처리\n",
        "\n",
        "**목표**: foreachBatch를 사용하여 각 배치마다:\n",
        "1. 배치 통계 출력 (총 건수, 에러 수)\n",
        "2. 에러 비율이 30% 이상이면 경고 메시지 출력\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "- `foreachBatch(process_function)`\n",
        "- `batch_df.count()`, `batch_df.filter(...).count()`\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "def process_batch(batch_df, batch_id):\n",
        "    \"\"\"배치별 통계 및 경고\"\"\"\n",
        "    if batch_df.isEmpty():\n",
        "        print(f\"배치 {batch_id}: 데이터 없음\")\n",
        "        return\n",
        "\n",
        "    total = batch_df.count()\n",
        "    errors = batch_df.filter(col(\"status_code\") >= 400).count()\n",
        "    error_rate = (errors / total) * 100 if total > 0 else 0\n",
        "\n",
        "    print(f\"=== 배치 {batch_id} ===\")\n",
        "    print(f\"  총 요청: {total}\")\n",
        "    print(f\"  에러: {errors}\")\n",
        "    print(f\"  에러율: {error_rate:.1f}%\")\n",
        "\n",
        "    if error_rate >= 30:\n",
        "        print(f\"  [경고] 에러율이 {error_rate:.1f}%로 높습니다!\")\n",
        "\n",
        "query = (\n",
        "    df_parsed.writeStream\n",
        "    .foreachBatch(process_batch)\n",
        "    .trigger(processingTime=\"10 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination()\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "### 보너스 과제: 에러 이벤트 Kafka 전달\n",
        "\n",
        "**목표**: status_code >= 400인 에러 이벤트를 `error-alerts` 토픽으로 전송하세요.\n",
        "\n",
        "**조건**:\n",
        "- key: endpoint\n",
        "- value: 전체 이벤트 JSON\n",
        "- 체크포인트: `/data/checkpoints/error-alerts`\n",
        "\n",
        "<details>\n",
        "<summary>힌트</summary>\n",
        "\n",
        "- `to_json(struct(\"*\"))`: 모든 컬럼을 JSON으로\n",
        "- `format(\"kafka\")`\n",
        "- `option(\"topic\", \"error-alerts\")`\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>모범 답안</summary>\n",
        "\n",
        "```python\n",
        "df_error_events = (\n",
        "    df_parsed\n",
        "    .filter(col(\"status_code\") >= 400)\n",
        "    .select(\n",
        "        col(\"endpoint\").alias(\"key\"),\n",
        "        to_json(struct(\n",
        "            col(\"event_time\"),\n",
        "            col(\"request_id\"),\n",
        "            col(\"endpoint\"),\n",
        "            col(\"status_code\"),\n",
        "            col(\"response_time_ms\")\n",
        "        )).alias(\"value\")\n",
        "    )\n",
        ")\n",
        "\n",
        "query = (\n",
        "    df_error_events.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .format(\"kafka\")\n",
        "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
        "    .option(\"topic\", \"error-alerts\")\n",
        "    .option(\"checkpointLocation\", \"/data/checkpoints/error-alerts\")\n",
        "    .trigger(processingTime=\"10 seconds\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination()\n",
        "```\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abcda328",
      "metadata": {
        "id": "abcda328"
      },
      "source": [
        "---\n",
        "\n",
        "## 핵심 요약\n",
        "\n",
        "### 스트리밍 소스\n",
        "```python\n",
        "# Kafka (프로덕션)\n",
        "spark.readStream.format(\"kafka\").option(\"subscribe\", \"topic\").load()\n",
        "\n",
        "# Rate (테스트)\n",
        "spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 10).load()\n",
        "```\n",
        "\n",
        "### JSON 파싱\n",
        "```python\n",
        "df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
        "  .select(\"data.*\")\n",
        "```\n",
        "\n",
        "### Output Mode\n",
        "| 모드 | 용도 |\n",
        "|------|------|\n",
        "| `append` | 집계 없는 변환, 파일 저장 |\n",
        "| `update` | 집계 결과 변경분 |\n",
        "| `complete` | 전체 집계 결과 |\n",
        "\n",
        "### 윈도우 집계\n",
        "```python\n",
        "# Tumbling (겹치지 않음)\n",
        "window(col(\"time\"), \"5 minutes\")\n",
        "\n",
        "# Sliding (겹침)\n",
        "window(col(\"time\"), \"10 minutes\", \"2 minutes\")\n",
        "```\n",
        "\n",
        "### Watermark\n",
        "```python\n",
        "df.withWatermark(\"event_time\", \"10 minutes\")\n",
        "  .groupBy(window(...))\n",
        "  .agg(...)\n",
        "```\n",
        "\n",
        "### 체크포인트\n",
        "```python\n",
        ".option(\"checkpointLocation\", \"/path/to/checkpoint\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c8e62d",
      "metadata": {
        "id": "b1c8e62d"
      },
      "source": [
        "---\n",
        "\n",
        "## 세션 종료"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d79053d5",
      "metadata": {
        "id": "d79053d5"
      },
      "outputs": [],
      "source": [
        "spark.stop()\n",
        "print(\"SparkSession 종료 완료!\")\n",
        "print(\"\\n수업을 마칩니다. 수고하셨습니다!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
