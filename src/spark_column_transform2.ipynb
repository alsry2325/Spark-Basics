{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a72ac1fe",
   "metadata": {},
   "source": [
    "집계와 조인\n",
    "이 교시를 마치면 다음을 할 수 있습니다:\n",
    "\n",
    "groupBy()와 agg()로 그룹별 집계를 수행할 수 있다\n",
    "count(), sum(), avg(), min(), max() 집계 함수를 활용할 수 있다\n",
    "다양한 조인 타입(inner, left, right, outer)을 이해하고 적용할 수 있다\n",
    "pivot()으로 데이터를 피벗 테이블 형태로 변환할 수 있다\n",
    "Spark SQL을 DataFrame과 함께 활용할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ce39db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 완료!\n",
      "직원: 50명, 부서: 6개, 매출: 100건\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 환경 설정: SparkSession 및 테스트 데이터 준비\n",
    "# -----------------------------------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when, count, sum, avg, min, max,\n",
    "    countDistinct, first, last, collect_list, collect_set,\n",
    "    round as spark_round, expr\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Aggregation-Join\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 테스트 데이터 디렉토리\n",
    "os.makedirs(\"/tmp/spark_tutorial\", exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 테스트 데이터 1: 직원 정보\n",
    "# -----------------------------------------------------------------------------\n",
    "employees = pd.DataFrame({\n",
    "    \"emp_id\": [f\"E{i:03d}\" for i in range(1, 51)],\n",
    "    \"name\": [f\"Employee_{i}\" for i in range(1, 51)],\n",
    "    \"department\": np.random.choice(\n",
    "        [\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\"], 50\n",
    "    ),\n",
    "    \"salary\": np.random.randint(40000, 120000, 50),\n",
    "    \"age\": np.random.randint(25, 55, 50),\n",
    "    \"hire_year\": np.random.choice([2020, 2021, 2022, 2023, 2024], 50),\n",
    "})\n",
    "employees.to_csv(\"/tmp/spark_tutorial/employees.csv\", index=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 테스트 데이터 2: 부서 정보 (조인용)\n",
    "# -----------------------------------------------------------------------------\n",
    "departments = pd.DataFrame({\n",
    "    \"dept_name\": [\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\", \"Legal\"],\n",
    "    \"dept_head\": [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\", \"Frank\"],\n",
    "    \"budget\": [500000, 300000, 200000, 150000, 400000, 100000],\n",
    "    \"location\": [\"Seoul\", \"Busan\", \"Seoul\", \"Daegu\", \"Seoul\", \"Incheon\"],\n",
    "})\n",
    "departments.to_csv(\"/tmp/spark_tutorial/departments.csv\", index=False)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 테스트 데이터 3: 매출 데이터 (시계열)\n",
    "# -----------------------------------------------------------------------------\n",
    "sales = pd.DataFrame({\n",
    "    \"date\": pd.date_range(\"2026-01-01\", periods=100, freq=\"D\").strftime(\"%Y-%m-%d\"),\n",
    "    \"product\": np.random.choice([\"A\", \"B\", \"C\"], 100),\n",
    "    \"region\": np.random.choice([\"Seoul\", \"Busan\", \"Daegu\"], 100),\n",
    "    \"amount\": np.random.randint(100, 1000, 100),\n",
    "    \"quantity\": np.random.randint(1, 20, 100),\n",
    "})\n",
    "sales.to_csv(\"/tmp/spark_tutorial/sales.csv\", index=False)\n",
    "\n",
    "# DataFrame 로드\n",
    "df_emp = spark.read.csv(\"/tmp/spark_tutorial/employees.csv\", header=True, inferSchema=True)\n",
    "df_dept = spark.read.csv(\"/tmp/spark_tutorial/departments.csv\", header=True, inferSchema=True)\n",
    "df_sales = spark.read.csv(\"/tmp/spark_tutorial/sales.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"데이터 로드 완료!\")\n",
    "print(f\"직원: {df_emp.count()}명, 부서: {df_dept.count()}개, 매출: {df_sales.count()}건\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7153a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56c10584",
   "metadata": {},
   "source": [
    "##View(뷰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358563d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL 테이블 등록 완료: employees, departments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/20 06:34:34 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# createOrReplaceTempView(): SQL 테이블 등록\n",
    "# -----------------------------------------------------------------------------\n",
    "#\n",
    "# createOrReplaceTempView(이름): 임시 뷰로 등록\n",
    "# - 해당 SparkSession 내에서만 유효\n",
    "# - 세션 종료 시 자동 삭제\n",
    "# - 같은 이름의 뷰가 있으면 덮어쓰기 (Replace)\n",
    "\n",
    "# DataFrame을 SQL 테이블(뷰)로 등록\n",
    "df_emp.createOrReplaceTempView(\"employees\")\n",
    "df_dept.createOrReplaceTempView(\"departments\")\n",
    "\n",
    "print(\"SQL 테이블 등록 완료: employees, departments\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 등록 후 사용 예시\n",
    "# -----------------------------------------------------------------------------\n",
    "# 이제 SQL 문법으로 DataFrame을 조회할 수 있습니다!\n",
    "# spark.sql(\"SELECT * FROM employees WHERE salary > 5000\")\n",
    "# spark.sql(\"SELECT dept_id, AVG(salary) FROM employees GROUP BY dept_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0901b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SQL 쿼리 결과 ===\n",
      "+-----------+------+--------+--------+\n",
      "| department|인원수|평균급여|최고급여|\n",
      "+-----------+------+--------+--------+\n",
      "|         HR|    13|85255.23|  112409|\n",
      "|  Marketing|    10| 76457.5|  117189|\n",
      "|      Sales|    10| 70509.7|  111211|\n",
      "|    Finance|    10| 82034.8|  118953|\n",
      "|Engineering|     7|79437.14|  107563|\n",
      "+-----------+------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# spark.sql(): SQL 쿼리 실행\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# spark.sql(쿼리문): SQL 실행 후 DataFrame 반환\n",
    "# 복잡한 로직을 SQL로 작성 가능\n",
    "\n",
    "# SQL로 부서별 통계\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        department,\n",
    "        COUNT(*) as `인원수`,\n",
    "        ROUND(AVG(salary), 2) as `평균급여`,\n",
    "        MAX(salary) as `최고급여`\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY `인원수` DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL 쿼리 결과 ===\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03f1354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SQL 쿼리 결과 ===\n",
      "+-----------+------+--------+--------+\n",
      "| department|인원수|평균급여|최고급여|\n",
      "+-----------+------+--------+--------+\n",
      "|         HR|    13|85255.23|  112409|\n",
      "|  Marketing|    10| 76457.5|  117189|\n",
      "|      Sales|    10| 70509.7|  111211|\n",
      "|    Finance|    10| 82034.8|  118953|\n",
      "|Engineering|     7|79437.14|  107563|\n",
      "+-----------+------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# spark.sql(): SQL 쿼리 실행\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# spark.sql(쿼리문): SQL 실행 후 DataFrame 반환\n",
    "# 복잡한 로직을 SQL로 작성 가능\n",
    "\n",
    "# SQL로 부서별 통계\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        department,\n",
    "        COUNT(*) as `인원수`,\n",
    "        ROUND(AVG(salary), 2) as `평균급여`,\n",
    "        MAX(salary) as `최고급여`\n",
    "    FROM employees\n",
    "    GROUP BY department\n",
    "    ORDER BY `인원수` DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL 쿼리 결과 ===\")\n",
    "sql_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06289fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SQL 조인 결과 ===\n",
      "+-----------+-----------+------+---------+--------+\n",
      "|       name| department|salary|dept_head|location|\n",
      "+-----------+-----------+------+---------+--------+\n",
      "|Employee_10|    Finance|118953|      Eve|   Seoul|\n",
      "| Employee_9|  Marketing|117189|  Charlie|   Seoul|\n",
      "|Employee_26|  Marketing|114065|  Charlie|   Seoul|\n",
      "|Employee_15|         HR|112409|    Diana|   Daegu|\n",
      "|Employee_16|      Sales|111211|      Bob|   Busan|\n",
      "| Employee_6|      Sales|109479|      Bob|   Busan|\n",
      "|Employee_39|Engineering|107563|    Alice|   Seoul|\n",
      "| Employee_5|    Finance|107121|      Eve|   Seoul|\n",
      "| Employee_8|  Marketing|106557|  Charlie|   Seoul|\n",
      "|Employee_17|         HR|105697|    Diana|   Daegu|\n",
      "+-----------+-----------+------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# SQL: 조인\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# SQL로 조인 쿼리\n",
    "sql_join = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        e.name,\n",
    "        e.department,\n",
    "        e.salary,\n",
    "        d.dept_head,\n",
    "        d.location\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.dept_name\n",
    "    WHERE e.salary >= 70000\n",
    "    ORDER BY e.salary DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL 조인 결과 ===\")\n",
    "sql_join.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf4530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# expr(): SQL 표현식을 DataFrame에서 사용\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# expr(): SQL 표현식을 Column으로 변환\n",
    "# select(), withColumn() 등에서 SQL 문법 사용 가능\n",
    "\n",
    "# SQL 표현식으로 새 컬럼 추가\n",
    "df_with_expr = df_emp.withColumn(\n",
    "    \"salary_grade\",\n",
    "    expr(\"CASE WHEN salary >= 80000 THEN 'High' ELSE 'Normal' END\")\n",
    ").withColumn(\n",
    "    \"bonus\",\n",
    "    expr(\"salary * 0.1\")  # 급여의 10%\n",
    ")\n",
    "\n",
    "print(\"=== expr() 사용 예시 ===\")\n",
    "df_with_expr.select(\"name\", \"salary\", \"salary_grade\", \"bonus\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0fae8f",
   "metadata": {},
   "source": [
    "### 실습 6-1: 임시 뷰 등록\n",
    "#### df_emp를 \"emp\"라는 이름의 임시 뷰로 등록하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726135c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emp.createOrReplaceTempView(\"emp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd874c32",
   "metadata": {},
   "source": [
    "#### 실습 6-2: SQL 쿼리 실행\n",
    "##### SQL로 employees 테이블에서 salary가 70000 이상인 직원의 name과 salary를 조회하세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f98e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      name|salary|\n",
      "+----------+------+\n",
      "|Employee_2| 88555|\n",
      "|Employee_4| 75920|\n",
      "|Employee_5|107121|\n",
      "|Employee_6|109479|\n",
      "|Employee_8|106557|\n",
      "+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_result = spark.sql('''\n",
    "            SELECT\n",
    "                name,\n",
    "                salary\n",
    "            FROM employees\n",
    "            WHERE salary >= 70000\n",
    "'''\n",
    ")\n",
    "sql_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450a2290",
   "metadata": {},
   "source": [
    "#### 실습 6-3: SQL 집계\n",
    "##### SQL로 부서별 평균 급여를 조회하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ebd581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "| department|평균급여|\n",
      "+-----------+--------+\n",
      "|         HR|85255.23|\n",
      "|  Marketing| 76457.5|\n",
      "|      Sales| 70509.7|\n",
      "|Engineering|79437.14|\n",
      "|    Finance| 82034.8|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_avg_result = spark.sql('''\n",
    "            SELECT\n",
    "                department,\n",
    "                ROUND(AVG(salary), 2) as `평균급여`\n",
    "            FROM employees \n",
    "            GROUP BY department\n",
    "''')\n",
    "sql_avg_result.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d357ffe",
   "metadata": {},
   "source": [
    "#### 실습 6-4: expr 사용\n",
    "##### expr()을 사용하여 salary의 5%를 \"bonus\" 컬럼으로 추가하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e22e91cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-------+\n",
      "|      name|salary|  bonus|\n",
      "+----------+------+-------+\n",
      "|Employee_1| 63483|3174.15|\n",
      "|Employee_2| 88555|4427.75|\n",
      "|Employee_3| 57159|2857.95|\n",
      "|Employee_4| 75920|3796.00|\n",
      "|Employee_5|107121|5356.05|\n",
      "+----------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_emp.withColumn(\"bonus\", expr(\"salary * 0.05\")).select(\"name\", \"salary\", \"bonus\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18e51e",
   "metadata": {},
   "source": [
    "### 채워야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d656a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20d127b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 완료!\n",
      "직원: 30명, 매출: 90건\n",
      "+------+---------------+-----------------+-----------+-------+----------+\n",
      "|emp_id|           name|            email| department| salary| join_date|\n",
      "+------+---------------+-----------------+-----------+-------+----------+\n",
      "|  E001|   Employee 1  | emp1@company.com|  Marketing|50000.0|2020-01-15|\n",
      "|  E002|   Employee 2  | emp2@company.com|       NULL|   NULL|2020-02-29|\n",
      "|  E003|   Employee 3  | emp3@company.com|Engineering|70000.0|2020-04-14|\n",
      "|  E004|   Employee 4  | emp4@company.com|  Marketing|80000.0|2020-05-29|\n",
      "|  E005|   Employee 5  | emp5@company.com|  Marketing|   NULL|2020-07-13|\n",
      "|  E006|   Employee 6  | emp6@company.com|       NULL|60000.0|2020-08-27|\n",
      "|  E007|   Employee 7  | emp7@company.com|Engineering|90000.0|2020-10-11|\n",
      "|  E008|   Employee 8  | emp8@company.com|Engineering|55000.0|2020-11-25|\n",
      "|  E009|   Employee 9  | emp9@company.com|  Marketing|   NULL|2021-01-09|\n",
      "|  E010|  Employee 10  |emp10@company.com|      Sales|75000.0|2021-02-23|\n",
      "+------+---------------+-----------------+-----------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 환경 설정\n",
    "# -----------------------------------------------------------------------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, when,\n",
    "    # NULL 처리\n",
    "    coalesce, isnan,\n",
    "    # 문자열 함수\n",
    "    concat, concat_ws, substring, length, trim, ltrim, rtrim,\n",
    "    upper, lower, initcap, regexp_replace, regexp_extract, split,\n",
    "    lpad, rpad,\n",
    "    # 날짜/시간 함수\n",
    "    current_date, current_timestamp, to_date, to_timestamp, date_format,\n",
    "    year, month, dayofmonth, dayofweek, hour, minute,\n",
    "    date_add, date_sub, datediff, months_between, trunc,\n",
    "    # 집계/윈도우\n",
    "    count, sum, avg, min, max, first, last,\n",
    "    row_number, rank, dense_rank, lag, lead,\n",
    "    # 기타\n",
    "    round as spark_round, expr,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Advanced-Patterns\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", 10) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "os.makedirs(\"/tmp/spark_tutorial\", exist_ok=True)\n",
    "np.random.seed(42)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 테스트 데이터: 다양한 상황을 포함한 직원 데이터\n",
    "# -----------------------------------------------------------------------------\n",
    "employees = pd.DataFrame({\n",
    "    \"emp_id\": [f\"E{i:03d}\" for i in range(1, 31)],\n",
    "    \"name\": [f\"  Employee {i}  \" for i in range(1, 31)],  # 앞뒤 공백\n",
    "    \"email\": [f\"emp{i}@company.com\" for i in range(1, 31)],\n",
    "    \"department\": np.random.choice(\n",
    "        [\"Engineering\", \"Sales\", \"Marketing\", None], 30\n",
    "    ),\n",
    "    \"salary\": [\n",
    "        50000, None, 70000, 80000, None,  # NULL 포함\n",
    "        60000, 90000, 55000, None, 75000,\n",
    "        65000, 85000, None, 72000, 68000,\n",
    "        None, 95000, 62000, 78000, None,\n",
    "        58000, 82000, 67000, None, 73000,\n",
    "        69000, 88000, None, 76000, 71000\n",
    "    ],\n",
    "    \"join_date\": pd.date_range(\"2020-01-15\", periods=30, freq=\"45D\").strftime(\"%Y-%m-%d\"),\n",
    "})\n",
    "employees.to_csv(\"/tmp/spark_tutorial/employees_advanced.csv\", index=False)\n",
    "\n",
    "# 매출 시계열 데이터\n",
    "sales = pd.DataFrame({\n",
    "    \"date\": pd.date_range(\"2026-01-01\", periods=90, freq=\"D\").strftime(\"%Y-%m-%d\"),\n",
    "    \"product\": np.random.choice([\"A\", \"B\", \"C\"], 90),\n",
    "    \"region\": np.random.choice([\"Seoul\", \"Busan\", \"Daegu\"], 90),\n",
    "    \"amount\": np.random.randint(100, 1000, 90),\n",
    "})\n",
    "sales.to_csv(\"/tmp/spark_tutorial/sales_ts.csv\", index=False)\n",
    "\n",
    "# DataFrame 로드\n",
    "df = spark.read.csv(\"/tmp/spark_tutorial/employees_advanced.csv\", header=True, inferSchema=True)\n",
    "df_sales = spark.read.csv(\"/tmp/spark_tutorial/sales_ts.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"데이터 로드 완료!\")\n",
    "print(f\"직원: {df.count()}명, 매출: {df_sales.count()}건\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07922f55",
   "metadata": {},
   "source": [
    "#### Window 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d5c8665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 부서별 급여 순위 ===\n",
      "+-----------+---------------+-------+-------+----+----------+\n",
      "| department|           name| salary|row_num|rank|dense_rank|\n",
      "+-----------+---------------+-------+-------+----+----------+\n",
      "|       NULL|  Employee 17  |95000.0|      1|   1|         1|\n",
      "|       NULL|  Employee 19  |78000.0|      2|   2|         2|\n",
      "|       NULL|  Employee 29  |76000.0|      3|   3|         3|\n",
      "|       NULL|  Employee 25  |73000.0|      4|   4|         4|\n",
      "|       NULL|  Employee 30  |71000.0|      5|   5|         5|\n",
      "|       NULL|  Employee 15  |68000.0|      6|   6|         6|\n",
      "|       NULL|  Employee 18  |62000.0|      7|   7|         7|\n",
      "|       NULL|   Employee 6  |60000.0|      8|   8|         8|\n",
      "|Engineering|   Employee 7  |90000.0|      1|   1|         1|\n",
      "|Engineering|  Employee 22  |82000.0|      2|   2|         2|\n",
      "|Engineering|   Employee 3  |70000.0|      3|   3|         3|\n",
      "|Engineering|   Employee 8  |55000.0|      4|   4|         4|\n",
      "|  Marketing|  Employee 12  |85000.0|      1|   1|         1|\n",
      "|  Marketing|   Employee 4  |80000.0|      2|   2|         2|\n",
      "|  Marketing|  Employee 14  |72000.0|      3|   3|         3|\n",
      "+-----------+---------------+-------+-------+----+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Window 기본: 순위 함수\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Window 정의: 부서별로 그룹화, 급여 내림차순 정렬\n",
    "window_rank = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# 순위 함수 적용\n",
    "# row_number(): 동점이어도 순차적 번호 (1, 2, 3, 4...)\n",
    "# rank(): 동점은 같은 순위, 다음 순위 건너뜀 (1, 1, 3, 4...)\n",
    "# dense_rank(): 동점은 같은 순위, 순위 안 건너뜀 (1, 1, 2, 3...)\n",
    "df_ranked = df.filter(col(\"salary\").isNotNull()).withColumn(\n",
    "    \"row_num\",row_number().over(window_rank)\n",
    ").withColumn(\n",
    "    \"rank\",rank().over(window_rank)\n",
    ").withColumn(\n",
    "    \"dense_rank\",dense_rank().over(window_rank)\n",
    ")\n",
    "\n",
    "print(\"=== 부서별 급여 순위 ===\")\n",
    "df_ranked.select(\n",
    "    \"department\", \"name\", \"salary\", \"row_num\", \"rank\", \"dense_rank\"\n",
    ").orderBy(\"department\", \"row_num\").show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb568066",
   "metadata": {},
   "source": [
    "#### lag(), lead(): 이전/다음 행 참조"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64cf546d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== lag/lead: 전일 대비 비교 ===\n",
      "+-----------+-------+------+-----------+------+\n",
      "|date_parsed|product|amount|prev_amount|change|\n",
      "+-----------+-------+------+-----------+------+\n",
      "| 2026-01-01|      A|   791|       NULL|  NULL|\n",
      "| 2026-01-02|      A|   212|        791|  -579|\n",
      "| 2026-01-05|      A|   541|        212|   329|\n",
      "| 2026-01-06|      A|   663|        541|   122|\n",
      "| 2026-01-07|      A|   367|        663|  -296|\n",
      "| 2026-01-19|      A|   741|        367|   374|\n",
      "| 2026-01-21|      A|   665|        741|   -76|\n",
      "| 2026-01-24|      A|   324|        665|  -341|\n",
      "| 2026-01-25|      A|   484|        324|   160|\n",
      "| 2026-01-28|      A|   229|        484|  -255|\n",
      "+-----------+-------+------+-----------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# lag(), lead(): 이전/다음 행 참조\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# lag(컬럼, n): n행 이전 값\n",
    "# lead(컬럼, n): n행 이후 값\n",
    "# 시계열 분석, 전일 대비 비교 등에 유용\n",
    "\n",
    "# 날짜순 정렬된 매출 데이터로 실습\n",
    "df_sales_parsed = df_sales.withColumn(\"date_parsed\", to_date(col(\"date\")))\n",
    "\n",
    "# Window: 제품별, 날짜순\n",
    "window_sales = Window.partitionBy(\"product\").orderBy(\"date_parsed\")\n",
    "\n",
    "df_with_prev = df_sales_parsed.withColumn(\n",
    "    \"prev_amount\", lag(\"amount\", 1).over(window_sales)      # 전일 매출\n",
    ").withColumn(\n",
    "    \"next_amount\", lead(\"amount\", 1).over(window_sales)     # 익일 매출\n",
    ").withColumn(\n",
    "    # 전일 대비 증감\n",
    "    \"change\", col(\"amount\") - col(\"prev_amount\")\n",
    ")\n",
    "\n",
    "print(\"=== lag/lead: 전일 대비 비교 ===\")\n",
    "df_with_prev.filter(col(\"product\") == \"A\").select(\n",
    "    \"date_parsed\", \"product\", \"amount\", \"prev_amount\", \"change\"\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047de914",
   "metadata": {},
   "source": [
    "#### 누적 합계 / 이동 평균"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa1cde69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 누적합 / 7일 이동평균 ===\n",
      "+-----------+-------+------+------+------+\n",
      "|date_parsed|product|amount|cumsum|   ma7|\n",
      "+-----------+-------+------+------+------+\n",
      "| 2026-01-01|      A|   791|   791| 791.0|\n",
      "| 2026-01-02|      A|   212|  1003| 501.5|\n",
      "| 2026-01-05|      A|   541|  1544|514.67|\n",
      "| 2026-01-06|      A|   663|  2207|551.75|\n",
      "| 2026-01-07|      A|   367|  2574| 514.8|\n",
      "| 2026-01-19|      A|   741|  3315| 552.5|\n",
      "| 2026-01-21|      A|   665|  3980|568.57|\n",
      "| 2026-01-24|      A|   324|  4304|501.86|\n",
      "| 2026-01-25|      A|   484|  4788|540.71|\n",
      "| 2026-01-28|      A|   229|  5017|496.14|\n",
      "| 2026-02-01|      A|   771|  5788|511.57|\n",
      "| 2026-02-03|      A|   515|  6303|532.71|\n",
      "| 2026-02-07|      A|   302|  6605| 470.0|\n",
      "| 2026-02-11|      A|   866|  7471|498.71|\n",
      "| 2026-02-19|      A|   851|  8322| 574.0|\n",
      "+-----------+-------+------+------+------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 누적 합계 / 이동 평균\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 범위 지정: rowsBetween(시작, 끝)\n",
    "# unboundedPreceding: 파티션 시작부터\n",
    "# currentRow: 현재 행까지\n",
    "# unboundedFollowing: 파티션 끝까지\n",
    "# 숫자: 현재 행 기준 상대 위치 (-3, 0, 2 등)\n",
    "\n",
    "# 누적 합계: 시작부터 현재까지\n",
    "window_cumsum = Window.partitionBy(\"product\").orderBy(\"date_parsed\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# 7일 이동 평균: 최근 7일\n",
    "window_ma7 = Window.partitionBy(\"product\").orderBy(\"date_parsed\") \\\n",
    "    .rowsBetween(-6, Window.currentRow)  # 현재 포함 7일\n",
    "\n",
    "df_cumsum = df_sales_parsed.withColumn(\n",
    "    \"cumsum\", sum(\"amount\").over(window_cumsum)\n",
    ").withColumn(\n",
    "    \"ma7\", spark_round(avg(\"amount\").over(window_ma7), 2)\n",
    ")\n",
    "\n",
    "print(\"=== 누적합 / 7일 이동평균 ===\")\n",
    "df_cumsum.filter(col(\"product\") == \"A\").select(\n",
    "    \"date_parsed\", \"product\", \"amount\", \"cumsum\", \"ma7\"\n",
    ").show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa4b86",
   "metadata": {},
   "source": [
    "#### 실무 패턴: 그룹별 Top N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b30c6d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 부서별 급여 Top 3 ===\n",
      "+-----------+---------------+-------+----+\n",
      "| department|           name| salary|rank|\n",
      "+-----------+---------------+-------+----+\n",
      "|       NULL|  Employee 17  |95000.0|   1|\n",
      "|       NULL|  Employee 19  |78000.0|   2|\n",
      "|       NULL|  Employee 29  |76000.0|   3|\n",
      "|Engineering|   Employee 7  |90000.0|   1|\n",
      "|Engineering|  Employee 22  |82000.0|   2|\n",
      "|Engineering|   Employee 3  |70000.0|   3|\n",
      "|  Marketing|  Employee 12  |85000.0|   1|\n",
      "|  Marketing|   Employee 4  |80000.0|   2|\n",
      "|  Marketing|  Employee 14  |72000.0|   3|\n",
      "|      Sales|  Employee 27  |88000.0|   1|\n",
      "|      Sales|  Employee 10  |75000.0|   2|\n",
      "|      Sales|  Employee 26  |69000.0|   3|\n",
      "+-----------+---------------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 실무 패턴: 그룹별 Top N\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 부서별 급여 Top 3 뽑기\n",
    "# 1. row_number()로 순위 매기기\n",
    "# 2. filter로 순위 <= 3 필터링\n",
    "\n",
    "window_top = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df_top3 = df.filter(col(\"salary\").isNotNull()).withColumn(\n",
    "    \"rank\", row_number().over(window_top)\n",
    ").filter(\n",
    "    col(\"rank\") <= 3\n",
    ")\n",
    "\n",
    "print(\"=== 부서별 급여 Top 3 ===\")\n",
    "df_top3.select(\"department\", \"name\", \"salary\", \"rank\") \\\n",
    "    .orderBy(\"department\", \"rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d662addc",
   "metadata": {},
   "source": [
    "#### 실습 4-1: 순위 함수\n",
    "##### 부서별로 급여 내림차순 순위(rank)를 매긴 \"salary_rank\" 컬럼을 추가하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "364c613a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------+-----------+\n",
      "| department|           name| salary|salary_rank|\n",
      "+-----------+---------------+-------+-----------+\n",
      "|       NULL|  Employee 17  |95000.0|          1|\n",
      "|       NULL|  Employee 19  |78000.0|          2|\n",
      "|       NULL|  Employee 29  |76000.0|          3|\n",
      "|       NULL|  Employee 25  |73000.0|          4|\n",
      "|       NULL|  Employee 30  |71000.0|          5|\n",
      "|       NULL|  Employee 15  |68000.0|          6|\n",
      "|       NULL|  Employee 18  |62000.0|          7|\n",
      "|       NULL|   Employee 6  |60000.0|          8|\n",
      "|Engineering|   Employee 7  |90000.0|          1|\n",
      "|Engineering|  Employee 22  |82000.0|          2|\n",
      "|Engineering|   Employee 3  |70000.0|          3|\n",
      "|Engineering|   Employee 8  |55000.0|          4|\n",
      "|  Marketing|  Employee 12  |85000.0|          1|\n",
      "|  Marketing|   Employee 4  |80000.0|          2|\n",
      "|  Marketing|  Employee 14  |72000.0|          3|\n",
      "|  Marketing|  Employee 11  |65000.0|          4|\n",
      "|  Marketing|   Employee 1  |50000.0|          5|\n",
      "|      Sales|  Employee 27  |88000.0|          1|\n",
      "|      Sales|  Employee 10  |75000.0|          2|\n",
      "|      Sales|  Employee 26  |69000.0|          3|\n",
      "+-----------+---------------+-------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "df.filter(col(\"salary\").isNotNull()).withColumn(\n",
    "    \"salary_rank\", row_number().over(window_spec)\n",
    ").select(\"department\", \"name\", \"salary\", \"salary_rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fa7a9b",
   "metadata": {},
   "source": [
    "#### 실습 4-2: lag 함수\n",
    "##### 매출 데이터에서 제품별 전일 매출(prev_amount)을 추가하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e9758d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+------+-----------+-----------+\n",
      "|      date|product|region|amount|date_parsed|prev_amount|\n",
      "+----------+-------+------+------+-----------+-----------+\n",
      "|2026-01-01|      A| Daegu|   791| 2026-01-01|       NULL|\n",
      "|2026-01-02|      A| Busan|   212| 2026-01-02|        791|\n",
      "|2026-01-05|      A| Seoul|   541| 2026-01-05|        212|\n",
      "|2026-01-06|      A| Busan|   663| 2026-01-06|        541|\n",
      "|2026-01-07|      A| Seoul|   367| 2026-01-07|        663|\n",
      "|2026-01-19|      A| Daegu|   741| 2026-01-19|        367|\n",
      "|2026-01-21|      A| Daegu|   665| 2026-01-21|        741|\n",
      "|2026-01-24|      A| Busan|   324| 2026-01-24|        665|\n",
      "|2026-01-25|      A| Daegu|   484| 2026-01-25|        324|\n",
      "|2026-01-28|      A| Daegu|   229| 2026-01-28|        484|\n",
      "+----------+-------+------+------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_parsed = df_sales.withColumn(\"date_parsed\", to_date(col(\"date\")))\n",
    "window_spec = Window.partitionBy(\"product\").orderBy(\"date_parsed\")\n",
    "\n",
    "df_sales_parsed.withColumn(\n",
    "    \"prev_amount\", lag(\"amount\", 1).over(window_spec)\n",
    ").filter(col(\"product\") == \"A\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895fd0b1",
   "metadata": {},
   "source": [
    "#### 실습 4-3: 누적 합계\n",
    "##### 제품별 날짜순 누적 매출(cumsum)을 계산하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d3ea92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+------+\n",
      "|date_parsed|amount|cumsum|\n",
      "+-----------+------+------+\n",
      "| 2026-01-01|   791|   791|\n",
      "| 2026-01-02|   212|  1003|\n",
      "| 2026-01-05|   541|  1544|\n",
      "| 2026-01-06|   663|  2207|\n",
      "| 2026-01-07|   367|  2574|\n",
      "| 2026-01-19|   741|  3315|\n",
      "| 2026-01-21|   665|  3980|\n",
      "| 2026-01-24|   324|  4304|\n",
      "| 2026-01-25|   484|  4788|\n",
      "| 2026-01-28|   229|  5017|\n",
      "+-----------+------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sales_parsed = df_sales.withColumn(\"date_parsed\", to_date(col(\"date\")))\n",
    "window_cumsum = Window.partitionBy(\"product\").orderBy(\"date_parsed\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_sales_parsed.withColumn(\n",
    "    \"cumsum\", sum(\"amount\").over(window_cumsum)\n",
    ").filter(col(\"product\") == \"A\").select(\"date_parsed\", \"amount\", \"cumsum\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfbfbb5",
   "metadata": {},
   "source": [
    "#### 실습 4-4: 그룹별 Top N\n",
    "##### 부서별 급여 상위 2명만 필터링하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6780b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------+----+\n",
      "| department|           name| salary|rank|\n",
      "+-----------+---------------+-------+----+\n",
      "|       NULL|  Employee 17  |95000.0|   1|\n",
      "|       NULL|  Employee 19  |78000.0|   2|\n",
      "|Engineering|   Employee 7  |90000.0|   1|\n",
      "|Engineering|  Employee 22  |82000.0|   2|\n",
      "|  Marketing|  Employee 12  |85000.0|   1|\n",
      "|  Marketing|   Employee 4  |80000.0|   2|\n",
      "|      Sales|  Employee 27  |88000.0|   1|\n",
      "|      Sales|  Employee 10  |75000.0|   2|\n",
      "+-----------+---------------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df.filter(col(\"salary\").isNotNull()).withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ").filter(col(\"rank\") <= 2) \\\n",
    "    .select(\"department\", \"name\", \"salary\", \"rank\").orderBy(\"department\", \"rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce17326",
   "metadata": {},
   "source": [
    "#### 자주 쓰는 실무 패턴\n",
    "##### 패턴 1: 조건부 집계\n",
    "##### groupBy + when 조합으로 조건별 카운트/합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be1f9cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 패턴 1: 조건부 집계 ===\n",
      "+-----------+-----+---------------+--------------+---------------+\n",
      "| department|total|high_salary_cnt|low_salary_cnt|high_salary_sum|\n",
      "+-----------+-----+---------------+--------------+---------------+\n",
      "|  Marketing|    9|              2|             0|       165000.0|\n",
      "|Engineering|    5|              2|             0|       172000.0|\n",
      "|      Sales|    6|              1|             0|        88000.0|\n",
      "|       NULL|   10|              1|             0|        95000.0|\n",
      "+-----------+-----+---------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conditional_agg = df.groupBy(\"department\").agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    # 조건별 카운트\n",
    "    count(when(col(\"salary\") >= 80000, 1)).alias(\"high_salary_cnt\"),\n",
    "    count(when(col(\"salary\") < 50000, 1)).alias(\"low_salary_cnt\"),\n",
    "    # 조건별 합계\n",
    "    sum(when(col(\"salary\") >= 80000, col(\"salary\"))).alias(\"high_salary_sum\"),\n",
    ")\n",
    "\n",
    "print(\"=== 패턴 1: 조건부 집계 ===\")\n",
    "conditional_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae99485",
   "metadata": {},
   "source": [
    "#### 패턴 2: 전체/그룹 대비 비율\n",
    "#####  Window 함수로 그룹 전체 합계를 각 행에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87a5342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 패턴 2: 부서 내 급여 비율 ===\n",
      "+-----------+---------------+-------+----------+-----------+\n",
      "| department|           name| salary|dept_total|pct_of_dept|\n",
      "+-----------+---------------+-------+----------+-----------+\n",
      "|       NULL|  Employee 17  |95000.0|  583000.0|       16.3|\n",
      "|       NULL|  Employee 19  |78000.0|  583000.0|      13.38|\n",
      "|       NULL|  Employee 29  |76000.0|  583000.0|      13.04|\n",
      "|       NULL|  Employee 25  |73000.0|  583000.0|      12.52|\n",
      "|       NULL|  Employee 30  |71000.0|  583000.0|      12.18|\n",
      "|       NULL|  Employee 15  |68000.0|  583000.0|      11.66|\n",
      "|       NULL|  Employee 18  |62000.0|  583000.0|      10.63|\n",
      "|       NULL|   Employee 6  |60000.0|  583000.0|      10.29|\n",
      "|Engineering|   Employee 7  |90000.0|  297000.0|       30.3|\n",
      "|Engineering|  Employee 22  |82000.0|  297000.0|      27.61|\n",
      "+-----------+---------------+-------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_dept = Window.partitionBy(\"department\")\n",
    "\n",
    "df_ratio = df.filter(col(\"salary\").isNotNull()).withColumn(\n",
    "    \"dept_total\", sum(\"salary\").over(window_dept)\n",
    ").withColumn(\n",
    "    \"pct_of_dept\", spark_round(col(\"salary\") / col(\"dept_total\") * 100, 2)  # 부서 내 비율\n",
    ")\n",
    "\n",
    "print(\"=== 패턴 2: 부서 내 급여 비율 ===\")\n",
    "df_ratio.select(\"department\", \"name\", \"salary\", \"dept_total\", \"pct_of_dept\") \\\n",
    "    .orderBy(\"department\", col(\"salary\").desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e276f9",
   "metadata": {},
   "source": [
    "#### 패턴 3: 데이터 품질 체크\n",
    "##### 한 번에 여러 품질 지표 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c94de973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 패턴 3: 데이터 품질 체크 ===\n",
      "+----------+---------------+-------------+---------------+---------------+---------+\n",
      "|total_rows|emp_id_non_null|emp_id_unique|salary_non_null|negative_salary|dept_null|\n",
      "+----------+---------------+-------------+---------------+---------------+---------+\n",
      "|        30|             30|           30|             22|              0|       10|\n",
      "+----------+---------------+-------------+---------------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quality_check = df.agg(\n",
    "    count(\"*\").alias(\"total_rows\"),\n",
    "    count(\"emp_id\").alias(\"emp_id_non_null\"),\n",
    "    countDistinct(\"emp_id\").alias(\"emp_id_unique\"),\n",
    "    count(\"salary\").alias(\"salary_non_null\"),\n",
    "    sum(when(col(\"salary\") < 0, 1).otherwise(0)).alias(\"negative_salary\"),\n",
    "    sum(when(col(\"department\").isNull(), 1).otherwise(0)).alias(\"dept_null\"),\n",
    ")\n",
    "\n",
    "print(\"=== 패턴 3: 데이터 품질 체크 ===\")\n",
    "quality_check.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c46ac7",
   "metadata": {},
   "source": [
    "#### 패턴 4: 컬럼명 일괄 변경 (소문자, 공백→언더스코어)\n",
    "#####  toDF()로 모든 컬럼명 한번에 변경\n",
    "##### 리스트 컴프리헨션으로 변환 규칙 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d62162e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변경 전: ['User ID', 'Product Name', 'Total Amount']\n",
      "변경 후: ['user_id', 'product_name', 'total_amount']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 패턴 4: 컬럼명 일괄 변경 (소문자, 공백→언더스코어)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# toDF()로 모든 컬럼명 한번에 변경\n",
    "# 리스트 컴프리헨션으로 변환 규칙 적용\n",
    "\n",
    "# 예시 DataFrame\n",
    "df_messy_cols = spark.createDataFrame([\n",
    "    (1, \"A\", 100),\n",
    "    (2, \"B\", 200),\n",
    "], [\"User ID\", \"Product Name\", \"Total Amount\"])\n",
    "\n",
    "print(\"변경 전:\", df_messy_cols.columns)\n",
    "\n",
    "# 소문자 + 공백을 _로 변환\n",
    "new_cols = [c.lower().replace(\" \", \"_\") for c in df_messy_cols.columns]\n",
    "df_clean_cols = df_messy_cols.toDF(*new_cols)\n",
    "\n",
    "print(\"변경 후:\", df_clean_cols.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14209d",
   "metadata": {},
   "source": [
    "#### 패턴 5: Forward Fill (이전 값으로 NULL 채우기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89ba9915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 패턴 5: Forward Fill ===\n",
      "+---+----------+-----+------------+\n",
      "| id|      date|value|value_filled|\n",
      "+---+----------+-----+------------+\n",
      "|  1|2024-01-01|  100|         100|\n",
      "|  1|2024-01-02| NULL|         100|\n",
      "|  1|2024-01-03| NULL|         100|\n",
      "|  1|2024-01-04|  200|         200|\n",
      "|  1|2024-01-05| NULL|         200|\n",
      "+---+----------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 패턴 5: Forward Fill (이전 값으로 NULL 채우기)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# last(ignorenulls=True)를 Window와 함께 사용\n",
    "# NULL을 직전 non-null 값으로 채움\n",
    "\n",
    "df_with_null = spark.createDataFrame([\n",
    "    (1, \"2024-01-01\", 100),\n",
    "    (1, \"2024-01-02\", None),\n",
    "    (1, \"2024-01-03\", None),\n",
    "    (1, \"2024-01-04\", 200),\n",
    "    (1, \"2024-01-05\", None),\n",
    "], [\"id\", \"date\", \"value\"])\n",
    "\n",
    "# Window: 시작~현재까지\n",
    "window_ff = Window.partitionBy(\"id\").orderBy(\"date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "df_filled = df_with_null.withColumn(\n",
    "    \"value_filled\",\n",
    "    # ignorenulls=True: NULL을 무시하고 마지막 non-null 값 반환\n",
    "    last(\"value\", ignorenulls=True).over(window_ff)\n",
    ")\n",
    "\n",
    "print(\"=== 패턴 5: Forward Fill ===\")\n",
    "df_filled.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f34ab892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 완료!\n",
      "행 수: 50, 컬럼: ['emp_id', 'name', 'department', 'salary', 'age', 'hire_year']\n",
      "+------+----------+----------+------+---+---------+\n",
      "|emp_id|      name|department|salary|age|hire_year|\n",
      "+------+----------+----------+------+---+---------+\n",
      "|  E001|Employee_1|        HR| 63483| 50|     2024|\n",
      "|  E002|Employee_2|   Finance| 88555| 33|     2022|\n",
      "|  E003|Employee_3| Marketing| 57159| 52|     2024|\n",
      "|  E004|Employee_4|   Finance| 75920| 31|     2023|\n",
      "|  E005|Employee_5|   Finance|107121| 33|     2024|\n",
      "+------+----------+----------+------+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 샘플 데이터\n",
    "sample_data = pd.DataFrame({\n",
    "    \"emp_id\": [f\"E{i:03d}\" for i in range(1, 101)],\n",
    "    \"name\": [f\"Employee_{i}\" for i in range(1, 101)],\n",
    "    \"department\": np.random.choice(\n",
    "        [\"Engineering\", \"Sales\", \"Marketing\", \"HR\", \"Finance\"], 100\n",
    "    ),\n",
    "    \"salary\": np.random.randint(40000, 120000, 100),\n",
    "    \"age\": np.random.randint(25, 55, 100),\n",
    "    \"is_manager\": np.random.choice([True, False], 100, p=[0.2, 0.8]),\n",
    "})\n",
    "# DataFrame 로드\n",
    "df = spark.read.csv(\"/tmp/spark_tutorial/employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "print(\"데이터 로드 완료!\")\n",
    "print(f\"행 수: {df.count()}, 컬럼: {df.columns}\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcb810",
   "metadata": {},
   "source": [
    "#### 패턴 6: 여러 컬럼에 같은 처리 일괄 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8645e86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 패턴 6: 여러 컬럼 일괄 집계 ===\n",
      "+------------+---------+----------+-------+\n",
      "|total_salary|total_age|avg_salary|avg_age|\n",
      "+------------+---------+----------+-------+\n",
      "|     3954398|     1912|  79087.96|  38.24|\n",
      "+------------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 패턴 6: 여러 컬럼에 같은 처리 일괄 적용\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 리스트 컴프리헨션 + agg로 여러 컬럼에 같은 집계 적용\n",
    "cols_to_sum = [\"salary\", \"age\"]\n",
    "\n",
    "# 방법: 리스트 컴프리헨션으로 집계 함수 생성\n",
    "agg_exprs = [sum(c).alias(f\"total_{c}\") for c in cols_to_sum]\n",
    "agg_exprs += [avg(c).alias(f\"avg_{c}\") for c in cols_to_sum]\n",
    "\n",
    "result = df.agg(*agg_exprs)\n",
    "\n",
    "print(\"=== 패턴 6: 여러 컬럼 일괄 집계 ===\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a80957",
   "metadata": {},
   "source": [
    "#### 과제: 직원 성과 분석 ETL 파이프라인\n",
    "\n",
    "##### 시나리오\n",
    "##### 데이터 엔지니어로서 직원 성과 분석 시스템을 구축해야 합니다. 원본 데이터는 품질 이슈(NULL, 공백, 형식 ##### 불일치)가 있어 정제가 필요하고, 날짜 기반 파생 변수와 Window 함수를 활용한 성과 지표 계산이 요구됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6297620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: 데이터 클렌징 완료 ===\n",
      "+----------+-------+-----------+\n",
      "|      name| salary| department|\n",
      "+----------+-------+-----------+\n",
      "|Employee 1|50000.0|  Marketing|\n",
      "|Employee 2|60000.0| Unassigned|\n",
      "|Employee 3|70000.0|Engineering|\n",
      "|Employee 4|80000.0|  Marketing|\n",
      "|Employee 5|60000.0|  Marketing|\n",
      "+----------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "NULL 개수 확인:\n",
      "+-----------+---------+\n",
      "|salary_null|dept_null|\n",
      "+-----------+---------+\n",
      "|          0|        0|\n",
      "+-----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_step1 = (\n",
    "    df\n",
    "    .withColumn(\"name\", trim(col(\"name\")))\n",
    "    .fillna({\"salary\": 60000, \"department\": \"Unassigned\"})\n",
    "    .withColumn(\"username\", split(col(\"email\"), \"@\")[0])\n",
    ")\n",
    "\n",
    "print(\"=== Step 1: 데이터 클렌징 완료 ===\")\n",
    "df_step1.select(\"name\", \"salary\", \"department\").show(5)\n",
    "\n",
    "# NULL 개수 확인\n",
    "print(\"NULL 개수 확인:\")\n",
    "df_step1.select(\n",
    "    count(when(col(\"salary\").isNull(), 1)).alias(\"salary_null\"),\n",
    "    count(when(col(\"department\").isNull(), 1)).alias(\"dept_null\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4d0c3",
   "metadata": {},
   "source": [
    "#### Step 2: 날짜 파생 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ec639f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 2: 날짜 파생 변수 ===\n",
      "+-----------+----------+--------+------+--------+--------+--------+\n",
      "|       name| join_date|입사연도|입사월|입사분기|근속일수|근속년수|\n",
      "+-----------+----------+--------+------+--------+--------+--------+\n",
      "| Employee 1|2020-01-15|    2020|     1|      Q1|    2198|     6.0|\n",
      "| Employee 2|2020-02-29|    2020|     2|      Q1|    2153|     5.9|\n",
      "| Employee 3|2020-04-14|    2020|     4|      Q2|    2108|     5.8|\n",
      "| Employee 4|2020-05-29|    2020|     5|      Q2|    2063|     5.7|\n",
      "| Employee 5|2020-07-13|    2020|     7|      Q3|    2018|     5.5|\n",
      "| Employee 6|2020-08-27|    2020|     8|      Q3|    1973|     5.4|\n",
      "| Employee 7|2020-10-11|    2020|    10|      Q4|    1928|     5.3|\n",
      "| Employee 8|2020-11-25|    2020|    11|      Q4|    1883|     5.2|\n",
      "| Employee 9|2021-01-09|    2021|     1|      Q1|    1838|     5.0|\n",
      "|Employee 10|2021-02-23|    2021|     2|      Q1|    1793|     4.9|\n",
      "+-----------+----------+--------+------+--------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------+-----------+-----------------+-----------+-------+----------+--------+------------+--------+------+--------+--------+--------+\n",
      "|emp_id|       name|            email| department| salary| join_date|username|join_date_dt|입사연도|입사월|근속일수|근속년수|입사분기|\n",
      "+------+-----------+-----------------+-----------+-------+----------+--------+------------+--------+------+--------+--------+--------+\n",
      "|  E001| Employee 1| emp1@company.com|  Marketing|50000.0|2020-01-15|    emp1|  2020-01-15|    2020|     1|    2198|     6.0|      Q1|\n",
      "|  E002| Employee 2| emp2@company.com| Unassigned|60000.0|2020-02-29|    emp2|  2020-02-29|    2020|     2|    2153|     5.9|      Q1|\n",
      "|  E003| Employee 3| emp3@company.com|Engineering|70000.0|2020-04-14|    emp3|  2020-04-14|    2020|     4|    2108|     5.8|      Q2|\n",
      "|  E004| Employee 4| emp4@company.com|  Marketing|80000.0|2020-05-29|    emp4|  2020-05-29|    2020|     5|    2063|     5.7|      Q2|\n",
      "|  E005| Employee 5| emp5@company.com|  Marketing|60000.0|2020-07-13|    emp5|  2020-07-13|    2020|     7|    2018|     5.5|      Q3|\n",
      "|  E006| Employee 6| emp6@company.com| Unassigned|60000.0|2020-08-27|    emp6|  2020-08-27|    2020|     8|    1973|     5.4|      Q3|\n",
      "|  E007| Employee 7| emp7@company.com|Engineering|90000.0|2020-10-11|    emp7|  2020-10-11|    2020|    10|    1928|     5.3|      Q4|\n",
      "|  E008| Employee 8| emp8@company.com|Engineering|55000.0|2020-11-25|    emp8|  2020-11-25|    2020|    11|    1883|     5.2|      Q4|\n",
      "|  E009| Employee 9| emp9@company.com|  Marketing|60000.0|2021-01-09|    emp9|  2021-01-09|    2021|     1|    1838|     5.0|      Q1|\n",
      "|  E010|Employee 10|emp10@company.com|      Sales|75000.0|2021-02-23|   emp10|  2021-02-23|    2021|     2|    1793|     4.9|      Q1|\n",
      "+------+-----------+-----------------+-----------+-------+----------+--------+------------+--------+------+--------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_step2 = (\n",
    "    df_step1.withColumn(\"join_date_dt\", to_date(col(\"join_date\"), \"yyyy-MM-dd\"))\n",
    "    .withColumn(\"입사연도\", year(col(\"join_date_dt\")))\n",
    "    .withColumn(\"입사월\", month(col(\"join_date_dt\")))\n",
    "    .withColumn(\"근속일수\", datediff(current_date(), col(\"join_date_dt\")))\n",
    "    .withColumn(\"근속년수\", spark_round(col(\"근속일수\") / 365, 1))\n",
    "    .withColumn(\n",
    "        \"입사분기\",\n",
    "        when(col(\"입사월\") <= 3, \"Q1\")\n",
    "        .when(col(\"입사월\") <= 6, \"Q2\")\n",
    "        .when(col(\"입사월\") <= 9, \"Q3\")\n",
    "        .otherwise(\"Q4\")\n",
    "    )\n",
    ") \n",
    "\n",
    "print(\"=== Step 2: 날짜 파생 변수 ===\")\n",
    "df_step2.select(\n",
    "    \"name\", \"join_date\", \"입사연도\", \"입사월\", \"입사분기\", \"근속일수\", \"근속년수\"\n",
    ").show(10)\n",
    "df_step2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a3ef4",
   "metadata": {},
   "source": [
    "#### Step 3: Window 함수로 성과 지표 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "beba7c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 3: 성과 지표 ===\n",
      "+-----------+-----------+-------+----------+------------+--------+----------+\n",
      "| department|       name| salary|부서내순위|부서평균급여|급여편차|부서내비율|\n",
      "+-----------+-----------+-------+----------+------------+--------+----------+\n",
      "|Engineering| Employee 7|90000.0|         1|     71400.0| 18600.0|      25.2|\n",
      "|Engineering|Employee 22|82000.0|         2|     71400.0| 10600.0|      23.0|\n",
      "|Engineering| Employee 3|70000.0|         3|     71400.0| -1400.0|      19.6|\n",
      "|Engineering|Employee 16|60000.0|         4|     71400.0|-11400.0|      16.8|\n",
      "|Engineering| Employee 8|55000.0|         5|     71400.0|-16400.0|      15.4|\n",
      "|  Marketing|Employee 12|85000.0|         1|     65778.0| 19222.0|      14.4|\n",
      "|  Marketing| Employee 4|80000.0|         2|     65778.0| 14222.0|      13.5|\n",
      "|  Marketing|Employee 14|72000.0|         3|     65778.0|  6222.0|      12.2|\n",
      "|  Marketing|Employee 11|65000.0|         4|     65778.0|  -778.0|      11.0|\n",
      "|  Marketing| Employee 5|60000.0|         5|     65778.0| -5778.0|      10.1|\n",
      "|  Marketing| Employee 9|60000.0|         6|     65778.0| -5778.0|      10.1|\n",
      "|  Marketing|Employee 13|60000.0|         7|     65778.0| -5778.0|      10.1|\n",
      "|  Marketing|Employee 20|60000.0|         8|     65778.0| -5778.0|      10.1|\n",
      "|  Marketing| Employee 1|50000.0|         9|     65778.0|-15778.0|       8.4|\n",
      "|      Sales|Employee 27|88000.0|         1|     69500.0| 18500.0|      21.1|\n",
      "+-----------+-----------+-------+----------+------------+--------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Window 정의\n",
    "window_dept = Window.partitionBy(\"department\")\n",
    "window_dept_rank = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "df_step3 = ( \n",
    "    df_step2.withColumn(\n",
    "    \"부서내순위\", row_number().over(window_dept_rank))\n",
    "    .withColumn(\"부서평균급여\", spark_round(avg(\"salary\").over(window_dept), 0))\n",
    "    .withColumn(\"급여편차\", col(\"salary\") - col(\"부서평균급여\"))\n",
    "    .withColumn(\n",
    "        \"부서내비율\",\n",
    "        spark_round(col(\"salary\") / sum(\"salary\").over(window_dept) * 100, 1)\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"=== Step 3: 성과 지표 ===\")\n",
    "df_step3.select(\n",
    "    \"department\", \"name\", \"salary\", \"부서내순위\", \"부서평균급여\", \"급여편차\", \"부서내비율\"\n",
    ").orderBy(\"department\", \"부서내순위\").show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ed75c",
   "metadata": {},
   "source": [
    "#### Step 4: 데이터 품질 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1128fc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 4: 데이터 품질 검증 ===\n",
      "전체 행 수: 30\n",
      "\n",
      "[NULL 개수]\n",
      "+-----------+---------+---------+\n",
      "|salary_null|dept_null|date_null|\n",
      "+-----------+---------+---------+\n",
      "|          0|        0|        0|\n",
      "+-----------+---------+---------+\n",
      "\n",
      "[급여 통계]\n",
      "+--------+--------+--------+\n",
      "|최소급여|최대급여|평균급여|\n",
      "+--------+--------+--------+\n",
      "| 50000.0| 95000.0| 68967.0|\n",
      "+--------+--------+--------+\n",
      "\n",
      "[부서별 인원 분포]\n",
      "+-----------+-----+\n",
      "| department|count|\n",
      "+-----------+-----+\n",
      "| Unassigned|   10|\n",
      "|  Marketing|    9|\n",
      "|      Sales|    6|\n",
      "|Engineering|    5|\n",
      "+-----------+-----+\n",
      "\n",
      "[중복 emp_id 개수]: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Step 4: 데이터 품질 검증 ===\")\n",
    "\n",
    "# 1. 전체 행 수\n",
    "print(f\"전체 행 수: {df_step3.count()}\")\n",
    "\n",
    "# 2. NULL 개수\n",
    "print(\"\\n[NULL 개수]\")\n",
    "df_step3.select(\n",
    "    count(when(col(\"salary\").isNull(), 1)).alias(\"salary_null\"),\n",
    "    count(when(col(\"department\").isNull(), 1)).alias(\"dept_null\"),\n",
    "    count(when(col(\"join_date_dt\").isNull(), 1)).alias(\"date_null\")\n",
    ").show()\n",
    "\n",
    "# 3. 급여 통계\n",
    "print(\"[급여 통계]\")\n",
    "df_step3.agg(\n",
    "    min(\"salary\").alias(\"최소급여\"),\n",
    "    max(\"salary\").alias(\"최대급여\"),\n",
    "    spark_round(avg(\"salary\"), 0).alias(\"평균급여\")\n",
    ").show()\n",
    "\n",
    "# 4. 부서별 인원 분포\n",
    "print(\"[부서별 인원 분포]\")\n",
    "df_step3.groupBy(\"department\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "# 5. 중복 emp_id 확인\n",
    "dup_count = df_step3.groupBy(\"emp_id\").count().filter(col(\"count\") > 1).count()\n",
    "print(f\"[중복 emp_id 개수]: {dup_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
